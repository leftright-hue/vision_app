"""
Lab 03: Transfer Learningê³¼ ë©€í‹°ëª¨ë‹¬ API í†µí•© ì‹¤ìŠµ
Week 3: ë”¥ëŸ¬ë‹ ì˜ìƒì²˜ë¦¬

ì´ ì‹¤ìŠµì—ì„œëŠ” ì „ì´í•™ìŠµê³¼ ë©€í‹°ëª¨ë‹¬ APIë¥¼ í†µí•©í•˜ì—¬
ìì—°ì–´ ê¸°ë°˜ ì‚¬ì§„ì²© ê²€ìƒ‰ ì•±ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""

import gradio as gr
import torch
import torch.nn as nn
from torchvision import models, transforms
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
import numpy as np
from typing import List, Dict, Optional, Tuple
import os
from pathlib import Path
import json
import time
from dataclasses import dataclass
import logging

# Google Gemini (ì„ íƒì )
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("Gemini API not available. Install google-generativeai to enable.")

# Together AI (ì„ íƒì )
try:
    import together
    TOGETHER_AVAILABLE = True
except ImportError:
    TOGETHER_AVAILABLE = False
    print("Together AI not available. Install together to enable.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PhotoMetadata:
    """ì‚¬ì§„ ë©”íƒ€ë°ì´í„°"""
    path: str
    filename: str
    caption: Optional[str] = None
    tags: Optional[List[str]] = None
    embedding: Optional[np.ndarray] = None
    timestamp: Optional[float] = None


class SmartPhotoAlbum:
    """
    ìŠ¤ë§ˆíŠ¸ ì‚¬ì§„ì²© ì• í”Œë¦¬ì¼€ì´ì…˜
    
    ê¸°ëŠ¥:
    1. Transfer Learningì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜
    2. CLIPì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê²€ìƒ‰
    3. Gemini/Together AIë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±
    4. í†µí•© ê²€ìƒ‰ ë° ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤
    """
    
    def __init__(self):
        """ëª¨ë¸ ë° API ì´ˆê¸°í™”"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # 1. Transfer Learning ëª¨ë¸ ì´ˆê¸°í™” (ì´ë¯¸ì§€ ë¶„ë¥˜ìš©)
        self.classifier = self._init_classifier()
        
        # 2. CLIP ëª¨ë¸ ì´ˆê¸°í™” (í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê²€ìƒ‰ìš©)
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_model.to(self.device)
        
        # 3. Gemini API ì´ˆê¸°í™” (ìº¡ì…˜ ìƒì„±ìš©)
        self.gemini_model = None
        if GEMINI_AVAILABLE and os.getenv('GEMINI_API_KEY'):
            genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
            self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
            logger.info("Gemini API initialized")
        
        # 4. Together AI ì´ˆê¸°í™” (ëŒ€ì²´ ìº¡ì…˜ ìƒì„±)
        self.together_available = False
        if TOGETHER_AVAILABLE and os.getenv('TOGETHER_API_KEY'):
            together.api_key = os.getenv('TOGETHER_API_KEY')
            self.together_available = True
            logger.info("Together AI initialized")
        
        # ì‚¬ì§„ ë°ì´í„°ë² ì´ìŠ¤
        self.photos: Dict[str, PhotoMetadata] = {}
        self.embeddings = None
        self.photo_paths = []
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                              std=[0.229, 0.224, 0.225])
        ])
    
    def _init_classifier(self) -> nn.Module:
        """Transfer Learning ë¶„ë¥˜ê¸° ì´ˆê¸°í™”"""
        # ResNet50 ì‚¬ìš© (ImageNet pretrained)
        model = models.resnet50(pretrained=True)
        
        # Feature Extraction ëª¨ë“œ
        for param in model.parameters():
            param.requires_grad = False
        
        # ìƒˆë¡œìš´ ë¶„ë¥˜ ë ˆì´ì–´ (ì˜ˆ: 10ê°œ ì¹´í…Œê³ ë¦¬)
        num_features = model.fc.in_features
        model.fc = nn.Sequential(
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 10),
            nn.Softmax(dim=1)
        )
        
        model.to(self.device)
        model.eval()
        return model
    
    def add_photo(self, image_path: str) -> PhotoMetadata:
        """
        ì‚¬ì§„ì„ ì•¨ë²”ì— ì¶”ê°€
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìƒì„±ëœ PhotoMetadata
        """
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = PhotoMetadata(
            path=image_path,
            filename=Path(image_path).name,
            timestamp=time.time()
        )
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path).convert('RGB')
        
        # 1. CLIP ì„ë² ë”© ìƒì„±
        clip_inputs = self.clip_processor(images=image, return_tensors="pt")
        clip_inputs = {k: v.to(self.device) for k, v in clip_inputs.items()}
        
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(**clip_inputs)
            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
            metadata.embedding = image_features.cpu().numpy()
        
        # 2. ìë™ ìº¡ì…˜ ìƒì„±
        if self.gemini_model:
            try:
                prompt = "Describe this image in one detailed sentence."
                response = self.gemini_model.generate_content([prompt, image])
                metadata.caption = response.text
            except Exception as e:
                logger.warning(f"Caption generation failed: {e}")
        
        # 3. ìë™ íƒœê·¸ ìƒì„± (ë¶„ë¥˜ê¸° ì‚¬ìš©)
        image_tensor = self.transform(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            outputs = self.classifier(image_tensor)
            probs, indices = outputs.topk(3)
            
            # ì¹´í…Œê³ ë¦¬ ì´ë¦„ (ì˜ˆì‹œ)
            categories = ['nature', 'people', 'animals', 'food', 'buildings',
                         'vehicles', 'sports', 'art', 'technology', 'other']
            
            tags = []
            for idx, prob in zip(indices[0], probs[0]):
                if prob > 0.1:  # ì„ê³„ê°’
                    tags.append(categories[idx])
            
            metadata.tags = tags
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€
        self.photos[image_path] = metadata
        self.photo_paths.append(image_path)
        
        # ì„ë² ë”© ì—…ë°ì´íŠ¸
        if self.embeddings is None:
            self.embeddings = metadata.embedding
        else:
            self.embeddings = np.vstack([self.embeddings, metadata.embedding])
        
        return metadata
    
    def search_by_text(self, query: str, top_k: int = 5) -> List[Tuple[str, float, PhotoMetadata]]:
        """
        í…ìŠ¤íŠ¸ë¡œ ì‚¬ì§„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            (ì´ë¯¸ì§€ ê²½ë¡œ, ìœ ì‚¬ë„ ì ìˆ˜, ë©”íƒ€ë°ì´í„°) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.photos:
            return []
        
        # ì¿¼ë¦¬ ì¸ì½”ë”©
        text_inputs = self.clip_processor(text=[query], return_tensors="pt", padding=True)
        text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}
        
        with torch.no_grad():
            text_features = self.clip_model.get_text_features(**text_inputs)
            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        text_features_np = text_features.cpu().numpy()
        similarities = np.dot(self.embeddings, text_features_np.T).squeeze()
        
        # Top-K ì„ íƒ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            path = self.photo_paths[idx]
            score = similarities[idx]
            metadata = self.photos[path]
            results.append((path, score, metadata))
        
        return results
    
    def search_by_image(self, query_image_path: str, top_k: int = 5) -> List[Tuple[str, float, PhotoMetadata]]:
        """
        ì´ë¯¸ì§€ë¡œ ìœ ì‚¬í•œ ì‚¬ì§„ ê²€ìƒ‰
        
        Args:
            query_image_path: ì¿¼ë¦¬ ì´ë¯¸ì§€ ê²½ë¡œ
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            (ì´ë¯¸ì§€ ê²½ë¡œ, ìœ ì‚¬ë„ ì ìˆ˜, ë©”íƒ€ë°ì´í„°) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.photos:
            return []
        
        # ì¿¼ë¦¬ ì´ë¯¸ì§€ ì¸ì½”ë”©
        image = Image.open(query_image_path).convert('RGB')
        clip_inputs = self.clip_processor(images=image, return_tensors="pt")
        clip_inputs = {k: v.to(self.device) for k, v in clip_inputs.items()}
        
        with torch.no_grad():
            query_features = self.clip_model.get_image_features(**clip_inputs)
            query_features = query_features / query_features.norm(p=2, dim=-1, keepdim=True)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        query_features_np = query_features.cpu().numpy()
        similarities = np.dot(self.embeddings, query_features_np.T).squeeze()
        
        # ìê¸° ìì‹  ì œì™¸
        if query_image_path in self.photo_paths:
            self_idx = self.photo_paths.index(query_image_path)
            similarities[self_idx] = -1
        
        # Top-K ì„ íƒ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > -1:
                path = self.photo_paths[idx]
                score = similarities[idx]
                metadata = self.photos[path]
                results.append((path, score, metadata))
        
        return results
    
    def advanced_search(
        self,
        include_terms: List[str],
        exclude_terms: Optional[List[str]] = None,
        tags_filter: Optional[List[str]] = None,
        top_k: int = 5
    ) -> List[Tuple[str, float, PhotoMetadata]]:
        """
        ê³ ê¸‰ ê²€ìƒ‰: ë‹¤ì¤‘ ì¡°ê±´
        
        Args:
            include_terms: í¬í•¨í•´ì•¼ í•  ê²€ìƒ‰ì–´
            exclude_terms: ì œì™¸í•  ê²€ìƒ‰ì–´
            tags_filter: íƒœê·¸ í•„í„°
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.photos:
            return []
        
        scores = np.zeros(len(self.photo_paths))
        
        # Include terms ì²˜ë¦¬
        for term in include_terms:
            text_inputs = self.clip_processor(text=[term], return_tensors="pt", padding=True)
            text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}
            
            with torch.no_grad():
                text_features = self.clip_model.get_text_features(**text_inputs)
                text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
            
            text_features_np = text_features.cpu().numpy()
            similarities = np.dot(self.embeddings, text_features_np.T).squeeze()
            scores += similarities / len(include_terms)
        
        # Exclude terms ì²˜ë¦¬
        if exclude_terms:
            for term in exclude_terms:
                text_inputs = self.clip_processor(text=[term], return_tensors="pt", padding=True)
                text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}
                
                with torch.no_grad():
                    text_features = self.clip_model.get_text_features(**text_inputs)
                    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
                
                text_features_np = text_features.cpu().numpy()
                similarities = np.dot(self.embeddings, text_features_np.T).squeeze()
                scores -= similarities * 0.5 / len(exclude_terms)
        
        # Tag í•„í„°ë§
        if tags_filter:
            for idx, path in enumerate(self.photo_paths):
                metadata = self.photos[path]
                if metadata.tags:
                    if not any(tag in tags_filter for tag in metadata.tags):
                        scores[idx] = -float('inf')
        
        # Top-K ì„ íƒ
        top_indices = np.argsort(scores)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if scores[idx] > -float('inf'):
                path = self.photo_paths[idx]
                score = scores[idx]
                metadata = self.photos[path]
                results.append((path, score, metadata))
        
        return results


def create_gradio_app():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    album = SmartPhotoAlbum()
    
    def upload_photos(files):
        """ì‚¬ì§„ ì—…ë¡œë“œ ì²˜ë¦¬"""
        if not files:
            return "No files uploaded"
        
        results = []
        for file in files:
            try:
                metadata = album.add_photo(file.name)
                results.append(f"âœ“ {metadata.filename}")
                if metadata.caption:
                    results.append(f"  Caption: {metadata.caption[:100]}...")
                if metadata.tags:
                    results.append(f"  Tags: {', '.join(metadata.tags)}")
            except Exception as e:
                results.append(f"âœ— Error processing {file.name}: {e}")
        
        return "\n".join(results)
    
    def text_search(query, num_results):
        """í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        results = album.search_by_text(query, int(num_results))
        
        if not results:
            return None, "No results found"
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ì´ë¯¸ì§€
        top_image = Image.open(results[0][0])
        
        # ê²°ê³¼ ì •ë³´
        info = []
        for path, score, metadata in results:
            info.append(f"ğŸ“· {metadata.filename}")
            info.append(f"   Score: {score:.3f}")
            if metadata.caption:
                info.append(f"   Caption: {metadata.caption[:100]}...")
            if metadata.tags:
                info.append(f"   Tags: {', '.join(metadata.tags)}")
            info.append("")
        
        return top_image, "\n".join(info)
    
    def image_search(query_image, num_results):
        """ì´ë¯¸ì§€ ê²€ìƒ‰"""
        if query_image is None:
            return None, "Please upload an image"
        
        # ì„ì‹œ ì €ì¥
        temp_path = "temp_query.jpg"
        query_image.save(temp_path)
        
        results = album.search_by_image(temp_path, int(num_results))
        
        if not results:
            return None, "No similar images found"
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ì´ë¯¸ì§€
        top_image = Image.open(results[0][0])
        
        # ê²°ê³¼ ì •ë³´
        info = []
        for path, score, metadata in results:
            info.append(f"ğŸ“· {metadata.filename}")
            info.append(f"   Similarity: {score:.3f}")
            if metadata.caption:
                info.append(f"   Caption: {metadata.caption[:100]}...")
            info.append("")
        
        return top_image, "\n".join(info)
    
    def advanced_search_fn(include_terms, exclude_terms, tag_filter, num_results):
        """ê³ ê¸‰ ê²€ìƒ‰"""
        include_list = [t.strip() for t in include_terms.split(',') if t.strip()]
        exclude_list = [t.strip() for t in exclude_terms.split(',') if t.strip()] if exclude_terms else None
        tag_list = [t.strip() for t in tag_filter.split(',') if t.strip()] if tag_filter else None
        
        results = album.advanced_search(
            include_list,
            exclude_list,
            tag_list,
            int(num_results)
        )
        
        if not results:
            return None, "No results found"
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ì´ë¯¸ì§€
        top_image = Image.open(results[0][0])
        
        # ê²°ê³¼ ì •ë³´
        info = []
        for path, score, metadata in results:
            info.append(f"ğŸ“· {metadata.filename}")
            info.append(f"   Score: {score:.3f}")
            if metadata.tags:
                info.append(f"   Tags: {', '.join(metadata.tags)}")
            info.append("")
        
        return top_image, "\n".join(info)
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title="Smart Photo Album") as app:
        gr.Markdown("""
        # ğŸ–¼ï¸ Smart Photo Album
        ### Transfer Learning + CLIP + Multimodal APIë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì‚¬ì§„ì²©
        """)
        
        with gr.Tab("ğŸ“¤ Upload Photos"):
            file_upload = gr.File(label="Select photos", file_count="multiple", file_types=["image"])
            upload_btn = gr.Button("Upload and Process", variant="primary")
            upload_output = gr.Textbox(label="Processing Results", lines=10)
            
            upload_btn.click(upload_photos, inputs=[file_upload], outputs=[upload_output])
        
        with gr.Tab("ğŸ” Text Search"):
            gr.Markdown("ìì—°ì–´ë¡œ ì‚¬ì§„ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤")
            text_query = gr.Textbox(label="Search Query", placeholder="e.g., 'sunset at beach', 'happy people'")
            num_results_text = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="Number of Results")
            search_text_btn = gr.Button("Search", variant="primary")
            
            with gr.Row():
                text_result_image = gr.Image(label="Top Result", type="pil")
                text_result_info = gr.Textbox(label="Search Results", lines=15)
            
            search_text_btn.click(
                text_search,
                inputs=[text_query, num_results_text],
                outputs=[text_result_image, text_result_info]
            )
        
        with gr.Tab("ğŸ–¼ï¸ Image Search"):
            gr.Markdown("ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤")
            query_image = gr.Image(label="Upload Query Image", type="pil")
            num_results_image = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="Number of Results")
            search_image_btn = gr.Button("Find Similar", variant="primary")
            
            with gr.Row():
                image_result_image = gr.Image(label="Top Similar", type="pil")
                image_result_info = gr.Textbox(label="Similar Images", lines=15)
            
            search_image_btn.click(
                image_search,
                inputs=[query_image, num_results_image],
                outputs=[image_result_image, image_result_info]
            )
        
        with gr.Tab("âš™ï¸ Advanced Search"):
            gr.Markdown("ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜")
            include_input = gr.Textbox(label="Include Terms (comma-separated)", 
                                      placeholder="sunset, beach, ocean")
            exclude_input = gr.Textbox(label="Exclude Terms (comma-separated)", 
                                      placeholder="people, buildings")
            tag_input = gr.Textbox(label="Tag Filter (comma-separated)", 
                                  placeholder="nature, landscape")
            num_results_adv = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="Number of Results")
            search_adv_btn = gr.Button("Advanced Search", variant="primary")
            
            with gr.Row():
                adv_result_image = gr.Image(label="Top Result", type="pil")
                adv_result_info = gr.Textbox(label="Search Results", lines=15)
            
            search_adv_btn.click(
                advanced_search_fn,
                inputs=[include_input, exclude_input, tag_input, num_results_adv],
                outputs=[adv_result_image, adv_result_info]
            )
        
        with gr.Tab("ğŸ“Š Album Stats"):
            stats_btn = gr.Button("Show Statistics", variant="primary")
            stats_output = gr.Textbox(label="Album Statistics", lines=10)
            
            def show_stats():
                stats = []
                stats.append(f"Total Photos: {len(album.photos)}")
                
                if album.photos:
                    # íƒœê·¸ í†µê³„
                    all_tags = []
                    for metadata in album.photos.values():
                        if metadata.tags:
                            all_tags.extend(metadata.tags)
                    
                    if all_tags:
                        from collections import Counter
                        tag_counts = Counter(all_tags)
                        stats.append("\nTop Tags:")
                        for tag, count in tag_counts.most_common(5):
                            stats.append(f"  â€¢ {tag}: {count}")
                    
                    # ìº¡ì…˜ ìˆëŠ” ì‚¬ì§„ ìˆ˜
                    caption_count = sum(1 for m in album.photos.values() if m.caption)
                    stats.append(f"\nPhotos with Captions: {caption_count}")
                
                return "\n".join(stats)
            
            stats_btn.click(show_stats, outputs=[stats_output])
    
    return app


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not os.getenv('GEMINI_API_KEY'):
        print("Warning: GEMINI_API_KEY not set. Caption generation will be disabled.")
    
    # ì•± ì‹¤í–‰
    app = create_gradio_app()
    app.launch(share=True, debug=True)