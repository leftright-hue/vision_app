"""
Week 2: CNN ì›ë¦¬ + Hugging Face ìƒíƒœê³„ ì‹¤ìŠµ
ë”¥ëŸ¬ë‹ ì˜ìƒì²˜ë¦¬ ê°•ì˜ - 2ì£¼ì°¨ ì‹¤ìŠµ ì½”ë“œ

ì‹¤ìŠµ ëª©í‘œ:
1. CNN ì•„í‚¤í…ì²˜ì˜ ìˆ˜ë™ êµ¬í˜„ ë° ì´í•´
2. Hugging Face Serverless Inference API í™œìš©
3. ì‚¬ì „í›ˆë ¨ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (CLIP, BERT, ViT)
4. Gradioë¥¼ í†µí•œ ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import requests
import json
import io
import os
from dotenv import load_dotenv
from transformers import (
    CLIPProcessor, CLIPModel,
    BertTokenizer, BertModel,
    ViTImageProcessor, ViTForImageClassification
)
import gradio as gr

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv('HF_TOKEN')

class CNNLab:
    """CNN ì‹¤ìŠµì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
    def visualize_convolution(self):
        """Convolution ì—°ì‚° ê³¼ì • ì‹œê°í™”"""
        print("=== Convolution ì—°ì‚° ì‹œê°í™” ===")
        
        # ì…ë ¥ ì´ë¯¸ì§€ ìƒì„± (5x5)
        input_img = torch.tensor([
            [1, 2, 3, 4, 5],
            [6, 7, 8, 9, 10],
            [11, 12, 13, 14, 15],
            [16, 17, 18, 19, 20],
            [21, 22, 23, 24, 25]
        ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        # ì—£ì§€ ê²€ì¶œ ì»¤ë„
        edge_kernel = torch.tensor([
            [-1, -1, -1],
            [-1,  8, -1],
            [-1, -1, -1]
        ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        # Convolution ì—°ì‚°
        output = F.conv2d(input_img, edge_kernel, padding=1)
        
        # ê²°ê³¼ ì¶œë ¥
        print("ì…ë ¥ ì´ë¯¸ì§€:")
        print(input_img.squeeze())
        print("\nì—£ì§€ ê²€ì¶œ ì»¤ë„:")
        print(edge_kernel.squeeze())
        print("\nConvolution ê²°ê³¼:")
        print(output.squeeze())
        
        return input_img, edge_kernel, output
    
    def build_simple_cnn(self):
        """ê°„ë‹¨í•œ CNN ëª¨ë¸ êµ¬ì¶•"""
        print("\n=== ê°„ë‹¨í•œ CNN ëª¨ë¸ êµ¬ì¶• ===")
        
        class SimpleCNN(nn.Module):
            def __init__(self):
                super(SimpleCNN, self).__init__()
                self.conv1 = nn.Conv2d(1, 6, 5)  # 1ì±„ë„ â†’ 6ì±„ë„, 5x5 ì»¤ë„
                self.conv2 = nn.Conv2d(6, 16, 5)  # 6ì±„ë„ â†’ 16ì±„ë„, 5x5 ì»¤ë„
                self.pool = nn.MaxPool2d(2, 2)    # 2x2 MaxPooling
                self.fc1 = nn.Linear(16 * 4 * 4, 120)
                self.fc2 = nn.Linear(120, 84)
                self.fc3 = nn.Linear(84, 10)
                
            def forward(self, x):
                # ì²« ë²ˆì§¸ Convolution + Pooling
                x = self.pool(F.relu(self.conv1(x)))
                
                # ë‘ ë²ˆì§¸ Convolution + Pooling
                x = self.pool(F.relu(self.conv2(x)))
                
                # Flatten
                x = x.view(-1, 16 * 4 * 4)
                
                # Fully Connected Layers
                x = F.relu(self.fc1(x))
                x = F.relu(self.fc2(x))
                x = self.fc3(x)
                
                return x
        
        model = SimpleCNN().to(self.device)
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        
        # í…ŒìŠ¤íŠ¸
        dummy_input = torch.randn(1, 1, 28, 28).to(self.device)
        output = model(dummy_input)
        print(f"ì…ë ¥ í¬ê¸°: {dummy_input.shape}")
        print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")
        
        return model
    
    def demonstrate_backpropagation(self, model):
        """ì—­ì „íŒŒ ê³¼ì • ì‹œì—°"""
        print("\n=== ì—­ì „íŒŒ ê³¼ì • ì‹œì—° ===")
        
        # ê°€ìƒì˜ ì…ë ¥ ë°ì´í„°
        dummy_input = torch.randn(1, 1, 28, 28).to(self.device)
        target = torch.randint(0, 10, (1,)).to(self.device)
        
        # Forward pass
        output = model(dummy_input)
        
        # Loss ê³„ì‚°
        criterion = nn.CrossEntropyLoss()
        loss = criterion(output, target)
        
        print(f"ì´ˆê¸° Loss: {loss.item():.4f}")
        
        # Backward pass
        loss.backward()
        
        # Gradient í™•ì¸
        print(f"Conv1 weight gradient í¬ê¸°: {model.conv1.weight.grad.shape}")
        print(f"Conv2 weight gradient í¬ê¸°: {model.conv2.weight.grad.shape}")
        print(f"FC1 weight gradient í¬ê¸°: {model.fc1.weight.grad.shape}")
        
        return loss

class HuggingFaceLab:
    """Hugging Face API ì‹¤ìŠµì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        if not HF_TOKEN:
            print("âš ï¸ HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("Hugging Faceì—ì„œ í† í°ì„ ìƒì„±í•˜ê³  .env íŒŒì¼ì— ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            print("âœ… Hugging Face í† í°ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def test_serverless_inference(self):
        """Serverless Inference API í…ŒìŠ¤íŠ¸"""
        if not HF_TOKEN:
            print("í† í°ì´ ì—†ì–´ API í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        print("\n=== Hugging Face Serverless Inference API í…ŒìŠ¤íŠ¸ ===")
        
        # ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ API
        API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±
        sample_image = Image.new('RGB', (224, 224), color='red')
        img_byte_arr = io.BytesIO()
        sample_image.save(img_byte_arr, format='PNG')
        img_byte_arr = img_byte_arr.getvalue()
        
        try:
            # API í˜¸ì¶œ
            response = requests.post(API_URL, headers=headers, data=img_byte_arr)
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… ì´ë¯¸ì§€ ë¶„ë¥˜ ê²°ê³¼:")
                for i, item in enumerate(result[:3]):
                    print(f"  {i+1}. {item['label']}: {item['score']:.4f}")
                return result
            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def test_clip_model(self):
        """CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n=== CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model_name = "openai/clip-vit-base-patch32"
            processor = CLIPProcessor.from_pretrained(model_name)
            model = CLIPModel.from_pretrained(model_name)
            
            # ìƒ˜í”Œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸
            image = Image.new('RGB', (224, 224), color='blue')
            texts = ["a red car", "a blue car", "a dog", "a cat", "a building"]
            
            # ì „ì²˜ë¦¬
            inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)
            
            # ê²°ê³¼ ì¶œë ¥
            print("CLIP í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë§¤ì¹­ ê²°ê³¼:")
            for text, prob in zip(texts, probs[0]):
                print(f"  {text}: {prob:.4f}")
            
            return model, processor
            
        except Exception as e:
            print(f"âŒ CLIP ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None, None
    
    def test_bert_model(self):
        """BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n=== BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model_name = "bert-base-uncased"
            tokenizer = BertTokenizer.from_pretrained(model_name)
            model = BertModel.from_pretrained(model_name)
            
            # ìƒ˜í”Œ í…ìŠ¤íŠ¸
            text = "I love computer vision and deep learning!"
            
            # í† í°í™”
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = model(**inputs)
                last_hidden_states = outputs.last_hidden_state
            
            print(f"BERT ì…ë ¥ í…ìŠ¤íŠ¸: {text}")
            print(f"ì¶œë ¥ í…ì„œ í¬ê¸°: {last_hidden_states.shape}")
            print(f"ì„ë² ë”© ì°¨ì›: {last_hidden_states.shape[-1]}")
            
            return model, tokenizer
            
        except Exception as e:
            print(f"âŒ BERT ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None, None
    
    def test_vit_model(self):
        """Vision Transformer ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n=== Vision Transformer ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model_name = "google/vit-base-patch16-224"
            processor = ViTImageProcessor.from_pretrained(model_name)
            model = ViTForImageClassification.from_pretrained(model_name)
            
            # ìƒ˜í”Œ ì´ë¯¸ì§€
            image = Image.new('RGB', (224, 224), color='green')
            
            # ì „ì²˜ë¦¬
            inputs = processor(images=image, return_tensors="pt")
            
            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                predicted_class = logits.argmax(-1).item()
            
            # ê²°ê³¼
            predicted_label = model.config.id2label[predicted_class]
            print(f"ViT ì˜ˆì¸¡ ê²°ê³¼: {predicted_label}")
            print(f"í´ë˜ìŠ¤ ID: {predicted_class}")
            
            return model, processor
            
        except Exception as e:
            print(f"âŒ ViT ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None, None

class GradioLab:
    """Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤ìŠµì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.vit_model = None
        self.vit_processor = None
    
    def load_models(self):
        """í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ë¡œë“œ"""
        print("\n=== Gradio ì•±ì„ ìœ„í•œ ëª¨ë¸ ë¡œë“œ ===")
        
        try:
            model_name = "google/vit-base-patch16-224"
            self.vit_processor = ViTImageProcessor.from_pretrained(model_name)
            self.vit_model = ViTForImageClassification.from_pretrained(model_name)
            print("âœ… ViT ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def classify_image(self, image):
        """ì´ë¯¸ì§€ ë¶„ë¥˜ í•¨ìˆ˜"""
        if image is None:
            return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        if self.vit_model is None or self.vit_processor is None:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
            
            # ViT ëª¨ë¸ë¡œ ë¶„ë¥˜
            inputs = self.vit_processor(images=image, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.vit_model(**inputs)
                logits = outputs.logits
                probs = torch.nn.functional.softmax(logits, dim=-1)
                
                # ìƒìœ„ 5ê°œ ê²°ê³¼
                top5_probs, top5_indices = torch.topk(probs, 5)
            
            results = []
            for prob, idx in zip(top5_probs[0], top5_indices[0]):
                label = self.vit_model.config.id2label[idx.item()]
                results.append(f"{label}: {prob:.4f}")
            
            return "\n".join(results)
            
        except Exception as e:
            return f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    
    def create_gradio_app(self):
        """Gradio ì•± ìƒì„±"""
        print("\n=== Gradio ì•± ìƒì„± ===")
        
        if self.vit_model is None:
            self.load_models()
        
        # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        iface = gr.Interface(
            fn=self.classify_image,
            inputs=gr.Image(type="pil"),
            outputs=gr.Textbox(label="ë¶„ë¥˜ ê²°ê³¼", lines=5),
            title="ğŸ–¼ï¸ AI ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°",
            description="ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ AIê°€ ë¬´ì—‡ì¸ì§€ ë¶„ë¥˜í•´ë“œë¦½ë‹ˆë‹¤.",
            examples=[
                ["sample1.jpg"],
                ["sample2.jpg"],
                ["sample3.jpg"]
            ],
            theme=gr.themes.Soft()
        )
        
        print("âœ… Gradio ì•±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return iface

def main():
    """ë©”ì¸ ì‹¤ìŠµ í•¨ìˆ˜"""
    print("ğŸš€ Week 2: CNN ì›ë¦¬ + Hugging Face ìƒíƒœê³„ ì‹¤ìŠµ ì‹œì‘")
    print("=" * 60)
    
    # 1. CNN ì‹¤ìŠµ
    print("\nğŸ“š 1ë‹¨ê³„: CNN ì›ë¦¬ ì‹¤ìŠµ")
    cnn_lab = CNNLab()
    
    # Convolution ì‹œê°í™”
    input_img, kernel, output = cnn_lab.visualize_convolution()
    
    # CNN ëª¨ë¸ êµ¬ì¶•
    model = cnn_lab.build_simple_cnn()
    
    # ì—­ì „íŒŒ ì‹œì—°
    loss = cnn_lab.demonstrate_backpropagation(model)
    
    # 2. Hugging Face ì‹¤ìŠµ
    print("\nğŸ”§ 2ë‹¨ê³„: Hugging Face API ì‹¤ìŠµ")
    hf_lab = HuggingFaceLab()
    
    # Serverless Inference API í…ŒìŠ¤íŠ¸
    api_result = hf_lab.test_serverless_inference()
    
    # ì‚¬ì „í›ˆë ¨ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    clip_model, clip_processor = hf_lab.test_clip_model()
    bert_model, bert_tokenizer = hf_lab.test_bert_model()
    vit_model, vit_processor = hf_lab.test_vit_model()
    
    # 3. Gradio ì‹¤ìŠµ
    print("\nğŸŒ 3ë‹¨ê³„: Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹¤ìŠµ")
    gradio_lab = GradioLab()
    app = gradio_lab.create_gradio_app()
    
    print("\n" + "=" * 60)
    print("âœ… Week 2 ì‹¤ìŠµ ì™„ë£Œ!")
    print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. app.launch()ë¡œ Gradio ì•± ì‹¤í–‰")
    print("2. Hugging Face Spaceì— ë°°í¬")
    print("3. ì¶”ê°€ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë¹„êµ")
    
    return app

if __name__ == "__main__":
    app = main()
    
    # Gradio ì•± ì‹¤í–‰ (ì„ íƒì‚¬í•­)
    # app.launch(share=True)
