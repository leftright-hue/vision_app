"""
Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸
Week 2: CNN ì›ë¦¬ + Hugging Face ìƒíƒœê³„

ì´ íŒŒì¼ì€ Hugging Faceì˜ ë‹¤ì–‘í•œ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ë“¤ì„ í…ŒìŠ¤íŠ¸í•˜ê³ 
ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont
import requests
import json
import io
import os
import time
from dotenv import load_dotenv
from transformers import (
    CLIPProcessor, CLIPModel,
    BertTokenizer, BertModel,
    ViTImageProcessor, ViTForImageClassification,
    AutoTokenizer, AutoModel,
    pipeline
)
import gradio as gr

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv('HF_TOKEN')

class HuggingFaceModelTester:
    """Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ìºì‹œ
        self.models = {}
        self.processors = {}
        
    def create_test_images(self):
        """í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ìƒì„±"""
        images = {}
        
        # 1. ë¹¨ê°„ìƒ‰ ì´ë¯¸ì§€
        red_img = Image.new('RGB', (224, 224), color='red')
        images['red'] = red_img
        
        # 2. íŒŒë€ìƒ‰ ì´ë¯¸ì§€
        blue_img = Image.new('RGB', (224, 224), color='blue')
        images['blue'] = blue_img
        
        # 3. ë…¹ìƒ‰ ì´ë¯¸ì§€
        green_img = Image.new('RGB', (224, 224), color='green')
        images['green'] = green_img
        
        # 4. ê·¸ë¼ë°ì´ì…˜ ì´ë¯¸ì§€
        gradient_img = Image.new('RGB', (224, 224))
        draw = ImageDraw.Draw(gradient_img)
        for i in range(224):
            color = int(255 * i / 224)
            draw.line([(i, 0), (i, 224)], fill=(color, color, color))
        images['gradient'] = gradient_img
        
        # 5. ì²´í¬ë¬´ëŠ¬ ì´ë¯¸ì§€
        checker_img = Image.new('RGB', (224, 224))
        draw = ImageDraw.Draw(checker_img)
        for i in range(0, 224, 28):
            for j in range(0, 224, 28):
                color = (255, 255, 255) if (i + j) % 56 == 0 else (0, 0, 0)
                draw.rectangle([i, j, i+28, j+28], fill=color)
        images['checker'] = checker_img
        
        return images
    
    def test_clip_model(self, images):
        """CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n=== CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model_name = "openai/clip-vit-base-patch32"
            processor = CLIPProcessor.from_pretrained(model_name)
            model = CLIPModel.from_pretrained(model_name)
            
            # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
            test_texts = [
                "a red object",
                "a blue object", 
                "a green object",
                "a black and white pattern",
                "a gradient image",
                "a colorful image",
                "a simple shape",
                "a complex pattern"
            ]
            
            results = {}
            
            for img_name, image in images.items():
                print(f"\nì´ë¯¸ì§€: {img_name}")
                
                # ì „ì²˜ë¦¬
                inputs = processor(text=test_texts, images=image, return_tensors="pt", padding=True)
                
                # ì¶”ë¡ 
                with torch.no_grad():
                    outputs = model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # ê²°ê³¼ ì €ì¥
                results[img_name] = {}
                for text, prob in zip(test_texts, probs[0]):
                    results[img_name][text] = prob.item()
                    print(f"  {text}: {prob:.4f}")
            
            # ëª¨ë¸ ìºì‹œì— ì €ì¥
            self.models['clip'] = model
            self.processors['clip'] = processor
            
            return results
            
        except Exception as e:
            print(f"âŒ CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return None
    
    def test_bert_model(self):
        """BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n=== BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model_name = "bert-base-uncased"
            tokenizer = BertTokenizer.from_pretrained(model_name)
            model = BertModel.from_pretrained(model_name)
            
            # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
            test_texts = [
                "I love computer vision and deep learning!",
                "The image shows a beautiful landscape.",
                "This is a red car parked on the street.",
                "The cat is sitting on the windowsill.",
                "Artificial intelligence is transforming our world."
            ]
            
            results = {}
            
            for text in test_texts:
                print(f"\ní…ìŠ¤íŠ¸: {text}")
                
                # í† í°í™”
                inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
                
                # ì¶”ë¡ 
                with torch.no_grad():
                    outputs = model(**inputs)
                    last_hidden_states = outputs.last_hidden_state
                    pooled_output = outputs.pooler_output
                
                # ê²°ê³¼ ì €ì¥
                results[text] = {
                    'hidden_states_shape': last_hidden_states.shape,
                    'pooled_output_shape': pooled_output.shape,
                    'embedding_dim': last_hidden_states.shape[-1],
                    'sequence_length': last_hidden_states.shape[1]
                }
                
                print(f"  Hidden states shape: {last_hidden_states.shape}")
                print(f"  Pooled output shape: {pooled_output.shape}")
                print(f"  Embedding dimension: {last_hidden_states.shape[-1]}")
            
            # ëª¨ë¸ ìºì‹œì— ì €ì¥
            self.models['bert'] = model
            self.processors['bert'] = tokenizer
            
            return results
            
        except Exception as e:
            print(f"âŒ BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return None
    
    def test_vit_model(self, images):
        """Vision Transformer ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n=== Vision Transformer ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model_name = "google/vit-base-patch16-224"
            processor = ViTImageProcessor.from_pretrained(model_name)
            model = ViTForImageClassification.from_pretrained(model_name)
            
            results = {}
            
            for img_name, image in images.items():
                print(f"\nì´ë¯¸ì§€: {img_name}")
                
                # ì „ì²˜ë¦¬
                inputs = processor(images=image, return_tensors="pt")
                
                # ì¶”ë¡ 
                with torch.no_grad():
                    outputs = model(**inputs)
                    logits = outputs.logits
                    probs = torch.nn.functional.softmax(logits, dim=-1)
                    
                    # ìƒìœ„ 5ê°œ ê²°ê³¼
                    top5_probs, top5_indices = torch.topk(probs, 5)
                
                # ê²°ê³¼ ì €ì¥
                results[img_name] = {}
                for prob, idx in zip(top5_probs[0], top5_indices[0]):
                    label = model.config.id2label[idx.item()]
                    results[img_name][label] = prob.item()
                    print(f"  {label}: {prob:.4f}")
            
            # ëª¨ë¸ ìºì‹œì— ì €ì¥
            self.models['vit'] = model
            self.processors['vit'] = processor
            
            return results
            
        except Exception as e:
            print(f"âŒ ViT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return None
    
    def test_serverless_inference(self, images):
        """Serverless Inference API í…ŒìŠ¤íŠ¸"""
        if not HF_TOKEN:
            print("âš ï¸ HF_TOKENì´ ì—†ì–´ Serverless Inference í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        print("\n=== Serverless Inference API í…ŒìŠ¤íŠ¸ ===")
        
        # ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ API
        API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
        headers = {"Authorization": f"Bearer {HF_TOKEN}"}
        
        results = {}
        
        for img_name, image in images.items():
            print(f"\nì´ë¯¸ì§€: {img_name}")
            
            # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
            img_byte_arr = io.BytesIO()
            image.save(img_byte_arr, format='PNG')
            img_byte_arr = img_byte_arr.getvalue()
            
            try:
                # API í˜¸ì¶œ
                start_time = time.time()
                response = requests.post(API_URL, headers=headers, data=img_byte_arr)
                end_time = time.time()
                
                if response.status_code == 200:
                    result = response.json()
                    results[img_name] = {
                        'predictions': result[:3],  # ìƒìœ„ 3ê°œ ê²°ê³¼
                        'response_time': end_time - start_time
                    }
                    
                    print(f"  ì‘ë‹µ ì‹œê°„: {end_time - start_time:.3f}ì´ˆ")
                    for i, item in enumerate(result[:3]):
                        print(f"  {i+1}. {item['label']}: {item['score']:.4f}")
                else:
                    print(f"  âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                    
            except Exception as e:
                print(f"  âŒ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return results
    
    def compare_model_performance(self, clip_results, vit_results, serverless_results):
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        print("\n=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===")
        
        if not clip_results or not vit_results:
            print("ëª¨ë¸ ê²°ê³¼ê°€ ì—†ì–´ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # ê³µí†µ ì´ë¯¸ì§€ì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµ
        common_images = set(clip_results.keys()) & set(vit_results.keys())
        
        for img_name in common_images:
            print(f"\nì´ë¯¸ì§€: {img_name}")
            
            # CLIP ê²°ê³¼ (ê°€ì¥ ë†’ì€ í™•ë¥ )
            if img_name in clip_results:
                clip_max_prob = max(clip_results[img_name].values())
                print(f"  CLIP ìµœê³  í™•ë¥ : {clip_max_prob:.4f}")
            
            # ViT ê²°ê³¼ (ê°€ì¥ ë†’ì€ í™•ë¥ )
            if img_name in vit_results:
                vit_max_prob = max(vit_results[img_name].values())
                print(f"  ViT ìµœê³  í™•ë¥ : {vit_max_prob:.4f}")
            
            # Serverless API ê²°ê³¼
            if serverless_results and img_name in serverless_results:
                serverless_max_prob = max(item['score'] for item in serverless_results[img_name]['predictions'])
                response_time = serverless_results[img_name]['response_time']
                print(f"  Serverless ìµœê³  í™•ë¥ : {serverless_max_prob:.4f} (ì‘ë‹µì‹œê°„: {response_time:.3f}ì´ˆ)")
    
    def create_model_comparison_visualization(self, clip_results, vit_results):
        """ëª¨ë¸ ë¹„êµ ì‹œê°í™”"""
        if not clip_results or not vit_results:
            return
        
        # ê³µí†µ ì´ë¯¸ì§€ ì°¾ê¸°
        common_images = list(set(clip_results.keys()) & set(vit_results.keys()))
        
        if len(common_images) == 0:
            return
        
        # ì‹œê°í™” ë°ì´í„° ì¤€ë¹„
        fig, axes = plt.subplots(2, len(common_images), figsize=(15, 8))
        
        for i, img_name in enumerate(common_images):
            # CLIP ê²°ê³¼
            if img_name in clip_results:
                clip_probs = list(clip_results[img_name].values())
                clip_labels = list(clip_results[img_name].keys())
                
                axes[0, i].barh(range(len(clip_probs)), clip_probs)
                axes[0, i].set_yticks(range(len(clip_labels)))
                axes[0, i].set_yticklabels([label[:15] + '...' if len(label) > 15 else label for label in clip_labels])
                axes[0, i].set_title(f'CLIP - {img_name}')
                axes[0, i].set_xlim(0, 1)
            
            # ViT ê²°ê³¼
            if img_name in vit_results:
                vit_probs = list(vit_results[img_name].values())
                vit_labels = list(vit_results[img_name].keys())
                
                axes[1, i].barh(range(len(vit_probs)), vit_probs)
                axes[1, i].set_yticks(range(len(vit_labels)))
                axes[1, i].set_yticklabels([label[:15] + '...' if len(label) > 15 else label for label in vit_labels])
                axes[1, i].set_title(f'ViT - {img_name}')
                axes[1, i].set_xlim(0, 1)
        
        plt.tight_layout()
        plt.show()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = HuggingFaceModelTester()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    print("\n1. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±")
    images = tester.create_test_images()
    print(f"ìƒì„±ëœ ì´ë¯¸ì§€: {list(images.keys())}")
    
    # 2. CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n2. CLIP ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    clip_results = tester.test_clip_model(images)
    
    # 3. BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n3. BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    bert_results = tester.test_bert_model()
    
    # 4. ViT ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n4. ViT ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    vit_results = tester.test_vit_model(images)
    
    # 5. Serverless Inference API í…ŒìŠ¤íŠ¸
    print("\n5. Serverless Inference API í…ŒìŠ¤íŠ¸")
    serverless_results = tester.test_serverless_inference(images)
    
    # 6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    print("\n6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    tester.compare_model_performance(clip_results, vit_results, serverless_results)
    
    # 7. ì‹œê°í™”
    print("\n7. ëª¨ë¸ ë¹„êµ ì‹œê°í™”")
    tester.create_model_comparison_visualization(clip_results, vit_results)
    
    print("\nâœ… Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"- CLIP ëª¨ë¸: {'ì„±ê³µ' if clip_results else 'ì‹¤íŒ¨'}")
    print(f"- BERT ëª¨ë¸: {'ì„±ê³µ' if bert_results else 'ì‹¤íŒ¨'}")
    print(f"- ViT ëª¨ë¸: {'ì„±ê³µ' if vit_results else 'ì‹¤íŒ¨'}")
    print(f"- Serverless API: {'ì„±ê³µ' if serverless_results else 'ì‹¤íŒ¨'}")
    
    return tester

if __name__ == "__main__":
    tester = main()
