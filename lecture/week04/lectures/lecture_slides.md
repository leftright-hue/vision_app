# Week 4: Vision Transformer + ìµœì‹  ëª¨ë¸ ë¹„êµ

## ê°•ì˜ ìŠ¬ë¼ì´ë“œ

---

# ğŸ“š 4ì£¼ì°¨ í•™ìŠµ ëª©í‘œ

## ì˜¤ëŠ˜ ë°°ìš¸ ë‚´ìš©

1. **Self-Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ì´í•´**
2. **Vision Transformer (ViT) ì•„í‚¤í…ì²˜**
3. **DINOì™€ ìê¸°ì§€ë„í•™ìŠµ**
4. **SAMê³¼ ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜**
5. **ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬**

---

# Part 1: Self-Attention ë©”ì»¤ë‹ˆì¦˜

## ğŸ§  Attention is All You Need

### í•µì‹¬ ì•„ì´ë””ì–´
> "ëª¨ë“  ìœ„ì¹˜ê°€ ë‹¤ë¥¸ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆë‹¤"

### CNN vs Attention
- **CNN**: Local receptive field â†’ ì ì§„ì  í™•ì¥
- **Attention**: Global receptive field from the start

### ìˆ˜ì‹ í‘œí˜„
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

---

## ğŸ“ Self-Attention ê³„ì‚° ê³¼ì •

### Step 1: Query, Key, Value ìƒì„±
```python
# ì…ë ¥ Xì—ì„œ Q, K, V ê³„ì‚°
Q = X @ W_q  # Query
K = X @ W_k  # Key  
V = X @ W_v  # Value
```

### Step 2: Attention Score ê³„ì‚°
```python
# ìœ ì‚¬ë„ ê³„ì‚°
scores = Q @ K.T / sqrt(d_k)
attention_weights = softmax(scores)
```

### Step 3: Weighted Sum
```python
# ê°€ì¤‘í•© ê³„ì‚°
output = attention_weights @ V
```

---

## ğŸ¯ Multi-Head Attention

### ì™œ Multi-Headì¸ê°€?

```
Single Head: í•˜ë‚˜ì˜ ê´€ì 
Multi-Head: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— í•™ìŠµ
```

### ë³‘ë ¬ ì²˜ë¦¬ì˜ ì¥ì 
- **Head 1**: í…ìŠ¤ì²˜ íŒ¨í„´ í•™ìŠµ
- **Head 2**: ì—£ì§€ ì •ë³´ í¬ì°©
- **Head 3**: ìƒ‰ìƒ ê´€ê³„ ëª¨ë¸ë§
- **Head N**: ê³ ìˆ˜ì¤€ ì˜ë¯¸ ì •ë³´

### êµ¬í˜„
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        self.heads = n_heads
        self.d_k = d_model // n_heads
        
    def forward(self, x):
        # Split into multiple heads
        # Apply attention per head
        # Concatenate results
```

---

# Part 2: Vision Transformer (ViT)

## ğŸ–¼ï¸ ì´ë¯¸ì§€ë¥¼ ì‹œí€€ìŠ¤ë¡œ

### í•µì‹¬ í˜ì‹ 
"An Image is Worth 16x16 Words"

### íŒ¨ì¹˜ ë¶„í•  ê³¼ì •
```
Original: 224Ã—224Ã—3
    â†“
Patches: 14Ã—14 patches of 16Ã—16Ã—3
    â†“
Flatten: 196 patches Ã— 768 dims
    â†“
Add [CLS] token: 197 Ã— 768
```

### ìœ„ì¹˜ ì¸ì½”ë”©ì˜ ì¤‘ìš”ì„±
- íŒ¨ì¹˜ì˜ ê³µê°„ì  ìœ„ì¹˜ ì •ë³´ ë³´ì¡´
- Learnable vs Sinusoidal encoding

---

## ğŸ—ï¸ ViT ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°
```
Input Image
    â†“
Patch Embedding + Position Encoding
    â†“
Transformer Encoder Ã— L
    â†“
MLP Head
    â†“
Class Prediction
```

### ë ˆì´ì–´ êµ¬ì„±
```python
class ViTBlock(nn.Module):
    def __init__(self):
        self.norm1 = LayerNorm()
        self.attn = MultiHeadAttention()
        self.norm2 = LayerNorm()
        self.mlp = MLP()
    
    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x
```

---

## ğŸ“Š ViT vs CNN ì„±ëŠ¥ ë¹„êµ

### ë°ì´í„° íš¨ìœ¨ì„±

| Dataset Size | CNN (ResNet) | ViT-B/16 |
|-------------|--------------|----------|
| 10K images | 70% | 55% |
| 100K images | 78% | 73% |
| 1M images | 82% | 84% |
| 14M images | 84% | 88% |

### Key Insights
- **Small data**: CNNì´ ìš°ì„¸ (inductive bias)
- **Large data**: ViTê°€ ìš°ì„¸ (flexibility)
- **Pre-training**: ViTì— í•„ìˆ˜ì 

---

## ğŸš€ ViT ë³€í˜• ëª¨ë¸ë“¤

### Model Zoo

```
ViT-Tiny:   12 layers, 192 dim, 3 heads
ViT-Small:  12 layers, 384 dim, 6 heads  
ViT-Base:   12 layers, 768 dim, 12 heads
ViT-Large:  24 layers, 1024 dim, 16 heads
ViT-Huge:   32 layers, 1280 dim, 16 heads
```

### íš¨ìœ¨ì„± ê°œì„ 
- **DeiT**: Data-efficient training
- **Swin Transformer**: Hierarchical architecture
- **CvT**: Convolutional vision transformer
- **PVT**: Pyramid vision transformer

---

# Part 3: DINO - ìê¸°ì§€ë„í•™ìŠµ

## ğŸ¦– DINOë€?

### Self-DIstillation with NO labels

```
Teacher Model â†’ Predictions
      â†“            â†“
   Momentum     Knowledge
    Update      Transfer
      â†“            â†“
Student Model â†’ Predictions
```

### í•µì‹¬ íŠ¹ì§•
- ë¼ë²¨ ì—†ì´ í•™ìŠµ
- Teacher-Student framework
- Vision Transformer ë°±ë³¸
- ë²”ìš© íŠ¹ì§• í•™ìŠµ

---

## ğŸ“š DINO í•™ìŠµ ê³¼ì •

### Knowledge Distillation
```python
# Teacher: Momentum updated
teacher_params = momentum * teacher_params + 
                (1 - momentum) * student_params

# Student: Gradient updated
loss = cross_entropy(student_output, 
                    teacher_output.detach())
```

### Multi-crop Strategy
- **Global views**: 2ê°œ (224Ã—224)
- **Local views**: 8ê°œ (96Ã—96)
- ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì—ì„œ ì¼ê´€ëœ í‘œí˜„ í•™ìŠµ

---

## ğŸ¯ DINOv2 ê°œì„ ì‚¬í•­

### ì£¼ìš” ì—…ê·¸ë ˆì´ë“œ
1. **ë” í° ë°ì´í„°ì…‹**: 142M ì´ë¯¸ì§€
2. **ê°œì„ ëœ ì•„í‚¤í…ì²˜**: ViT-g (1.1B params)
3. **ë” ë‚˜ì€ ì¦ê°•**: Advanced augmentations
4. **Self-supervised objectives**: Multiple tasks

### ì„±ëŠ¥ í–¥ìƒ
```
Task           | DINOv1 | DINOv2
---------------|--------|--------
ImageNet       | 82.8%  | 86.3%
Segmentation   | 45.1   | 49.2 mIoU
Depth          | 0.417  | 0.356 RMSE
Retrieval      | 89.5%  | 94.2%
```

---

## ğŸ’¡ DINO í™œìš© ì‚¬ë¡€

### 1. Feature Extraction
```python
features = dino_model.forward_features(image)
# Shape: [B, 768] for ViT-B
```

### 2. Semantic Segmentation
```python
patch_features = get_patch_features(image)
clusters = kmeans.fit_predict(patch_features)
```

### 3. Image Retrieval
```python
query_features = extract_features(query_image)
similarities = compute_similarity(query_features, 
                                 database_features)
```

### 4. Few-shot Learning
- ì ì€ ìƒ˜í”Œë¡œë„ ë†’ì€ ì„±ëŠ¥
- Pre-trained features í™œìš©

---

# Part 4: SAM - Segment Anything

## âœ‚ï¸ SAMì˜ í˜ì‹ 

### Promptable Segmentation
> "ì–´ë–¤ í”„ë¡¬í”„íŠ¸ë“ , ì–´ë–¤ ê°ì²´ë“  ì„¸ê·¸ë©˜í…Œì´ì…˜"

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸
```
Image Encoder (ViT)
      â†“
Prompt Encoder
      â†“
Mask Decoder
      â†“
Segmentation Masks
```

### í”„ë¡¬í”„íŠ¸ íƒ€ì…
- **Points**: Positive/Negative clicks
- **Boxes**: Bounding rectangles
- **Masks**: Rough masks to refine
- **Text**: Natural language (SAM 2)

---

## ğŸ—ï¸ SAM ì•„í‚¤í…ì²˜

### Image Encoder
```python
# ViT-H backbone
image_encoder = ViT(
    img_size=1024,
    patch_size=16,
    embed_dim=1280,
    depth=32,
    n_heads=16
)
```

### Prompt Encoder
```python
# Handles different prompt types
def encode_prompt(prompt_type, prompt_data):
    if prompt_type == "point":
        return embed_points(prompt_data)
    elif prompt_type == "box":
        return embed_box(prompt_data)
    # ...
```

### Mask Decoder
- Transformer-based decoder
- Outputs multiple masks with scores
- IoU prediction head

---

## ğŸ“Š SAM ë°ì´í„°ì…‹ - SA-1B

### ê·œëª¨
- **11M ì´ë¯¸ì§€**
- **1.1B ë§ˆìŠ¤í¬**
- **í‰ê·  100 ë§ˆìŠ¤í¬/ì´ë¯¸ì§€**

### Data Engine
```
Stage 1: Model-Assisted Manual
    â†“
Stage 2: Semi-Automatic
    â†“  
Stage 3: Fully Automatic
```

### í’ˆì§ˆ ë©”íŠ¸ë¦­
- IoU with ground truth: 94.6%
- ì¸ê°„ í‰ê°€ìì™€ ì¼ì¹˜ìœ¨: 89%

---

## ğŸš€ SAM ì‘ìš© ë¶„ì•¼

### 1. Interactive Segmentation
```python
predictor.set_image(image)
masks = predictor.predict(
    point_coords=[[x, y]],
    point_labels=[1],  # 1: foreground
)
```

### 2. Everything Mode
```python
mask_generator = SamAutomaticMaskGenerator(sam)
masks = mask_generator.generate(image)
# Returns all possible masks
```

### 3. Video Segmentation (SAM 2)
- Temporal consistency
- Object tracking
- Real-time processing

### 4. 3D Segmentation
- Point cloud segmentation
- Medical imaging
- Robotics applications

---

# Part 5: ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¹„êµ

## ğŸ† ì£¼ìš” ê²½ìŸìë“¤

### API ê¸°ë°˜ ëª¨ë¸

| Model | Company | Strengths | Weaknesses |
|-------|---------|-----------|------------|
| **Gemini Vision** | Google | Speed, Multilingual | Limited control |
| **GPT-4V** | OpenAI | Reasoning, Accuracy | Cost, Speed |
| **Claude Vision** | Anthropic | Safety, Analysis | Availability |
| **Llama Vision** | Meta | Open source | Deployment complexity |

### ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸
- **BLIP-2**: Bootstrapped language-image pretraining
- **LLaVA**: Large language and vision assistant
- **MiniGPT-4**: Lightweight multimodal model

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Image Captioning (COCO)

```
Model      | BLEU-4 | METEOR | CIDEr
-----------|--------|--------|-------
GPT-4V     | 40.3   | 31.2   | 138.2
Gemini     | 39.7   | 30.8   | 136.5
BLIP-2     | 38.1   | 29.9   | 133.7
LLaVA      | 36.2   | 28.7   | 128.3
```

### Visual Question Answering (VQA v2)

```
Model      | Accuracy | Yes/No | Number | Other
-----------|----------|--------|--------|-------
GPT-4V     | 82.1%    | 95.2%  | 61.3%  | 73.8%
Gemini     | 81.3%    | 94.7%  | 59.8%  | 72.1%
Claude     | 80.7%    | 94.1%  | 58.2%  | 71.5%
```

---

## ğŸ’° ë¹„ìš© ë¶„ì„

### API Pricing (per 1K images)

```python
pricing = {
    'gpt4v': {
        'input': 0.01,    # per image
        'output': 0.03    # per 1K tokens
    },
    'gemini': {
        'input': 0.0025,  # per image
        'output': 0.01    # per 1K tokens
    },
    'claude': {
        'input': 0.008,   # per image
        'output': 0.024   # per 1K tokens
    }
}
```

### ë¹„ìš© ìµœì í™” ì „ëµ
1. **Caching**: ë°˜ë³µ ì¿¼ë¦¬ ìºì‹±
2. **Batching**: ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ ê°œì„ 
3. **Model Selection**: íƒœìŠ¤í¬ë³„ ìµœì  ëª¨ë¸
4. **Hybrid Approach**: ì˜¤í”ˆì†ŒìŠ¤ + API ì¡°í•©

---

## ğŸ¯ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### Decision Tree
```
ìš”êµ¬ì‚¬í•­ ë¶„ì„
â”œâ”€ ì‹¤ì‹œê°„ ì²˜ë¦¬ í•„ìš”?
â”‚  â”œâ”€ Yes â†’ ë¡œì»¬ ëª¨ë¸ (ViT, DINO, SAM)
â”‚  â””â”€ No â†’ API ê°€ëŠ¥
â”œâ”€ ë†’ì€ ì •í™•ë„ í•„ìˆ˜?
â”‚  â”œâ”€ Yes â†’ GPT-4V, Gemini
â”‚  â””â”€ No â†’ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸
â”œâ”€ ë¹„ìš© ë¯¼ê°?
â”‚  â”œâ”€ Yes â†’ ì˜¤í”ˆì†ŒìŠ¤ ìš°ì„ 
â”‚  â””â”€ No â†’ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â””â”€ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”?
   â”œâ”€ Yes â†’ ì˜¤í”ˆì†ŒìŠ¤ (fine-tuning)
   â””â”€ No â†’ API ì‚¬ìš©
```

---

# ì‹¤ìŠµ ë° ê³¼ì œ

## ğŸ§ª Lab 4: í†µí•© ì‹¤ìŠµ

### ì‹¤ìŠµ ëª©í‘œ
1. ViT ì–´í…ì…˜ ì‹œê°í™”
2. DINOv2ë¡œ íŠ¹ì§• ì¶”ì¶œ
3. SAMìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
4. ë©€í‹°ëª¨ë‹¬ API ë²¤ì¹˜ë§ˆí¬

### ë‹¨ê³„ë³„ ê°€ì´ë“œ
```python
# Step 1: ViT ì–´í…ì…˜
attention_maps = vit_model.get_attention()
visualize_attention(attention_maps)

# Step 2: DINO íŠ¹ì§•
features = dino_model.extract_features(image)
similar_images = find_similar(features)

# Step 3: SAM ì„¸ê·¸ë©˜í…Œì´ì…˜
masks = sam_model.segment(image, prompts)

# Step 4: API ë²¤ì¹˜ë§ˆí¬
results = benchmark_apis(image, task)
```

---

## ğŸ“ Assignment 4: ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ ì•±

### ìš”êµ¬ì‚¬í•­
1. **ViT êµ¬í˜„ ë° ë¶„ì„**
2. **DINOv2 í™œìš© ì‹œìŠ¤í…œ**
3. **SAM í†µí•© ì¸í„°í˜ì´ìŠ¤**
4. **API ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬**
5. **í†µí•© ëŒ€ì‹œë³´ë“œ**

### í‰ê°€ ê¸°ì¤€
- êµ¬í˜„ ì™„ì„±ë„: 40%
- ì„±ëŠ¥ ë¶„ì„: 25%
- UI/UX: 15%
- ë¬¸ì„œí™”: 10%
- ì°½ì˜ì„±: 10%

### ì œì¶œë¬¼
- GitHub ì €ì¥ì†Œ
- Hugging Face Space ë°°í¬
- ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ (PDF)
- 5ë¶„ ë°œí‘œ ìë£Œ

---

## ğŸ”‘ í•µì‹¬ ì •ë¦¬

### ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©

âœ… **Self-Attention**: ì „ì—­ì  ê´€ê³„ ëª¨ë¸ë§
âœ… **Vision Transformer**: ì´ë¯¸ì§€ë¥¼ ì‹œí€€ìŠ¤ë¡œ
âœ… **DINO**: ë¼ë²¨ ì—†ëŠ” í•™ìŠµì˜ í˜
âœ… **SAM**: ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜
âœ… **ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬**: ìµœì  ëª¨ë¸ ì„ íƒ

### Key Takeaways
1. ViTëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ CNNì„ ëŠ¥ê°€
2. ìê¸°ì§€ë„í•™ìŠµì´ ë§Œë“œëŠ” ë²”ìš© íŠ¹ì§•
3. Promptable ëª¨ë¸ì˜ ë¯¸ë˜
4. íƒœìŠ¤í¬ë³„ ìµœì  ëª¨ë¸ì´ ë‹¤ë¦„

---

## ğŸš€ ë‹¤ìŒ ì£¼ ì˜ˆê³ 

### Week 5: í”„ë¡œì íŠ¸ ë°œí‘œ ë° ë§ˆë¬´ë¦¬

**ì¤€ë¹„ì‚¬í•­**:
- ìµœì¢… í”„ë¡œì íŠ¸ ì™„ì„±
- ë°œí‘œ ìë£Œ ì¤€ë¹„
- ì½”ë“œ ì •ë¦¬ ë° ë¬¸ì„œí™”

**ë°œí‘œ ë‚´ìš©**:
- í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ ë° ëª©í‘œ
- ê¸°ìˆ ì  êµ¬í˜„ ìƒì„¸
- ì‹¤í—˜ ê²°ê³¼ ë° ë¶„ì„
- ë°ëª¨ ì‹œì—°
- Q&A

---

## ğŸ’¬ Q&A

### ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

**Q1: ViTê°€ í•­ìƒ CNNë³´ë‹¤ ì¢‹ë‚˜ìš”?**
- ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë©´ ViT
- ì‘ì€ ë°ì´í„°ì…‹ì€ CNN
- Hybrid ëª¨ë¸ë„ ì¢‹ì€ ì„ íƒ

**Q2: DINO vs Supervised Learning?**
- DINO: ë²”ìš©ì„±, ë¼ë²¨ ë¶ˆí•„ìš”
- Supervised: íŠ¹ì • íƒœìŠ¤í¬ ìµœì í™”
- ë‘˜ ë‹¤ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìµœì„ 

**Q3: SAMì˜ í•œê³„ëŠ”?**
- íˆ¬ëª… ê°ì²´ ì–´ë ¤ì›€
- ë§¤ìš° ì‘ì€ ê°ì²´ ì œí•œ
- í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¯¸ì§€ì› (SAM 1)

---

## ğŸ“š ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
2. [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)
3. [Emerging Properties in Self-Supervised ViT](https://arxiv.org/abs/2104.14294)
4. [DINOv2](https://arxiv.org/abs/2304.07193)
5. [Segment Anything](https://arxiv.org/abs/2304.02643)

### êµ¬í˜„ ë¦¬ì†ŒìŠ¤
- [timm library](https://github.com/rwightman/pytorch-image-models)
- [DINOv2 official](https://github.com/facebookresearch/dinov2)
- [SAM official](https://github.com/facebookresearch/segment-anything)
- [Transformers library](https://huggingface.co/transformers)

### íŠœí† ë¦¬ì–¼
- [ViT from Scratch](https://github.com/lucidrains/vit-pytorch)
- [DINO Tutorial](https://github.com/facebookresearch/dino)
- [SAM Demo](https://segment-anything.com/demo)

---

# Thank You! ğŸ™

## ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!

### ì—°ë½ì²˜
- ì´ë©”ì¼: newmind68@hs.ac.kr
- ì˜¤í”¼ìŠ¤ ì•„ì›Œ: ìˆ˜ìš”ì¼ 14:00-16:00

### ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤
- ê°•ì˜ ìë£Œ: [Course GitHub]
- ì§ˆë¬¸ í¬ëŸ¼: [Course Discord]
- ê³¼ì œ ì œì¶œ: [Assignment Portal]

### ë‹¤ìŒ ì£¼ ì¤€ë¹„
- í”„ë¡œì íŠ¸ ìµœì¢… ì ê²€
- ë°œí‘œ ë¦¬í—ˆì„¤
- ë™ë£Œ í‰ê°€ ì¤€ë¹„

---