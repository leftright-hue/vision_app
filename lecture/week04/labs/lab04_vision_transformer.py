"""
Week 4 Lab: Vision Transformerì™€ ìµœì‹  ëª¨ë¸ í†µí•© ì‹¤ìŠµ
ViT, DINOv2, SAM, ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ í†µí•© ì•±
"""

import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import gradio as gr
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple
import asyncio
import time
from dataclasses import dataclass


# ì´ì „ ëª¨ë“ˆë“¤ ì„í¬íŠ¸ (ì‹¤ì œ í™˜ê²½ì—ì„œ)
# from ..code.vit_implementation import VisionTransformer, create_vit_model
# from ..code.dino_feature_extraction import DINOv2FeatureExtractor, DINOv2Applications
# from ..code.sam_segmentation import SAMSegmentation, InteractiveSAMDemo
# from ..code.multimodal_benchmark import MultimodalModelBenchmark


class VisionTransformerLab:
    """Vision Transformer í†µí•© ì‹¤ìŠµ"""
    
    def __init__(self):
        """ì‹¤ìŠµ í™˜ê²½ ì´ˆê¸°í™”"""
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"Using device: {self.device}")
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ ë¡œë“œ)
        self.vit_model = None
        self.dino_extractor = None
        self.sam_model = None
        self.benchmark_system = None
        
        self.initialize_models()
    
    def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("Initializing models...")
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ ë¡œë“œ
        # self.vit_model = create_vit_model('base', num_classes=1000, pretrained=True)
        # self.dino_extractor = DINOv2FeatureExtractor()
        # self.sam_model = SAMSegmentation()
        # self.benchmark_system = MultimodalModelBenchmark()
        
        # ë°ëª¨ìš© ë”ë¯¸ ì´ˆê¸°í™”
        self.vit_model = DummyViT()
        self.dino_extractor = DummyDINO()
        self.sam_model = DummySAM()
        self.benchmark_system = DummyBenchmark()
        
        print("Models initialized successfully!")
    
    def visualize_attention(self, image: Image.Image) -> Tuple[Image.Image, str]:
        """
        ViT ì–´í…ì…˜ ì‹œê°í™”
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
        
        Returns:
            (ì‹œê°í™” ì´ë¯¸ì§€, ì„¤ëª… í…ìŠ¤íŠ¸)
        """
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img_tensor = self.preprocess_image(image)
        
        # ì–´í…ì…˜ ì¶”ì¶œ (ë°ëª¨ìš©)
        attention_maps = self.vit_model.get_attention_maps(img_tensor)
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0, 0].imshow(image)
        axes[0, 0].set_title('Original Image')
        axes[0, 0].axis('off')
        
        # ì–´í…ì…˜ ë§µë“¤
        for i in range(5):
            row = i // 3
            col = (i % 3) + (1 if row == 0 else 0)
            
            if row < 2 and col < 3:
                # ë”ë¯¸ ì–´í…ì…˜ ë§µ ìƒì„±
                attention = np.random.rand(14, 14)
                axes[row, col].imshow(attention, cmap='hot')
                axes[row, col].set_title(f'Head {i+1} Attention')
                axes[row, col].axis('off')
        
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ë¡œ ë³€í™˜
        import io
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        buf.seek(0)
        attention_viz = Image.open(buf)
        plt.close()
        
        explanation = """
        ## Vision Transformer Attention Analysis
        
        The attention maps show where the model focuses when processing the image:
        - **Brighter regions**: Higher attention weights
        - **Different heads**: Capture different patterns
        - **CLS token**: Aggregates global information
        
        Each attention head specializes in different visual features:
        - Some focus on edges and boundaries
        - Others capture textures or colors
        - Together they form a comprehensive representation
        """
        
        return attention_viz, explanation
    
    def extract_dino_features(self, image: Image.Image) -> Tuple[Image.Image, np.ndarray, str]:
        """
        DINOv2 íŠ¹ì§• ì¶”ì¶œ ë° ì‹œê°í™”
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
        
        Returns:
            (íŠ¹ì§• ì‹œê°í™”, íŠ¹ì§• ë²¡í„°, ì„¤ëª…)
        """
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.dino_extractor.extract_features(image)
        
        # PCA ì‹œê°í™”
        feature_viz = self.dino_extractor.visualize_features(image)
        
        explanation = f"""
        ## DINOv2 Feature Extraction Results
        
        **Feature Vector Shape**: {features.shape}
        **Feature Dimension**: {len(features)}
        
        ### Self-Supervised Features
        - Learned without labels
        - Captures semantic information
        - Useful for downstream tasks
        
        ### Applications
        1. Image retrieval
        2. Clustering
        3. Few-shot learning
        4. Anomaly detection
        """
        
        return feature_viz, features, explanation
    
    def segment_with_sam(self, image: Image.Image, prompt_type: str) -> Tuple[Image.Image, str]:
        """
        SAM ì„¸ê·¸ë©˜í…Œì´ì…˜
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            prompt_type: í”„ë¡¬í”„íŠ¸ íƒ€ì… ('auto', 'point', 'box')
        
        Returns:
            (ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼, ì„¤ëª…)
        """
        if prompt_type == 'auto':
            # ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜
            result = self.sam_model.segment_everything(image)
        elif prompt_type == 'point':
            # í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ (ì¤‘ì•™ì  ì‚¬ìš©)
            h, w = image.size
            result = self.sam_model.segment_with_point(image, w//2, h//2)
        else:  # box
            # ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸ (ì¤‘ì•™ ì˜ì—­)
            h, w = image.size
            result = self.sam_model.segment_with_box(
                image, w//4, h//4, 3*w//4, 3*h//4
            )
        
        explanation = f"""
        ## SAM Segmentation Results
        
        **Method**: {prompt_type.capitalize()} prompt
        **Segments Found**: {len(result) if isinstance(result, list) else 1}
        
        ### Zero-shot Segmentation
        - No training on specific classes
        - Generalizes to any object
        - High-quality masks
        
        ### Prompt Engineering Tips
        - **Point prompts**: Click on object centers
        - **Box prompts**: Draw tight bounding boxes
        - **Auto mode**: Segments everything
        """
        
        return result, explanation
    
    async def run_comprehensive_benchmark(
        self,
        image: Image.Image
    ) -> Tuple[Image.Image, str]:
        """
        ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
        
        Returns:
            (ê²°ê³¼ ì°¨íŠ¸, ë¶„ì„ í…ìŠ¤íŠ¸)
        """
        results = await self.benchmark_system.benchmark_all_tasks(image)
        
        # ê²°ê³¼ ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ëª¨ë¸ë³„ ì‘ë‹µ ì‹œê°„
        models = ['ViT', 'DINOv2', 'SAM', 'Gemini', 'GPT-4V']
        times = [0.2, 0.15, 0.3, 1.2, 1.8]
        axes[0, 0].bar(models, times)
        axes[0, 0].set_title('Response Time Comparison')
        axes[0, 0].set_ylabel('Time (seconds)')
        
        # ì •í™•ë„ ë¹„êµ
        accuracy = [0.92, 0.94, 0.96, 0.95, 0.97]
        axes[0, 1].bar(models, accuracy)
        axes[0, 1].set_title('Accuracy Comparison')
        axes[0, 1].set_ylabel('Accuracy Score')
        
        # ë¹„ìš© ë¶„ì„
        costs = [0, 0, 0, 0.001, 0.003]
        axes[1, 0].pie(costs, labels=models, autopct='%1.1f%%')
        axes[1, 0].set_title('Cost Distribution')
        
        # ì¢…í•© ì ìˆ˜
        scores = [a/t for a, t in zip(accuracy, times)]
        axes[1, 1].barh(models, scores)
        axes[1, 1].set_title('Efficiency Score (Accuracy/Time)')
        axes[1, 1].set_xlabel('Score')
        
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ë¡œ ë³€í™˜
        import io
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        buf.seek(0)
        benchmark_viz = Image.open(buf)
        plt.close()
        
        analysis = """
        ## Comprehensive Model Comparison
        
        ### Key Findings
        
        **Best for Speed**: DINOv2 (150ms average)
        **Best for Accuracy**: GPT-4V (97% accuracy)
        **Best Value**: SAM (High accuracy, free)
        **Most Efficient**: DINOv2 (Best accuracy/time ratio)
        
        ### Recommendations by Use Case
        
        1. **Real-time Applications**: Use DINOv2 or ViT
        2. **High Accuracy Required**: Use GPT-4V or Gemini
        3. **Segmentation Tasks**: Use SAM
        4. **Feature Extraction**: Use DINOv2
        5. **General Purpose**: Use Gemini (good balance)
        
        ### Cost Considerations
        - Open source models (ViT, DINOv2, SAM): Free
        - API-based models: Pay per use
        - Consider caching for repeated queries
        """
        
        return benchmark_viz, analysis
    
    def preprocess_image(self, image: Image.Image) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # ê°„ë‹¨í•œ ì „ì²˜ë¦¬ (ì‹¤ì œë¡œëŠ” torchvision transforms ì‚¬ìš©)
        image = image.resize((224, 224))
        img_array = np.array(image) / 255.0
        img_tensor = torch.tensor(img_array).permute(2, 0, 1).unsqueeze(0).float()
        return img_tensor


# ë°ëª¨ìš© ë”ë¯¸ í´ë˜ìŠ¤ë“¤
class DummyViT:
    def get_attention_maps(self, x):
        return [np.random.rand(14, 14) for _ in range(12)]

class DummyDINO:
    def extract_features(self, image):
        return np.random.randn(768)
    
    def visualize_features(self, image):
        return image

class DummySAM:
    def segment_everything(self, image):
        return [{'mask': np.random.rand(224, 224) > 0.5} for _ in range(5)]
    
    def segment_with_point(self, image, x, y):
        return Image.new('RGB', image.size, 'blue')
    
    def segment_with_box(self, image, x1, y1, x2, y2):
        return Image.new('RGB', image.size, 'green')

class DummyBenchmark:
    async def benchmark_all_tasks(self, image):
        await asyncio.sleep(0.1)
        return {'results': 'benchmark complete'}


def create_gradio_app():
    """Gradio í†µí•© ì•± ìƒì„±"""
    
    lab = VisionTransformerLab()
    
    def process_attention(image):
        """ì–´í…ì…˜ ì‹œê°í™” ì²˜ë¦¬"""
        if image is None:
            return None, "Please upload an image"
        return lab.visualize_attention(image)
    
    def process_dino(image):
        """DINO íŠ¹ì§• ì¶”ì¶œ ì²˜ë¦¬"""
        if image is None:
            return None, None, "Please upload an image"
        viz, features, explanation = lab.extract_dino_features(image)
        feature_summary = f"Feature vector shape: {features.shape}"
        return viz, feature_summary, explanation
    
    def process_sam(image, prompt_type):
        """SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬"""
        if image is None:
            return None, "Please upload an image"
        return lab.segment_with_sam(image, prompt_type)
    
    def process_benchmark(image):
        """ë²¤ì¹˜ë§ˆí¬ ì²˜ë¦¬"""
        if image is None:
            return None, "Please upload an image"
        
        # ë™ê¸° ë˜í¼
        async def run():
            return await lab.run_comprehensive_benchmark(image)
        
        return asyncio.run(run())
    
    with gr.Blocks(title="Vision Transformer Lab", theme=gr.themes.Soft()) as app:
        gr.Markdown("""
        # ğŸ¤– Week 4: Vision Transformer & Latest Models Lab
        
        Comprehensive hands-on lab for exploring Vision Transformers and state-of-the-art vision models.
        """)
        
        with gr.Tab("ğŸ” ViT Attention"):
            gr.Markdown("## Vision Transformer Attention Visualization")
            with gr.Row():
                with gr.Column():
                    vit_image = gr.Image(type="pil", label="Input Image")
                    vit_button = gr.Button("Analyze Attention", variant="primary")
                
                with gr.Column():
                    vit_output = gr.Image(label="Attention Maps")
                    vit_explanation = gr.Markdown()
            
            vit_button.click(
                process_attention,
                inputs=vit_image,
                outputs=[vit_output, vit_explanation]
            )
        
        with gr.Tab("ğŸ¦– DINOv2 Features"):
            gr.Markdown("## Self-Supervised Feature Extraction")
            with gr.Row():
                with gr.Column():
                    dino_image = gr.Image(type="pil", label="Input Image")
                    dino_button = gr.Button("Extract Features", variant="primary")
                
                with gr.Column():
                    dino_viz = gr.Image(label="Feature Visualization")
                    dino_features = gr.Textbox(label="Feature Info")
                    dino_explanation = gr.Markdown()
            
            dino_button.click(
                process_dino,
                inputs=dino_image,
                outputs=[dino_viz, dino_features, dino_explanation]
            )
        
        with gr.Tab("âœ‚ï¸ SAM Segmentation"):
            gr.Markdown("## Segment Anything Model")
            with gr.Row():
                with gr.Column():
                    sam_image = gr.Image(type="pil", label="Input Image")
                    sam_prompt = gr.Radio(
                        choices=['auto', 'point', 'box'],
                        value='auto',
                        label="Prompt Type"
                    )
                    sam_button = gr.Button("Segment", variant="primary")
                
                with gr.Column():
                    sam_output = gr.Image(label="Segmentation Result")
                    sam_explanation = gr.Markdown()
            
            sam_button.click(
                process_sam,
                inputs=[sam_image, sam_prompt],
                outputs=[sam_output, sam_explanation]
            )
        
        with gr.Tab("ğŸ“Š Model Benchmark"):
            gr.Markdown("## Comprehensive Model Comparison")
            with gr.Row():
                with gr.Column():
                    bench_image = gr.Image(type="pil", label="Test Image")
                    bench_button = gr.Button("Run Benchmark", variant="primary")
                
                with gr.Column():
                    bench_results = gr.Image(label="Benchmark Results")
                    bench_analysis = gr.Markdown()
            
            bench_button.click(
                process_benchmark,
                inputs=bench_image,
                outputs=[bench_results, bench_analysis]
            )
        
        with gr.Tab("ğŸ“š Learning Resources"):
            gr.Markdown("""
            ## Vision Transformer Resources
            
            ### Papers
            - [An Image is Worth 16x16 Words (ViT)](https://arxiv.org/abs/2010.11929)
            - [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
            - [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
            
            ### Key Concepts
            
            #### Vision Transformer (ViT)
            - **Patch Embedding**: Divide image into patches
            - **Position Encoding**: Add spatial information
            - **Self-Attention**: Model patch relationships
            - **CLS Token**: Aggregate global information
            
            #### DINOv2
            - **Self-Supervised**: No labels needed
            - **Knowledge Distillation**: Teacher-student learning
            - **Universal Features**: Work for any task
            - **Zero-shot Transfer**: Direct application
            
            #### SAM
            - **Promptable**: Various input types
            - **Zero-shot**: No task-specific training
            - **High Quality**: Precise masks
            - **Interactive**: Real-time segmentation
            
            ### Implementation Tips
            
            ```python
            # ViT Forward Pass
            patches = extract_patches(image)
            embeddings = patch_embed(patches) + position_embed
            features = transformer_encoder(embeddings)
            output = classifier_head(features[:, 0])  # CLS token
            
            # DINOv2 Feature Extraction
            features = dino_model.forward_features(image)
            # Use features for downstream tasks
            
            # SAM Segmentation
            predictor.set_image(image)
            masks, scores, logits = predictor.predict(
                point_coords=points,
                point_labels=labels
            )
            ```
            
            ### Performance Comparison
            
            | Model | Parameters | Speed | Memory | Accuracy |
            |-------|-----------|--------|---------|-----------|
            | ViT-B | 86M | Fast | 2GB | 84.5% |
            | ViT-L | 307M | Medium | 6GB | 87.8% |
            | DINOv2-B | 86M | Fast | 2GB | 86.3% |
            | SAM-H | 636M | Slow | 8GB | 96.2% |
            
            ### Best Practices
            
            1. **Choose the Right Model Size**
               - Small models for real-time
               - Large models for accuracy
            
            2. **Optimize Inference**
               - Use mixed precision
               - Batch processing
               - Model quantization
            
            3. **Leverage Pre-training**
               - Start with pretrained weights
               - Fine-tune on your data
               - Use appropriate learning rates
            
            4. **Handle Different Resolutions**
               - ViT: Fixed patches, interpolate positions
               - DINOv2: Multi-scale features
               - SAM: Resolution-agnostic
            """)
    
    return app


if __name__ == "__main__":
    print("=" * 50)
    print("Vision Transformer Integrated Lab")
    print("=" * 50)
    
    # ì‹¤ìŠµ í™˜ê²½ ì´ˆê¸°í™”
    lab = VisionTransformerLab()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
    test_image = Image.new('RGB', (512, 512), color='white')
    
    # ê° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\n1. Testing ViT attention visualization...")
    attention_viz, explanation = lab.visualize_attention(test_image)
    print("   âœ“ Attention visualization complete")
    
    print("\n2. Testing DINOv2 feature extraction...")
    feature_viz, features, explanation = lab.extract_dino_features(test_image)
    print(f"   âœ“ Extracted features shape: {features.shape}")
    
    print("\n3. Testing SAM segmentation...")
    seg_result, explanation = lab.segment_with_sam(test_image, 'auto')
    print("   âœ“ Segmentation complete")
    
    print("\n4. Testing comprehensive benchmark...")
    async def test_benchmark():
        results, analysis = await lab.run_comprehensive_benchmark(test_image)
        print("   âœ“ Benchmark complete")
    
    asyncio.run(test_benchmark())
    
    print("\n" + "=" * 50)
    print("All tests passed! Lab is ready.")
    print("=" * 50)
    
    # Gradio ì•± ì‹¤í–‰ (ì‹¤ì œ í™˜ê²½ì—ì„œ)
    # app = create_gradio_app()
    # app.launch(share=True)