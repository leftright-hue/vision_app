# Week 04: Vision Transformer + ìµœì‹  ëª¨ë¸ ë¹„êµ

## ğŸ“š ì´ë²ˆ ì£¼ í•™ìŠµ ëª©í‘œ

### ì´ë¡  í•™ìŠµ ëª©í‘œ
1. **Self-Attention ë©”ì»¤ë‹ˆì¦˜ ì™„ë²½ ì´í•´**
   - Query, Key, Valueì˜ ì—­í• 
   - Multi-Head Attentionì˜ í•„ìš”ì„±
   - Position Encodingì˜ ì¤‘ìš”ì„±

2. **Vision Transformer (ViT) ì•„í‚¤í…ì²˜ ë§ˆìŠ¤í„°**
   - CNN vs Transformer ë¹„êµ
   - Patch Embedding ê³¼ì •
   - Classification Tokenì˜ ì—­í• 

3. **ìê¸°ì§€ë„í•™ìŠµ (Self-Supervised Learning)**
   - DINOì˜ ì›ë¦¬ì™€ í˜ì‹ ì„±
   - Teacher-Student í”„ë ˆì„ì›Œí¬
   - Vision Foundation Models

4. **ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¹„êµ**
   - Gemini vs GPT-4V vs Llama Vision
   - ê° ëª¨ë¸ì˜ ì¥ë‹¨ì ê³¼ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
   - ì‹¤ì „ ì„ íƒ ê°€ì´ë“œ

### ì‹¤ìŠµ ëª©í‘œ
- Vision Transformerë¥¼ PyTorchë¡œ ì§ì ‘ êµ¬í˜„
- DINOv2ë¡œ ê°•ë ¥í•œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
- SAMìœ¼ë¡œ ì œë¡œìƒ· ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²´í—˜
- ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- **ìµœì¢… í”„ë¡œì íŠ¸**: ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì•± êµ¬ì¶•

---

## ğŸ¯ í•µì‹¬ ê°œë…

### 1. Attention is All You Need (in Vision)

#### 1.1 Self-Attentionì˜ í•µì‹¬
```python
"""
Self-Attentionì´ë€?
ì…ë ¥ì˜ ëª¨ë“  ìœ„ì¹˜ê°€ ë‹¤ë¥¸ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì°¸ì¡°í•˜ì—¬
ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜
"""

# í•µì‹¬ ìˆ˜ì‹
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

# Where:
# Q (Query): ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€?
# K (Key): ë¬´ì—‡ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ê°€?
# V (Value): ì‹¤ì œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€?
```

#### 1.2 ì™œ Visionì— Transformerì¸ê°€?

| íŠ¹ì„± | CNN | Vision Transformer |
|------|-----|-------------------|
| **Receptive Field** | ì œí•œì  (ì»¤ë„ í¬ê¸°) | ì „ì—­ì  (ì „ì²´ ì´ë¯¸ì§€) |
| **Inductive Bias** | ê°•í•¨ (locality, translation) | ì•½í•¨ (ë” ë§ì€ ë°ì´í„° í•„ìš”) |
| **ê³„ì‚° ë³µì¡ë„** | O(n) | O(nÂ²) |
| **ë³‘ë ¬í™”** | ì œí•œì  | ë§¤ìš° ë†’ìŒ |
| **í•´ì„ê°€ëŠ¥ì„±** | ì–´ë ¤ì›€ | Attention Map ì‹œê°í™” ê°€ëŠ¥ |

### 2. Vision Transformer (ViT) ì•„í‚¤í…ì²˜

#### 2.1 ì´ë¯¸ì§€ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
```python
"""
ViTì˜ í•µì‹¬ ì•„ì´ë””ì–´:
ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì‹œí€€ìŠ¤ì²˜ëŸ¼ ì²˜ë¦¬
"""

# ì´ë¯¸ì§€ â†’ íŒ¨ì¹˜ â†’ í† í°
Image (224Ã—224Ã—3) 
â†’ Patches (16Ã—16Ã—3) Ã— 196
â†’ Linear Projection (768-dim) Ã— 196
â†’ + Position Embedding
â†’ + [CLS] Token
â†’ Transformer Encoder
```

#### 2.2 ViTì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ
1. **Patch Embedding**: ì´ë¯¸ì§€ë¥¼ ê³ ì • í¬ê¸° íŒ¨ì¹˜ë¡œ ë¶„í• 
2. **Position Embedding**: íŒ¨ì¹˜ì˜ ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©
3. **[CLS] Token**: ì „ì²´ ì´ë¯¸ì§€ í‘œí˜„ì„ ìœ„í•œ íŠ¹ë³„ í† í°
4. **Transformer Encoder**: Multi-Head Self-Attention + FFN
5. **MLP Head**: ìµœì¢… ë¶„ë¥˜ë¥¼ ìœ„í•œ í—¤ë“œ

### 3. DINO: Self-Supervised Vision Transformers

#### 3.1 ìê¸°ì§€ë„í•™ìŠµì˜ í˜ëª…
```python
"""
DINO (Self-DIstillation with NO labels)
ë ˆì´ë¸” ì—†ì´ ê°•ë ¥í•œ visual representation í•™ìŠµ
"""

# Teacher-Student Framework
Teacher Network â†’ Momentum Update â†’ EMA
      â†‘                               â†“
   Gradient                      Predictions
      â†‘                               â†“
Student Network â† Loss â† Cross-Entropy
```

#### 3.2 DINOì˜ ë†€ë¼ìš´ íŠ¹ì„±
- **Semantic Segmentation**: í•™ìŠµí•˜ì§€ ì•Šì•˜ëŠ”ë°ë„ ê°ì²´ ë¶„í• 
- **Object Discovery**: ìë™ìœ¼ë¡œ ê°ì²´ ê²½ê³„ ë°œê²¬
- **Fine-grained Features**: ë§¤ìš° ì„¸ë°€í•œ íŠ¹ì§• ì¶”ì¶œ
- **Transfer Learning**: ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ë›°ì–´ë‚œ ì „ì´

### 4. SAM (Segment Anything Model)

#### 4.1 ì œë¡œìƒ· ì„¸ê·¸ë©˜í…Œì´ì…˜
```python
"""
SAMì˜ 3ê°€ì§€ í”„ë¡¬í”„íŒ… ë°©ì‹:
1. Point Prompts: í´ë¦­ìœ¼ë¡œ ê°ì²´ ì„ íƒ
2. Box Prompts: ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ì˜ì—­ ì§€ì •
3. Mask Prompts: ê¸°ì¡´ ë§ˆìŠ¤í¬ ê°œì„ 
"""
```

#### 4.2 SAMì˜ êµ¬ì¡°
- **Image Encoder**: ViT-based (Heavy)
- **Prompt Encoder**: ê²½ëŸ‰ (Light)
- **Mask Decoder**: ê²½ëŸ‰ (Light)
- **Data Engine**: 10ì–µ+ ë§ˆìŠ¤í¬ë¡œ í•™ìŠµ

---

## ğŸ’» ì‹¤ìŠµ í™˜ê²½ ì„¤ì •

### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install torch torchvision transformers
pip install numpy pandas matplotlib seaborn

# Vision Transformer ê´€ë ¨
pip install timm  # PyTorch Image Models
pip install einops  # í…ì„œ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬

# DINO & SAM
pip install git+https://github.com/facebookresearch/dino.git
pip install segment-anything

# ë©€í‹°ëª¨ë‹¬ API
pip install google-generativeai
pip install together
pip install openai

# ìœ í‹¸ë¦¬í‹°
pip install gradio
pip install opencv-python
pip install scikit-image
```

### GPU ì„¤ì • í™•ì¸
```python
import torch
print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
```

---

## ğŸ”¬ ì´ë¡  íŒŒíŠ¸ 1: Self-Attention ë©”ì»¤ë‹ˆì¦˜

### 1. Attentionì˜ ìˆ˜í•™ì  ê¸°ì´ˆ

#### 1.1 Scaled Dot-Product Attention
```python
import torch
import torch.nn.functional as F
import math

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attention
    
    Args:
        Q: Query tensor [batch, seq_len, d_k]
        K: Key tensor [batch, seq_len, d_k]
        V: Value tensor [batch, seq_len, d_v]
        mask: Optional mask [batch, seq_len, seq_len]
    
    Returns:
        output: Attention output [batch, seq_len, d_v]
        attention_weights: Attention weights [batch, seq_len, seq_len]
    """
    d_k = Q.size(-1)
    
    # 1. Qì™€ Kì˜ dot product ê³„ì‚°
    scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, seq_len, seq_len]
    
    # 2. Scaling (gradient vanishing ë°©ì§€)
    scores = scores / math.sqrt(d_k)
    
    # 3. Mask ì ìš© (optional)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # 4. Softmaxë¡œ attention weights ê³„ì‚°
    attention_weights = F.softmax(scores, dim=-1)
    
    # 5. Valueì— weights ì ìš©
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

#### 1.2 Multi-Head Attention
```python
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention êµ¬í˜„
    ì—¬ëŸ¬ ê°œì˜ attention headë¥¼ ë³‘ë ¬ë¡œ ê³„ì‚°
    """
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # Linear projections
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 1. Linear projections in batch
        Q = self.W_q(query)  # [batch, seq_len, d_model]
        K = self.W_k(key)
        V = self.W_v(value)
        
        # 2. Reshape for multi-head
        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        # [batch, n_heads, seq_len, d_k]
        
        # 3. Apply attention
        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)
        
        # 4. Concatenate heads
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # 5. Final linear projection
        output = self.W_o(attn_output)
        
        return output, attn_weights
```

### 2. Position Encoding

#### 2.1 Sinusoidal Position Encoding
```python
def get_sinusoidal_encoding(seq_len, d_model):
    """
    Sinusoidal position encoding
    """
    position = torch.arange(seq_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2) * 
                         -(math.log(10000.0) / d_model))
    
    pe = torch.zeros(seq_len, d_model)
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    return pe
```

#### 2.2 Learnable Position Encoding
```python
class LearnablePositionEncoding(nn.Module):
    """
    í•™ìŠµ ê°€ëŠ¥í•œ position encoding (ViTì—ì„œ ì‚¬ìš©)
    """
    def __init__(self, seq_len, d_model):
        super().__init__()
        self.position_embedding = nn.Parameter(
            torch.randn(1, seq_len, d_model)
        )
    
    def forward(self, x):
        return x + self.position_embedding
```

---

## ğŸ”¬ ì´ë¡  íŒŒíŠ¸ 2: Vision Transformer êµ¬í˜„

### 1. ì™„ì „í•œ ViT êµ¬í˜„

```python
import torch
import torch.nn as nn
from einops import rearrange, repeat
from einops.layers.torch import Rearrange

class VisionTransformer(nn.Module):
    """
    Vision Transformer (ViT) êµ¬í˜„
    An Image is Worth 16x16 Words ë…¼ë¬¸ ê¸°ë°˜
    """
    def __init__(
        self,
        image_size=224,
        patch_size=16,
        num_classes=1000,
        dim=768,
        depth=12,
        heads=12,
        mlp_dim=3072,
        pool='cls',
        channels=3,
        dim_head=64,
        dropout=0.1,
        emb_dropout=0.1
    ):
        super().__init__()
        
        # ì´ë¯¸ì§€ì™€ íŒ¨ì¹˜ ì •ë³´
        assert image_size % patch_size == 0
        num_patches = (image_size // patch_size) ** 2
        patch_dim = channels * patch_size ** 2
        
        self.pool = pool
        
        # Patch Embedding
        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', 
                     p1=patch_size, p2=patch_size),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            nn.LayerNorm(dim)
        )
        
        # Position Embedding
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        
        # CLS Token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        
        # Dropout
        self.dropout = nn.Dropout(emb_dropout)
        
        # Transformer Encoder
        self.transformer = TransformerEncoder(
            dim=dim,
            depth=depth,
            heads=heads,
            dim_head=dim_head,
            mlp_dim=mlp_dim,
            dropout=dropout
        )
        
        # Classification Head
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )
    
    def forward(self, img):
        # 1. Patch Embedding
        x = self.to_patch_embedding(img)
        batch_size, num_patches, _ = x.shape
        
        # 2. Add CLS Token
        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=batch_size)
        x = torch.cat((cls_tokens, x), dim=1)
        
        # 3. Add Position Embedding
        x += self.pos_embedding[:, :(num_patches + 1)]
        x = self.dropout(x)
        
        # 4. Transformer Encoder
        x = self.transformer(x)
        
        # 5. Classification
        if self.pool == 'cls':
            x = x[:, 0]  # CLS token
        else:
            x = x.mean(dim=1)  # Global average pooling
        
        return self.mlp_head(x)


class TransformerEncoder(nn.Module):
    """
    Transformer Encoder Stack
    """
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.1):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerBlock(dim, heads, dim_head, mlp_dim, dropout)
            for _ in range(depth)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x


class TransformerBlock(nn.Module):
    """
    Single Transformer Block
    """
    def __init__(self, dim, heads, dim_head, mlp_dim, dropout=0.1):
        super().__init__()
        
        # Multi-Head Self-Attention
        self.norm1 = nn.LayerNorm(dim)
        self.attn = MultiHeadSelfAttention(dim, heads, dim_head, dropout)
        
        # Feed-Forward Network
        self.norm2 = nn.LayerNorm(dim)
        self.ffn = FeedForward(dim, mlp_dim, dropout)
    
    def forward(self, x):
        # Self-Attention with residual connection
        x = x + self.attn(self.norm1(x))
        
        # FFN with residual connection
        x = x + self.ffn(self.norm2(x))
        
        return x


class MultiHeadSelfAttention(nn.Module):
    """
    Multi-Head Self-Attention Module
    """
    def __init__(self, dim, heads, dim_head, dropout=0.1):
        super().__init__()
        inner_dim = dim_head * heads
        
        self.heads = heads
        self.scale = dim_head ** -0.5
        
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        batch, seq_len, _ = x.shape
        
        # Generate Q, K, V
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)
        
        # Attention scores
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = F.softmax(dots, dim=-1)
        
        # Apply attention to values
        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        
        return self.to_out(out)


class FeedForward(nn.Module):
    """
    Feed-Forward Network
    """
    def __init__(self, dim, hidden_dim, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        return self.net(x)
```

### 2. ViT ì‚¬ìš© ì˜ˆì œ

```python
def test_vit():
    """
    Vision Transformer í…ŒìŠ¤íŠ¸
    """
    # ëª¨ë¸ ìƒì„±
    model = VisionTransformer(
        image_size=224,
        patch_size=16,
        num_classes=1000,
        dim=768,
        depth=12,
        heads=12,
        mlp_dim=3072,
        channels=3
    )
    
    # ì…ë ¥ ì´ë¯¸ì§€ (ë°°ì¹˜)
    img = torch.randn(2, 3, 224, 224)
    
    # Forward pass
    preds = model(img)
    print(f"Input shape: {img.shape}")
    print(f"Output shape: {preds.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
```

---

## ğŸ”¬ ì´ë¡  íŒŒíŠ¸ 3: DINOì™€ ìê¸°ì§€ë„í•™ìŠµ

### 1. DINO ì›ë¦¬ ì´í•´

```python
class DINOLoss(nn.Module):
    """
    DINO Loss: Self-Distillation with NO labels
    Teacher-Student í”„ë ˆì„ì›Œí¬ ê¸°ë°˜
    """
    def __init__(self, student_temp=0.1, teacher_temp=0.04, center_momentum=0.9):
        super().__init__()
        self.student_temp = student_temp
        self.teacher_temp = teacher_temp
        self.center_momentum = center_momentum
        self.register_buffer("center", torch.zeros(1, 1))
        
    def forward(self, student_output, teacher_output):
        """
        Cross-entropy between softmax outputs
        """
        student_out = student_output / self.student_temp
        teacher_out = teacher_output / self.teacher_temp
        
        # Center the teacher output
        teacher_out = teacher_out - self.center
        
        # Softmax
        student_softmax = F.log_softmax(student_out, dim=-1)
        teacher_softmax = F.softmax(teacher_out, dim=-1)
        
        # Cross-entropy loss
        loss = -torch.sum(teacher_softmax * student_softmax, dim=-1).mean()
        
        # Update center
        self.update_center(teacher_output)
        
        return loss
    
    @torch.no_grad()
    def update_center(self, teacher_output):
        """
        EMA update of center
        """
        batch_center = teacher_output.mean(dim=0, keepdim=True)
        self.center = self.center * self.center_momentum + \
                     batch_center * (1 - self.center_momentum)
```

### 2. DINOì˜ ë†€ë¼ìš´ íŠ¹ì„± ì‹œê°í™”

```python
def visualize_dino_attention():
    """
    DINOì˜ attention map ì‹œê°í™”
    ê°ì²´ ê²½ê³„ë¥¼ ìë™ìœ¼ë¡œ ë°œê²¬í•˜ëŠ” ëŠ¥ë ¥ ì‹œì—°
    """
    # ì‚¬ì „í›ˆë ¨ëœ DINO ëª¨ë¸ ë¡œë“œ
    from transformers import ViTModel
    
    model = ViTModel.from_pretrained('facebook/dino-vitb16')
    
    # Attention ì¶”ì¶œ ë° ì‹œê°í™” ì½”ë“œ
    # ...
```

---

## ğŸ› ï¸ ì‹¤ìŠµ 1: Vision Transformer í™œìš©

### timm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‚¬ì „í›ˆë ¨ ViT ì‚¬ìš©
```python
import timm
from PIL import Image
import torch

def use_pretrained_vit():
    """
    ì‚¬ì „í›ˆë ¨ëœ ViT ëª¨ë¸ ì‚¬ìš©
    """
    # 1. ëª¨ë¸ ë¡œë“œ
    model = timm.create_model('vit_base_patch16_224', pretrained=True)
    model.eval()
    
    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    config = timm.data.resolve_data_config({}, model=model)
    transform = timm.data.create_transform(**config)
    
    # 3. ì´ë¯¸ì§€ ì¤€ë¹„
    img = Image.open('sample.jpg')
    img_tensor = transform(img).unsqueeze(0)
    
    # 4. ì˜ˆì¸¡
    with torch.no_grad():
        output = model(img_tensor)
        probs = torch.softmax(output, dim=1)
        top5_prob, top5_idx = probs.topk(5)
    
    # 5. ê²°ê³¼ ì¶œë ¥
    for i in range(5):
        print(f"Class {top5_idx[0][i]}: {top5_prob[0][i]:.2%}")
    
    return model
```

---

## ğŸ› ï¸ ì‹¤ìŠµ 2: DINOv2 íŠ¹ì§• ì¶”ì¶œ

### Hugging Faceë¥¼ ì‚¬ìš©í•œ DINOv2
```python
from transformers import AutoImageProcessor, AutoModel
import torch
from PIL import Image
import numpy as np

class DINOv2FeatureExtractor:
    """
    DINOv2ë¥¼ ì‚¬ìš©í•œ ê°•ë ¥í•œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
    """
    def __init__(self, model_name='facebook/dinov2-base'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = AutoImageProcessor.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
    
    def extract_features(self, image_path):
        """
        ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
        """
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        image = Image.open(image_path)
        inputs = self.processor(images=image, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # íŠ¹ì§• ì¶”ì¶œ
        with torch.no_grad():
            outputs = self.model(**inputs)
            
            # CLS token features
            cls_features = outputs.last_hidden_state[:, 0]
            
            # Patch features
            patch_features = outputs.last_hidden_state[:, 1:]
            
            # Global average pooling
            global_features = patch_features.mean(dim=1)
        
        return {
            'cls_features': cls_features.cpu().numpy(),
            'global_features': global_features.cpu().numpy(),
            'patch_features': patch_features.cpu().numpy()
        }
    
    def compute_similarity(self, features1, features2):
        """
        ë‘ ì´ë¯¸ì§€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        """
        # Cosine similarity
        feat1 = features1['global_features']
        feat2 = features2['global_features']
        
        similarity = np.dot(feat1, feat2.T) / (
            np.linalg.norm(feat1) * np.linalg.norm(feat2)
        )
        
        return similarity.item()
    
    def find_similar_regions(self, image_path1, image_path2):
        """
        ë‘ ì´ë¯¸ì§€ì—ì„œ ìœ ì‚¬í•œ ì˜ì—­ ì°¾ê¸°
        """
        features1 = self.extract_features(image_path1)
        features2 = self.extract_features(image_path2)
        
        # Patch-level similarity
        patch_sim = np.dot(
            features1['patch_features'][0], 
            features2['patch_features'][0].T
        )
        
        return patch_sim

# ì‚¬ìš© ì˜ˆì œ
def demo_dinov2():
    extractor = DINOv2FeatureExtractor()
    
    # íŠ¹ì§• ì¶”ì¶œ
    features = extractor.extract_features('image.jpg')
    print(f"CLS features shape: {features['cls_features'].shape}")
    print(f"Global features shape: {features['global_features'].shape}")
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    features1 = extractor.extract_features('image1.jpg')
    features2 = extractor.extract_features('image2.jpg')
    similarity = extractor.compute_similarity(features1, features2)
    print(f"Similarity: {similarity:.3f}")
```

---

## ğŸ› ï¸ ì‹¤ìŠµ 3: SAM ì„¸ê·¸ë©˜í…Œì´ì…˜

### Segment Anything Model í™œìš©
```python
from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator
import cv2
import numpy as np
import matplotlib.pyplot as plt

class SAMSegmentation:
    """
    SAMì„ ì‚¬ìš©í•œ ì œë¡œìƒ· ì„¸ê·¸ë©˜í…Œì´ì…˜
    """
    def __init__(self, model_type='vit_h', checkpoint_path='sam_vit_h.pth'):
        # SAM ëª¨ë¸ ë¡œë“œ
        self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)
        self.sam.to(device='cuda' if torch.cuda.is_available() else 'cpu')
        
        # Predictor (í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì„¸ê·¸ë©˜í…Œì´ì…˜)
        self.predictor = SamPredictor(self.sam)
        
        # Automatic Mask Generator (ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜)
        self.mask_generator = SamAutomaticMaskGenerator(self.sam)
    
    def segment_with_points(self, image_path, points, labels):
        """
        í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
        
        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            points: í¬ì¸íŠ¸ ì¢Œí‘œ [(x1, y1), (x2, y2), ...]
            labels: í¬ì¸íŠ¸ ë ˆì´ë¸” [1, 0, ...] (1: í¬í•¨, 0: ì œì™¸)
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Predictor ì„¤ì •
        self.predictor.set_image(image)
        
        # í¬ì¸íŠ¸ë¡œ ì˜ˆì¸¡
        points_np = np.array(points)
        labels_np = np.array(labels)
        
        masks, scores, logits = self.predictor.predict(
            point_coords=points_np,
            point_labels=labels_np,
            multimask_output=True
        )
        
        # ìµœê³  ì ìˆ˜ ë§ˆìŠ¤í¬ ì„ íƒ
        best_mask_idx = np.argmax(scores)
        best_mask = masks[best_mask_idx]
        
        return best_mask, scores[best_mask_idx]
    
    def segment_with_box(self, image_path, box):
        """
        ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
        
        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            box: [x1, y1, x2, y2] í˜•íƒœì˜ ë°”ìš´ë”© ë°•ìŠ¤
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Predictor ì„¤ì •
        self.predictor.set_image(image)
        
        # ë°•ìŠ¤ë¡œ ì˜ˆì¸¡
        box_np = np.array(box)
        
        masks, scores, logits = self.predictor.predict(
            box=box_np,
            multimask_output=True
        )
        
        # ìµœê³  ì ìˆ˜ ë§ˆìŠ¤í¬ ì„ íƒ
        best_mask_idx = np.argmax(scores)
        best_mask = masks[best_mask_idx]
        
        return best_mask, scores[best_mask_idx]
    
    def automatic_segmentation(self, image_path):
        """
        ìë™ìœ¼ë¡œ ëª¨ë“  ê°ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜
        masks = self.mask_generator.generate(image)
        
        # í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
        masks = sorted(masks, key=lambda x: x['area'], reverse=True)
        
        return masks
    
    def visualize_masks(self, image_path, masks):
        """
        ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ ì‹œê°í™”
        """
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        plt.figure(figsize=(12, 8))
        plt.imshow(image)
        
        # ê° ë§ˆìŠ¤í¬ë¥¼ ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ ì˜¤ë²„ë ˆì´
        for i, mask_data in enumerate(masks[:10]):  # ìƒìœ„ 10ê°œë§Œ
            mask = mask_data['segmentation']
            color = np.random.random(3)
            
            # ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
            h, w = mask.shape
            mask_img = mask.reshape(h, w, 1) * color.reshape(1, 1, 3)
            plt.imshow(mask_img, alpha=0.4)
        
        plt.axis('off')
        plt.title(f"Found {len(masks)} objects")
        plt.show()

# ì‚¬ìš© ì˜ˆì œ
def demo_sam():
    sam = SAMSegmentation()
    
    # 1. í¬ì¸íŠ¸ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
    mask, score = sam.segment_with_points(
        'image.jpg',
        points=[(100, 200), (150, 250)],
        labels=[1, 1]  # ëª¨ë‘ í¬í•¨
    )
    print(f"Point segmentation score: {score:.3f}")
    
    # 2. ë°•ìŠ¤ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
    mask, score = sam.segment_with_box(
        'image.jpg',
        box=[50, 50, 200, 200]
    )
    print(f"Box segmentation score: {score:.3f}")
    
    # 3. ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜
    masks = sam.automatic_segmentation('image.jpg')
    print(f"Found {len(masks)} objects")
    
    # ì‹œê°í™”
    sam.visualize_masks('image.jpg', masks)
```

---

## ğŸ› ï¸ ì‹¤ìŠµ 4: ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬

### Gemini vs Llama Vision ë¹„êµ
```python
import time
import pandas as pd
from typing import Dict, List
import google.generativeai as genai
import together

class MultimodalBenchmark:
    """
    ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    """
    def __init__(self):
        # API ì„¤ì •
        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
        together.api_key = os.getenv('TOGETHER_API_KEY')
        
        self.models = {
            'gemini': genai.GenerativeModel('gemini-1.5-flash'),
            'llama': 'meta-llama/Llama-3.2-11B-Vision-Instruct'
        }
        
        self.results = []
    
    def benchmark_image_understanding(self, image_path, tasks):
        """
        ì´ë¯¸ì§€ ì´í•´ ëŠ¥ë ¥ ë²¤ì¹˜ë§ˆí¬
        
        Tasks:
        - Caption: ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
        - Q&A: ì§ˆë¬¸ ë‹µë³€
        - OCR: í…ìŠ¤íŠ¸ ì¶”ì¶œ
        - Object Detection: ê°ì²´ íƒì§€
        """
        results = {}
        
        for task_name, prompt in tasks.items():
            # Gemini í…ŒìŠ¤íŠ¸
            start = time.time()
            gemini_response = self.test_gemini(image_path, prompt)
            gemini_time = time.time() - start
            
            # Llama Vision í…ŒìŠ¤íŠ¸
            start = time.time()
            llama_response = self.test_llama(image_path, prompt)
            llama_time = time.time() - start
            
            results[task_name] = {
                'gemini': {
                    'response': gemini_response,
                    'time': gemini_time
                },
                'llama': {
                    'response': llama_response,
                    'time': llama_time
                }
            }
        
        return results
    
    def test_gemini(self, image_path, prompt):
        """Gemini í…ŒìŠ¤íŠ¸"""
        try:
            image = Image.open(image_path)
            response = self.models['gemini'].generate_content([prompt, image])
            return response.text
        except Exception as e:
            return f"Error: {e}"
    
    def test_llama(self, image_path, prompt):
        """Llama Vision í…ŒìŠ¤íŠ¸"""
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            import base64
            with open(image_path, "rb") as f:
                image_base64 = base64.b64encode(f.read()).decode()
            
            response = together.Complete.create(
                model=self.models['llama'],
                prompt=f"<image>{image_base64}</image>\n{prompt}",
                max_tokens=512
            )
            
            return response['output']['choices'][0]['text']
        except Exception as e:
            return f"Error: {e}"
    
    def compare_accuracy(self, ground_truth, predictions):
        """
        ì •í™•ë„ ë¹„êµ (human evaluation needed)
        """
        # ì‹¤ì œë¡œëŠ” human evaluationì´ë‚˜ ìë™ ë©”íŠ¸ë¦­ í•„ìš”
        pass
    
    def generate_report(self):
        """
        ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±
        """
        df = pd.DataFrame(self.results)
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„
        avg_times = df.groupby('model')['response_time'].mean()
        
        print("="*50)
        print("MULTIMODAL MODEL BENCHMARK REPORT")
        print("="*50)
        print("\nAverage Response Times:")
        print(avg_times)
        
        return df

# ì‚¬ìš© ì˜ˆì œ
def run_benchmark():
    benchmark = MultimodalBenchmark()
    
    # í…ŒìŠ¤íŠ¸ íƒœìŠ¤í¬ ì •ì˜
    tasks = {
        'caption': "Describe this image in detail.",
        'qa': "What is the main subject of this image?",
        'ocr': "Extract all text from this image.",
        'objects': "List all objects visible in this image."
    }
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = benchmark.benchmark_image_understanding('test.jpg', tasks)
    
    # ê²°ê³¼ ë¶„ì„
    for task, model_results in results.items():
        print(f"\n{task.upper()}:")
        for model, data in model_results.items():
            print(f"  {model}: {data['time']:.2f}s")
            print(f"    Response: {data['response'][:100]}...")
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = benchmark.generate_report()
```

---

## ğŸ¯ í†µí•© í”„ë¡œì íŠ¸: ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ ì•±

### ì™„ì „í•œ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ êµ¬í˜„
```python
import gradio as gr
import torch
from transformers import AutoModel, AutoImageProcessor
import timm
import numpy as np
from PIL import Image
import time

class ComprehensiveBenchmarkApp:
    """
    Vision Transformer ëª¨ë¸ê³¼ API ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì•±
    """
    def __init__(self):
        self.models = {}
        self.load_models()
        self.benchmark_results = []
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        # ViT
        self.models['vit'] = timm.create_model('vit_base_patch16_224', pretrained=True)
        self.models['vit'].eval()
        
        # DINOv2
        self.models['dino_processor'] = AutoImageProcessor.from_pretrained('facebook/dinov2-base')
        self.models['dino'] = AutoModel.from_pretrained('facebook/dinov2-base')
        
        # API ëª¨ë¸ë“¤
        # Gemini, Llama ë“±...
        
        print("All models loaded successfully!")
    
    def benchmark_classification(self, image):
        """ë¶„ë¥˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = {}
        
        # ViT í…ŒìŠ¤íŠ¸
        start = time.time()
        vit_preds = self.classify_with_vit(image)
        vit_time = time.time() - start
        results['ViT'] = {
            'predictions': vit_preds,
            'time': vit_time
        }
        
        return results
    
    def benchmark_feature_extraction(self, image):
        """íŠ¹ì§• ì¶”ì¶œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = {}
        
        # DINOv2 í…ŒìŠ¤íŠ¸
        start = time.time()
        dino_features = self.extract_with_dino(image)
        dino_time = time.time() - start
        results['DINOv2'] = {
            'features_shape': dino_features.shape,
            'time': dino_time
        }
        
        return results
    
    def benchmark_segmentation(self, image):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        # SAM í…ŒìŠ¤íŠ¸
        pass
    
    def classify_with_vit(self, image):
        """ViTë¡œ ë¶„ë¥˜"""
        # ì „ì²˜ë¦¬
        config = timm.data.resolve_data_config({}, model=self.models['vit'])
        transform = timm.data.create_transform(**config)
        img_tensor = transform(image).unsqueeze(0)
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            output = self.models['vit'](img_tensor)
            probs = torch.softmax(output, dim=1)
            top5_prob, top5_idx = probs.topk(5)
        
        return [(idx.item(), prob.item()) for idx, prob in zip(top5_idx[0], top5_prob[0])]
    
    def extract_with_dino(self, image):
        """DINOv2ë¡œ íŠ¹ì§• ì¶”ì¶œ"""
        inputs = self.models['dino_processor'](images=image, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.models['dino'](inputs['pixel_values'])
            features = outputs.last_hidden_state.mean(dim=1)
        
        return features.numpy()
    
    def create_comparison_plot(self, results):
        """ê²°ê³¼ ë¹„êµ ì‹œê°í™”"""
        import matplotlib.pyplot as plt
        
        models = list(results.keys())
        times = [r['time'] for r in results.values()]
        
        plt.figure(figsize=(10, 6))
        plt.bar(models, times)
        plt.xlabel('Model')
        plt.ylabel('Response Time (s)')
        plt.title('Model Performance Comparison')
        
        return plt.gcf()

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    app = ComprehensiveBenchmarkApp()
    
    with gr.Blocks(title="Vision Model Benchmark") as interface:
        gr.Markdown("""
        # ğŸ”¬ Vision Transformer & Multimodal Model Benchmark
        ### Compare ViT, DINOv2, SAM, and various multimodal APIs
        """)
        
        with gr.Tab("Classification Benchmark"):
            with gr.Row():
                input_image = gr.Image(label="Input Image", type="pil")
                output_results = gr.JSON(label="Classification Results")
            
            classify_btn = gr.Button("Run Classification Benchmark")
            classify_btn.click(
                app.benchmark_classification,
                inputs=[input_image],
                outputs=[output_results]
            )
        
        with gr.Tab("Feature Extraction"):
            with gr.Row():
                feat_image = gr.Image(label="Input Image", type="pil")
                feat_results = gr.JSON(label="Feature Extraction Results")
            
            feature_btn = gr.Button("Run Feature Extraction Benchmark")
            feature_btn.click(
                app.benchmark_feature_extraction,
                inputs=[feat_image],
                outputs=[feat_results]
            )
        
        with gr.Tab("Segmentation"):
            with gr.Row():
                seg_image = gr.Image(label="Input Image", type="pil")
                seg_output = gr.Image(label="Segmentation Result")
            
            seg_btn = gr.Button("Run Segmentation")
        
        with gr.Tab("Model Comparison"):
            gr.Markdown("""
            ### Head-to-Head Model Comparison
            Compare all models on the same image
            """)
            
            comp_image = gr.Image(label="Test Image", type="pil")
            comp_btn = gr.Button("Compare All Models")
            comp_output = gr.Plot(label="Performance Comparison")
    
    return interface

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    interface = create_gradio_interface()
    interface.launch(share=True)
```

---

## ğŸ“Š ì‹¤ìŠµ 5: ìµœì  ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### ëª¨ë¸ ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬
```python
def select_optimal_model(requirements):
    """
    ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ
    
    Requirements:
    - task: 'classification', 'detection', 'segmentation', 'caption'
    - speed: 'realtime', 'fast', 'normal'
    - accuracy: 'high', 'medium', 'low'
    - deployment: 'cloud', 'edge', 'mobile'
    """
    
    recommendations = {
        'classification': {
            'realtime': 'MobileViT or EfficientNet',
            'fast': 'ViT-Small or DeiT',
            'normal': 'ViT-Base or Swin Transformer'
        },
        'detection': {
            'realtime': 'YOLO or EfficientDet',
            'fast': 'DETR',
            'normal': 'Mask R-CNN with ViT backbone'
        },
        'segmentation': {
            'realtime': 'MobileNetV3 + DeepLab',
            'fast': 'SegFormer',
            'normal': 'SAM or Mask2Former'
        },
        'caption': {
            'realtime': 'CLIP + GPT-2',
            'fast': 'BLIP',
            'normal': 'Gemini or GPT-4V'
        }
    }
    
    task = requirements.get('task', 'classification')
    speed = requirements.get('speed', 'normal')
    
    recommendation = recommendations.get(task, {}).get(speed, 'ViT-Base')
    
    return recommendation

# ì‚¬ìš© ì˜ˆì œ
requirements = {
    'task': 'segmentation',
    'speed': 'fast',
    'accuracy': 'high',
    'deployment': 'cloud'
}

model = select_optimal_model(requirements)
print(f"Recommended model: {model}")
```

---

## ğŸ“ ê³¼ì œ

### Assignment 4: Vision Transformer ë§ˆìŠ¤í„°í•˜ê¸°

#### ê³¼ì œ ëª©í‘œ
1. ViTë¥¼ ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ê³  í•™ìŠµì‹œí‚¤ê¸°
2. DINOv2ì™€ SAMì„ í™œìš©í•œ ì‘ìš© í”„ë¡œê·¸ë¨ ê°œë°œ
3. ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ ì‘ì„±
4. í†µí•© ë²¤ì¹˜ë§ˆí¬ ì•± êµ¬ì¶• ë° ë°°í¬

#### í‰ê°€ ê¸°ì¤€
- **êµ¬í˜„ ì •í™•ë„ (30%)**: ViT êµ¬í˜„ì˜ ì •í™•ì„±
- **ì‘ìš© ì°½ì˜ì„± (25%)**: DINOv2/SAM í™œìš© ë°©ë²•
- **ì„±ëŠ¥ ë¶„ì„ (25%)**: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„
- **ì½”ë“œ í’ˆì§ˆ (10%)**: êµ¬ì¡°í™”, ë¬¸ì„œí™”
- **ë°°í¬ ì™„ì„±ë„ (10%)**: Gradio/HF Space ë°°í¬

#### ì œì¶œ ìš”êµ¬ì‚¬í•­
1. ì†ŒìŠ¤ ì½”ë“œ (GitHub)
2. í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
3. ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ (PDF)
4. ë°°í¬ëœ ì•± URL
5. ë°ëª¨ ë¹„ë””ì˜¤

---

## ğŸ“ í•™ìŠµ ì •ë¦¬

### í•µì‹¬ ê°œë… ë³µìŠµ
1. **Self-Attention**: ëª¨ë“  ìœ„ì¹˜ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµ
2. **Vision Transformer**: ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ë¡œ ì²˜ë¦¬
3. **DINO**: ë ˆì´ë¸” ì—†ëŠ” ìê¸°ì§€ë„í•™ìŠµ
4. **SAM**: í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë²”ìš© ì„¸ê·¸ë©˜í…Œì´ì…˜

### ì‹¤ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ViT êµ¬í˜„ ë° í•™ìŠµ
- [ ] DINOv2ë¡œ íŠ¹ì§• ì¶”ì¶œ
- [ ] SAMìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
- [ ] ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¹„êµ
- [ ] ë²¤ì¹˜ë§ˆí¬ ì•± ê°œë°œ
- [ ] ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”

### ë‹¤ìŒ ì£¼ ì˜ˆìŠµ
- YOLO ì‹œë¦¬ì¦ˆ ë°œì „ì‚¬
- One-stage vs Two-stage Detectors
- ì‹¤ì‹œê°„ ê°ì²´ íƒì§€

---

## ğŸ”— ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)
- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
- [Segment Anything](https://arxiv.org/abs/2304.02643)

### íŠœí† ë¦¬ì–¼
- [Vision Transformer from Scratch](https://github.com/lucidrains/vit-pytorch)
- [DINOv2 Official Repository](https://github.com/facebookresearch/dinov2)
- [SAM Demo](https://segment-anything.com/)

### ì½”ë“œ ì €ì¥ì†Œ
- [Week 4 ì™„ì „í•œ ì½”ë“œ](https://github.com/your-repo/week04)
- [ì‚¬ì „í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸](https://drive.google.com/your-checkpoints)

---

## ğŸ’¡ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²°ë²•

#### 1. ViT í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# í•´ê²°ë²•: Gradient Accumulation
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

#### 2. DINOv2 ëŠë¦° ì¶”ë¡  ì†ë„
```python
# í•´ê²°ë²•: ë°°ì¹˜ ì²˜ë¦¬ ë° mixed precision
with torch.cuda.amp.autocast():
    features = model(batch_images)
```

#### 3. SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ ë¬¸ì œ
```python
# í•´ê²°ë²•: ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ì¡°í•©
points = [(x1, y1), (x2, y2)]  # ì—¬ëŸ¬ í¬ì¸íŠ¸
boxes = [x1, y1, x2, y2]  # ë°”ìš´ë”© ë°•ìŠ¤ë„ ì¶”ê°€
masks = sam.predict(points=points, boxes=boxes)
```

---

**ì´ë²ˆ ì£¼ í•™ìŠµì„ ì™„ë£Œí•˜ì‹  ê²ƒì„ ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰**

Vision Transformerì˜ ì„¸ê³„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ë‹¤ìŒ ì£¼ì—ëŠ” ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì˜ ìµœì‹  ê¸°ìˆ ì„ íƒêµ¬í•´ë³´ê² ìŠµë‹ˆë‹¤.