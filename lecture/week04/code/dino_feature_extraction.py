"""
DINOv2ë¥¼ í™œìš©í•œ íŠ¹ì§• ì¶”ì¶œ
ìê¸°ì§€ë„í•™ìŠµ Vision Transformerì˜ í™œìš©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
import numpy as np
from typing import List, Dict, Optional, Union, Tuple
import requests
from io import BytesIO
from transformers import AutoImageProcessor, AutoModel
import gradio as gr
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns


class DINOv2FeatureExtractor:
    """DINOv2ë¥¼ í™œìš©í•œ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(
        self,
        model_name: str = "facebook/dinov2-base",
        device: str = None
    ):
        """
        Args:
            model_name: DINOv2 ëª¨ë¸ ì´ë¦„
            device: ì—°ì‚° ë””ë°”ì´ìŠ¤
        """
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
        print(f"Loading DINOv2 model: {model_name}")
        self.processor = AutoImageProcessor.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        
        # íŠ¹ì§• ì°¨ì›
        self.feature_dim = self.model.config.hidden_size
        
    def extract_features(
        self,
        images: Union[Image.Image, List[Image.Image], torch.Tensor],
        layer: str = 'last',
        pool: str = 'cls'
    ) -> torch.Tensor:
        """
        ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
        
        Args:
            images: ì…ë ¥ ì´ë¯¸ì§€(ë“¤)
            layer: ì¶”ì¶œí•  ë ˆì´ì–´ ('last', 'penultimate', ë˜ëŠ” ë ˆì´ì–´ ë²ˆí˜¸)
            pool: í’€ë§ ë°©ë²• ('cls', 'mean', 'max')
        
        Returns:
            íŠ¹ì§• ë²¡í„° [B, D]
        """
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if isinstance(images, Image.Image):
            images = [images]
        
        if isinstance(images, list):
            inputs = self.processor(images, return_tensors="pt")
            pixel_values = inputs['pixel_values'].to(self.device)
        else:
            pixel_values = images.to(self.device)
        
        # íŠ¹ì§• ì¶”ì¶œ
        with torch.no_grad():
            outputs = self.model(pixel_values, output_hidden_states=True)
        
        # ë ˆì´ì–´ ì„ íƒ
        if layer == 'last':
            features = outputs.last_hidden_state
        elif layer == 'penultimate':
            features = outputs.hidden_states[-2]
        elif isinstance(layer, int):
            features = outputs.hidden_states[layer]
        else:
            features = outputs.last_hidden_state
        
        # í’€ë§
        if pool == 'cls':
            # CLS í† í° ì‚¬ìš©
            features = features[:, 0]
        elif pool == 'mean':
            # í‰ê·  í’€ë§ (CLS í† í° ì œì™¸)
            features = features[:, 1:].mean(dim=1)
        elif pool == 'max':
            # ìµœëŒ€ í’€ë§ (CLS í† í° ì œì™¸)
            features = features[:, 1:].max(dim=1)[0]
        else:
            features = features[:, 0]
        
        return features
    
    def extract_patch_features(
        self,
        images: Union[Image.Image, List[Image.Image]],
        reshape: bool = True
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Tuple[int, int]]]:
        """
        íŒ¨ì¹˜ë³„ íŠ¹ì§• ì¶”ì¶œ (ê³µê°„ ì •ë³´ í¬í•¨)
        
        Args:
            images: ì…ë ¥ ì´ë¯¸ì§€
            reshape: 2D ê·¸ë¦¬ë“œë¡œ ì¬êµ¬ì„±í• ì§€ ì—¬ë¶€
        
        Returns:
            íŒ¨ì¹˜ íŠ¹ì§• [B, N, D] ë˜ëŠ” [B, H, W, D]
        """
        if isinstance(images, Image.Image):
            images = [images]
        
        inputs = self.processor(images, return_tensors="pt")
        pixel_values = inputs['pixel_values'].to(self.device)
        
        with torch.no_grad():
            outputs = self.model(pixel_values)
            patch_features = outputs.last_hidden_state[:, 1:]  # CLS í† í° ì œì™¸
        
        if reshape:
            B, N, D = patch_features.shape
            H = W = int(N ** 0.5)
            patch_features = patch_features.reshape(B, H, W, D)
            return patch_features, (H, W)
        
        return patch_features
    
    def compute_similarity(
        self,
        features1: torch.Tensor,
        features2: torch.Tensor,
        metric: str = 'cosine'
    ) -> torch.Tensor:
        """
        íŠ¹ì§• ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            features1: ì²« ë²ˆì§¸ íŠ¹ì§• ë²¡í„°
            features2: ë‘ ë²ˆì§¸ íŠ¹ì§• ë²¡í„°
            metric: ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ('cosine', 'euclidean', 'dot')
        """
        if metric == 'cosine':
            features1 = F.normalize(features1, p=2, dim=-1)
            features2 = F.normalize(features2, p=2, dim=-1)
            similarity = torch.matmul(features1, features2.T)
        elif metric == 'euclidean':
            similarity = -torch.cdist(features1, features2, p=2)
        elif metric == 'dot':
            similarity = torch.matmul(features1, features2.T)
        else:
            raise ValueError(f"Unknown metric: {metric}")
        
        return similarity


class DINOv2Applications:
    """DINOv2ë¥¼ í™œìš©í•œ ë‹¤ì–‘í•œ ì‘ìš©"""
    
    def __init__(self, feature_extractor: DINOv2FeatureExtractor):
        self.extractor = feature_extractor
        
    def semantic_segmentation(
        self,
        image: Image.Image,
        n_clusters: int = 5
    ) -> np.ndarray:
        """
        ë¹„ì§€ë„ ì˜ë¯¸ë¡ ì  ë¶„í• 
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
        
        Returns:
            ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ
        """
        # íŒ¨ì¹˜ íŠ¹ì§• ì¶”ì¶œ
        patch_features, (H, W) = self.extractor.extract_patch_features(image, reshape=True)
        patch_features = patch_features.squeeze(0).cpu().numpy()  # [H, W, D]
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        features_flat = patch_features.reshape(-1, patch_features.shape[-1])
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(features_flat)
        
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ ìƒì„±
        segmentation_map = labels.reshape(H, W)
        
        return segmentation_map
    
    def image_retrieval(
        self,
        query_image: Image.Image,
        database_images: List[Image.Image],
        top_k: int = 5
    ) -> List[Tuple[int, float]]:
        """
        ì´ë¯¸ì§€ ê²€ìƒ‰
        
        Args:
            query_image: ì¿¼ë¦¬ ì´ë¯¸ì§€
            database_images: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¯¸ì§€ë“¤
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼
        
        Returns:
            (ì¸ë±ìŠ¤, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ íŠ¹ì§• ì¶”ì¶œ
        query_features = self.extractor.extract_features(query_image)
        
        # ë°ì´í„°ë² ì´ìŠ¤ íŠ¹ì§• ì¶”ì¶œ
        db_features = []
        for img in database_images:
            features = self.extractor.extract_features(img)
            db_features.append(features)
        db_features = torch.stack(db_features).squeeze()
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self.extractor.compute_similarity(
            query_features,
            db_features,
            metric='cosine'
        ).squeeze()
        
        # Top-K ì„ íƒ
        top_k_values, top_k_indices = torch.topk(similarities, min(top_k, len(database_images)))
        
        results = [(idx.item(), val.item()) for idx, val in zip(top_k_indices, top_k_values)]
        
        return results
    
    def visual_correspondence(
        self,
        image1: Image.Image,
        image2: Image.Image,
        n_points: int = 10
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        ì´ë¯¸ì§€ ê°„ ì‹œê°ì  ëŒ€ì‘ì  ì°¾ê¸°
        
        Args:
            image1: ì²« ë²ˆì§¸ ì´ë¯¸ì§€
            image2: ë‘ ë²ˆì§¸ ì´ë¯¸ì§€
            n_points: ëŒ€ì‘ì  ìˆ˜
        
        Returns:
            (ì´ë¯¸ì§€1 í¬ì¸íŠ¸, ì´ë¯¸ì§€2 í¬ì¸íŠ¸, ë§¤ì¹­ ìŠ¤ì½”ì–´)
        """
        # íŒ¨ì¹˜ íŠ¹ì§• ì¶”ì¶œ
        features1, (H1, W1) = self.extractor.extract_patch_features(image1, reshape=True)
        features2, (H2, W2) = self.extractor.extract_patch_features(image2, reshape=True)
        
        features1 = features1.squeeze(0).reshape(-1, features1.shape[-1])  # [N1, D]
        features2 = features2.squeeze(0).reshape(-1, features2.shape[-1])  # [N2, D]
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        similarity_matrix = self.extractor.compute_similarity(
            features1,
            features2,
            metric='cosine'
        )
        
        # ìƒìœ„ ëŒ€ì‘ì  ì°¾ê¸°
        scores, indices = torch.topk(similarity_matrix.flatten(), n_points)
        
        # ì¢Œí‘œ ë³€í™˜
        indices_2d = torch.stack([indices // similarity_matrix.shape[1], 
                                  indices % similarity_matrix.shape[1]], dim=1)
        
        points1 = []
        points2 = []
        match_scores = []
        
        for i in range(n_points):
            idx1 = indices_2d[i, 0].item()
            idx2 = indices_2d[i, 1].item()
            
            # íŒ¨ì¹˜ ì¸ë±ìŠ¤ë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            y1, x1 = idx1 // W1, idx1 % W1
            y2, x2 = idx2 // W2, idx2 % W2
            
            # ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
            h1, w1 = image1.size
            h2, w2 = image2.size
            
            points1.append([x1 * w1 // W1, y1 * h1 // H1])
            points2.append([x2 * w2 // W2, y2 * h2 // H2])
            match_scores.append(scores[i].item())
        
        return np.array(points1), np.array(points2), np.array(match_scores)
    
    def visualize_features(
        self,
        image: Image.Image,
        method: str = 'pca'
    ) -> Image.Image:
        """
        íŠ¹ì§• ì‹œê°í™”
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            method: ì‹œê°í™” ë°©ë²• ('pca', 'attention')
        
        Returns:
            ì‹œê°í™”ëœ ì´ë¯¸ì§€
        """
        # íŒ¨ì¹˜ íŠ¹ì§• ì¶”ì¶œ
        patch_features, (H, W) = self.extractor.extract_patch_features(image, reshape=True)
        patch_features = patch_features.squeeze(0).cpu().numpy()
        
        if method == 'pca':
            # PCAë¥¼ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ
            features_flat = patch_features.reshape(-1, patch_features.shape[-1])
            pca = PCA(n_components=3)
            features_pca = pca.fit_transform(features_flat)
            
            # RGB ì´ë¯¸ì§€ë¡œ ë³€í™˜
            features_rgb = features_pca.reshape(H, W, 3)
            features_rgb = (features_rgb - features_rgb.min()) / (features_rgb.max() - features_rgb.min())
            features_rgb = (features_rgb * 255).astype(np.uint8)
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            from PIL import Image as PILImage
            visualization = PILImage.fromarray(features_rgb)
            visualization = visualization.resize(image.size, PILImage.LANCZOS)
            
        elif method == 'attention':
            # ìê¸° ì–´í…ì…˜ ë§µ ê³„ì‚°
            features_flat = patch_features.reshape(-1, patch_features.shape[-1])
            features_norm = features_flat / np.linalg.norm(features_flat, axis=-1, keepdims=True)
            attention_map = np.matmul(features_norm, features_norm.T)
            
            # í‰ê·  ì–´í…ì…˜
            avg_attention = attention_map.mean(axis=0).reshape(H, W)
            
            # íˆíŠ¸ë§µ ìƒì„±
            plt.figure(figsize=(10, 5))
            
            plt.subplot(1, 2, 1)
            plt.imshow(image)
            plt.title('Original Image')
            plt.axis('off')
            
            plt.subplot(1, 2, 2)
            plt.imshow(avg_attention, cmap='hot')
            plt.title('Attention Heatmap')
            plt.colorbar()
            plt.axis('off')
            
            plt.tight_layout()
            
            # ì´ë¯¸ì§€ë¡œ ë³€í™˜
            import io
            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=100)
            buf.seek(0)
            visualization = Image.open(buf)
            plt.close()
        else:
            visualization = image
        
        return visualization


def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    # DINOv2 ì´ˆê¸°í™”
    extractor = DINOv2FeatureExtractor()
    apps = DINOv2Applications(extractor)
    
    def process_segmentation(image, n_clusters):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬"""
        segmap = apps.semantic_segmentation(image, n_clusters)
        
        # ì»¬ëŸ¬ë§µ ì ìš©
        plt.figure(figsize=(10, 5))
        
        plt.subplot(1, 2, 1)
        plt.imshow(image)
        plt.title('Original Image')
        plt.axis('off')
        
        plt.subplot(1, 2, 2)
        plt.imshow(segmap, cmap='tab20')
        plt.title(f'Segmentation (K={n_clusters})')
        plt.axis('off')
        
        plt.tight_layout()
        
        import io
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        buf.seek(0)
        result = Image.open(buf)
        plt.close()
        
        return result
    
    def process_visualization(image, method):
        """íŠ¹ì§• ì‹œê°í™” ì²˜ë¦¬"""
        return apps.visualize_features(image, method)
    
    with gr.Blocks(title="DINOv2 Feature Extraction") as app:
        gr.Markdown("# ğŸ¦– DINOv2 Feature Extraction Demo")
        
        with gr.Tab("Semantic Segmentation"):
            with gr.Row():
                with gr.Column():
                    seg_image = gr.Image(type="pil", label="Input Image")
                    seg_clusters = gr.Slider(
                        minimum=2,
                        maximum=10,
                        value=5,
                        step=1,
                        label="Number of Clusters"
                    )
                    seg_button = gr.Button("Segment")
                
                seg_output = gr.Image(label="Segmentation Result")
            
            seg_button.click(
                process_segmentation,
                inputs=[seg_image, seg_clusters],
                outputs=seg_output
            )
        
        with gr.Tab("Feature Visualization"):
            with gr.Row():
                with gr.Column():
                    viz_image = gr.Image(type="pil", label="Input Image")
                    viz_method = gr.Radio(
                        choices=["pca", "attention"],
                        value="pca",
                        label="Visualization Method"
                    )
                    viz_button = gr.Button("Visualize")
                
                viz_output = gr.Image(label="Visualization")
            
            viz_button.click(
                process_visualization,
                inputs=[viz_image, viz_method],
                outputs=viz_output
            )
        
        gr.Markdown("""
        ## About DINOv2
        
        DINOv2ëŠ” ìê¸°ì§€ë„í•™ìŠµìœ¼ë¡œ í›ˆë ¨ëœ Vision Transformerì…ë‹ˆë‹¤.
        - ë¼ë²¨ ì—†ì´ í•™ìŠµë¨
        - ë²”ìš©ì ì¸ ì‹œê° íŠ¹ì§• ì œê³µ
        - ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì— í™œìš© ê°€ëŠ¥
        
        ### Applications
        1. **Semantic Segmentation**: ë¹„ì§€ë„ í´ëŸ¬ìŠ¤í„°ë§
        2. **Image Retrieval**: ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰
        3. **Visual Correspondence**: ëŒ€ì‘ì  ì°¾ê¸°
        4. **Feature Visualization**: íŠ¹ì§• ì‹œê°í™”
        """)
    
    return app


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("Initializing DINOv2...")
    extractor = DINOv2FeatureExtractor()
    apps = DINOv2Applications(extractor)
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    test_image = Image.new('RGB', (224, 224), color='red')
    
    # íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    features = extractor.extract_features(test_image)
    print(f"Feature shape: {features.shape}")
    
    # íŒ¨ì¹˜ íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    patch_features, (H, W) = extractor.extract_patch_features(test_image)
    print(f"Patch features shape: {patch_features.shape}")
    print(f"Grid size: {H}x{W}")
    
    # Gradio ì•± ì‹¤í–‰ (ì‹¤ì œ í™˜ê²½ì—ì„œ)
    # app = create_gradio_interface()
    # app.launch()
    
    print("\nDINOv2 feature extraction setup complete!")