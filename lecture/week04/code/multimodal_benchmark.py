"""
ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
Gemini vs GPT-4V vs Llama Vision ì„±ëŠ¥ ë¹„êµ
"""

import time
import json
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any
import gradio as gr
from dataclasses import dataclass
from datetime import datetime
import asyncio
import aiohttp


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í´ë˜ìŠ¤"""
    model_name: str
    task_type: str
    response_time: float
    accuracy_score: float
    cost: float
    response_text: str
    metadata: Dict[str, Any]


class MultimodalModelBenchmark:
    """ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.models = {
            'gemini': GeminiVisionAPI(),
            'gpt4v': GPT4VisionAPI(),
            'llama': LlamaVisionAPI(),
            'claude': ClaudeVisionAPI()
        }
        
        self.tasks = {
            'caption': "Generate a detailed caption for this image",
            'vqa': "Answer the following question about the image: {}",
            'ocr': "Extract all text from this image",
            'object_detection': "List all objects visible in this image",
            'scene_understanding': "Describe the scene, including context and relationships",
            'reasoning': "What is unusual or noteworthy about this image?"
        }
        
        self.results = []
        
    async def benchmark_single_model(
        self,
        model_name: str,
        image: Image.Image,
        task_type: str,
        task_prompt: str = None
    ) -> BenchmarkResult:
        """
        ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            image: ì…ë ¥ ì´ë¯¸ì§€
            task_type: íƒœìŠ¤í¬ íƒ€ì…
            task_prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
        
        Returns:
            ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        model = self.models[model_name]
        
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        if task_prompt is None:
            task_prompt = self.tasks.get(task_type, "Describe this image")
        
        # ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        try:
            # ëª¨ë¸ í˜¸ì¶œ
            response = await model.process_image(image, task_prompt)
            response_time = time.time() - start_time
            
            # ì •í™•ë„ í‰ê°€ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ground truthì™€ ë¹„êµ)
            accuracy_score = self.evaluate_response(response, task_type)
            
            # ë¹„ìš© ê³„ì‚°
            cost = model.calculate_cost(len(task_prompt), len(response))
            
            result = BenchmarkResult(
                model_name=model_name,
                task_type=task_type,
                response_time=response_time,
                accuracy_score=accuracy_score,
                cost=cost,
                response_text=response,
                metadata={
                    'timestamp': datetime.now().isoformat(),
                    'image_size': image.size,
                    'prompt_length': len(task_prompt)
                }
            )
            
        except Exception as e:
            result = BenchmarkResult(
                model_name=model_name,
                task_type=task_type,
                response_time=-1,
                accuracy_score=0,
                cost=0,
                response_text=f"Error: {str(e)}",
                metadata={'error': str(e)}
            )
        
        return result
    
    async def benchmark_all_models(
        self,
        image: Image.Image,
        task_type: str,
        task_prompt: str = None
    ) -> List[BenchmarkResult]:
        """
        ëª¨ë“  ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            task_type: íƒœìŠ¤í¬ íƒ€ì…
            task_prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
        
        Returns:
            ëª¨ë“  ëª¨ë¸ì˜ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        tasks = []
        for model_name in self.models.keys():
            task = self.benchmark_single_model(model_name, image, task_type, task_prompt)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        self.results.extend(results)
        return results
    
    def evaluate_response(self, response: str, task_type: str) -> float:
        """
        ì‘ë‹µ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        
        Args:
            response: ëª¨ë¸ ì‘ë‹µ
            task_type: íƒœìŠ¤í¬ íƒ€ì…
        
        Returns:
            ì •í™•ë„ ì ìˆ˜ (0-1)
        """
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ground truthì™€ ë¹„êµ
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ì‘ë‹µ ê¸¸ì´ ì²´í¬
        if len(response) > 50:
            score += 0.1
        
        # íƒœìŠ¤í¬ë³„ í‰ê°€
        if task_type == 'caption':
            # ìº¡ì…˜ í’ˆì§ˆ ì²´í¬
            if any(word in response.lower() for word in ['image', 'shows', 'contains']):
                score += 0.2
        elif task_type == 'ocr':
            # OCR ê²°ê³¼ ì²´í¬
            if len(response.split()) > 5:
                score += 0.2
        elif task_type == 'object_detection':
            # ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì²´í¬
            if ',' in response or '\n' in response:
                score += 0.2
        
        # ì—ëŸ¬ ì²´í¬
        if 'error' in response.lower() or 'unable' in response.lower():
            score -= 0.3
        
        return min(max(score, 0), 1)  # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
    
    def generate_report(self) -> pd.DataFrame:
        """
        ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±
        
        Returns:
            ê²°ê³¼ DataFrame
        """
        if not self.results:
            return pd.DataFrame()
        
        data = []
        for result in self.results:
            data.append({
                'Model': result.model_name,
                'Task': result.task_type,
                'Response Time (s)': result.response_time,
                'Accuracy': result.accuracy_score,
                'Cost ($)': result.cost,
                'Timestamp': result.metadata.get('timestamp', '')
            })
        
        df = pd.DataFrame(data)
        return df
    
    def visualize_results(self) -> plt.Figure:
        """
        ê²°ê³¼ ì‹œê°í™”
        
        Returns:
            matplotlib Figure
        """
        df = self.generate_report()
        
        if df.empty:
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            ax.text(0.5, 0.5, 'No results to display', 
                   ha='center', va='center', fontsize=16)
            return fig
        
        # 4ê°œ ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. ì‘ë‹µ ì‹œê°„ ë¹„êµ
        ax1 = axes[0, 0]
        df_pivot = df.pivot_table(values='Response Time (s)', 
                                  index='Task', columns='Model', aggfunc='mean')
        df_pivot.plot(kind='bar', ax=ax1)
        ax1.set_title('Average Response Time by Task')
        ax1.set_ylabel('Time (seconds)')
        ax1.legend(title='Model')
        ax1.grid(True, alpha=0.3)
        
        # 2. ì •í™•ë„ ë¹„êµ
        ax2 = axes[0, 1]
        df_pivot = df.pivot_table(values='Accuracy', 
                                  index='Task', columns='Model', aggfunc='mean')
        df_pivot.plot(kind='bar', ax=ax2)
        ax2.set_title('Average Accuracy by Task')
        ax2.set_ylabel('Accuracy Score')
        ax2.legend(title='Model')
        ax2.grid(True, alpha=0.3)
        
        # 3. ë¹„ìš© ë¹„êµ
        ax3 = axes[1, 0]
        df_cost = df.groupby('Model')['Cost ($)'].sum()
        df_cost.plot(kind='pie', ax=ax3, autopct='%1.2f%%')
        ax3.set_title('Total Cost Distribution')
        ax3.set_ylabel('')
        
        # 4. ì¢…í•© ì ìˆ˜ (ì†ë„ì™€ ì •í™•ë„ ì¡°í•©)
        ax4 = axes[1, 1]
        df['Combined Score'] = df['Accuracy'] / (df['Response Time (s)'] + 0.1)
        df_pivot = df.pivot_table(values='Combined Score', 
                                  index='Model', columns='Task', aggfunc='mean')
        sns.heatmap(df_pivot, annot=True, fmt='.2f', ax=ax4, cmap='YlOrRd')
        ax4.set_title('Combined Performance Score (Accuracy/Time)')
        
        plt.tight_layout()
        return fig


# API êµ¬í˜„ (ë°ëª¨ìš©)
class GeminiVisionAPI:
    """Google Gemini Vision API"""
    
    async def process_image(self, image: Image.Image, prompt: str) -> str:
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” google.generativeai ì‚¬ìš©
        await asyncio.sleep(0.5)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        return f"Gemini response for: {prompt[:50]}... The image shows various objects and scenes."
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        # Gemini pricing (ì˜ˆì‹œ)
        return (input_tokens * 0.00001 + output_tokens * 0.00003)


class GPT4VisionAPI:
    """OpenAI GPT-4 Vision API"""
    
    async def process_image(self, image: Image.Image, prompt: str) -> str:
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” openai ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        await asyncio.sleep(0.8)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        return f"GPT-4V analysis: {prompt[:50]}... This image contains detailed visual information."
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        # GPT-4V pricing (ì˜ˆì‹œ)
        return (input_tokens * 0.00003 + output_tokens * 0.00006)


class LlamaVisionAPI:
    """Meta Llama Vision API (Together AI)"""
    
    async def process_image(self, image: Image.Image, prompt: str) -> str:
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” together ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        await asyncio.sleep(0.6)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        return f"Llama Vision output: {prompt[:50]}... The visual elements suggest multiple interpretations."
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        # Together AI pricing (ì˜ˆì‹œ)
        return (input_tokens * 0.000008 + output_tokens * 0.000016)


class ClaudeVisionAPI:
    """Anthropic Claude Vision API"""
    
    async def process_image(self, image: Image.Image, prompt: str) -> str:
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        await asyncio.sleep(0.7)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        return f"Claude analysis: {prompt[:50]}... I can see several interesting aspects in this image."
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        # Claude pricing (ì˜ˆì‹œ)
        return (input_tokens * 0.00002 + output_tokens * 0.00004)


def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    benchmark = MultimodalModelBenchmark()
    
    async def run_benchmark(image, task_type, custom_prompt, models_to_test):
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        if image is None:
            return None, None, "Please upload an image first"
        
        # ì„ íƒëœ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
        original_models = benchmark.models.copy()
        benchmark.models = {k: v for k, v in original_models.items() if k in models_to_test}
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        prompt = custom_prompt if custom_prompt else None
        results = await benchmark.benchmark_all_models(image, task_type, prompt)
        
        # ì›ë˜ ëª¨ë¸ ë³µì›
        benchmark.models = original_models
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        df = benchmark.generate_report()
        
        # ì‹œê°í™”
        fig = benchmark.visualize_results()
        
        # í…ìŠ¤íŠ¸ ê²°ê³¼
        result_text = "## Benchmark Results\n\n"
        for result in results:
            result_text += f"### {result.model_name.upper()}\n"
            result_text += f"- Response Time: {result.response_time:.2f}s\n"
            result_text += f"- Accuracy Score: {result.accuracy_score:.2%}\n"
            result_text += f"- Cost: ${result.cost:.6f}\n"
            result_text += f"- Response: {result.response_text[:200]}...\n\n"
        
        return fig, df, result_text
    
    with gr.Blocks(title="Multimodal Model Benchmark") as app:
        gr.Markdown("# ğŸ† Multimodal Model Benchmark")
        gr.Markdown("Compare Gemini, GPT-4V, Llama Vision, and Claude on various vision tasks")
        
        with gr.Row():
            with gr.Column(scale=1):
                input_image = gr.Image(type="pil", label="Input Image")
                task_type = gr.Radio(
                    choices=['caption', 'vqa', 'ocr', 'object_detection', 
                            'scene_understanding', 'reasoning'],
                    value='caption',
                    label="Task Type"
                )
                custom_prompt = gr.Textbox(
                    label="Custom Prompt (Optional)",
                    placeholder="Leave empty to use default prompt for selected task"
                )
                models_to_test = gr.CheckboxGroup(
                    choices=['gemini', 'gpt4v', 'llama', 'claude'],
                    value=['gemini', 'gpt4v', 'llama', 'claude'],
                    label="Models to Test"
                )
                run_button = gr.Button("Run Benchmark", variant="primary")
            
            with gr.Column(scale=2):
                output_plot = gr.Plot(label="Benchmark Visualization")
                output_table = gr.Dataframe(label="Results Table")
                output_text = gr.Markdown(label="Detailed Results")
        
        run_button.click(
            lambda *args: asyncio.run(run_benchmark(*args)),
            inputs=[input_image, task_type, custom_prompt, models_to_test],
            outputs=[output_plot, output_table, output_text]
        )
        
        gr.Markdown("""
        ## ğŸ“Š Benchmark Metrics
        
        ### Performance Metrics
        - **Response Time**: API call latency (lower is better)
        - **Accuracy Score**: Task-specific quality metric (higher is better)
        - **Cost**: API usage cost in USD (lower is better)
        - **Combined Score**: Accuracy / Response Time (higher is better)
        
        ### Task Types
        - **Caption**: Generate image descriptions
        - **VQA**: Visual Question Answering
        - **OCR**: Optical Character Recognition
        - **Object Detection**: Identify objects in image
        - **Scene Understanding**: Comprehensive scene analysis
        - **Reasoning**: Logical reasoning about visual content
        
        ### Model Comparison
        | Model | Strengths | Best For |
        |-------|-----------|----------|
        | **Gemini** | Fast, multilingual | General purpose, high volume |
        | **GPT-4V** | High accuracy, reasoning | Complex analysis, creative tasks |
        | **Llama** | Open source, customizable | Research, custom deployments |
        | **Claude** | Safety, detailed analysis | Content moderation, careful analysis |
        
        ### Tips
        - Test with diverse images for comprehensive comparison
        - Run multiple times to account for API variability
        - Consider both performance and cost for production use
        """)
    
    return app


if __name__ == "__main__":
    print("Multimodal Model Benchmark System")
    print("=" * 50)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    benchmark = MultimodalModelBenchmark()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    test_image = Image.new('RGB', (512, 512), color='white')
    
    # ë™ê¸° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë˜í¼
    async def test_benchmark():
        # ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        result = await benchmark.benchmark_single_model(
            'gemini',
            test_image,
            'caption'
        )
        print(f"Single model test - {result.model_name}:")
        print(f"  Response time: {result.response_time:.2f}s")
        print(f"  Accuracy: {result.accuracy_score:.2%}")
        print(f"  Cost: ${result.cost:.6f}")
        
        # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("\nRunning full benchmark...")
        results = await benchmark.benchmark_all_models(
            test_image,
            'caption'
        )
        
        print(f"\nBenchmarked {len(results)} models")
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        df = benchmark.generate_report()
        print("\nBenchmark Report:")
        print(df)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_benchmark())
    
    # Gradio ì•± ì‹¤í–‰ (ì‹¤ì œ í™˜ê²½ì—ì„œ)
    # app = create_gradio_interface()
    # app.launch()
    
    print("\nMultimodal benchmark system ready!")