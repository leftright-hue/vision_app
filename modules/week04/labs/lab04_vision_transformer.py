#!/usr/bin/env python3
"""
Week 4 Lab: Vision Transformer ì‹¤ìŠµ
Vision Transformer (ViT) êµ¬í˜„ê³¼ Self-Attention ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„

ì´ ì‹¤ìŠµì—ì„œëŠ”:
1. ViT ì•„í‚¤í…ì²˜ ì™„ì „ êµ¬í˜„
2. Self-Attention ì‹œê°í™”
3. íŒ¨ì¹˜ ì„ë² ë”© ë¶„ì„
4. CNN vs ViT ì„±ëŠ¥ ë¹„êµ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import torchvision.transforms as transforms
from transformers import ViTModel, ViTImageProcessor
import time
import warnings
warnings.filterwarnings('ignore')

# ì„¤ì •
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {DEVICE}")

class PatchEmbedding(nn.Module):
    """
    ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.patch_dim = in_channels * patch_size * patch_size
        
        # ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ íŒ¨ì¹˜ ì„ë² ë”© êµ¬í˜„ (ë” íš¨ìœ¨ì )
        self.projection = nn.Conv2d(
            in_channels, embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
        
    def forward(self, x):
        # x: [B, C, H, W]
        B, C, H, W = x.shape
        
        # íŒ¨ì¹˜ ì„ë² ë”©
        x = self.projection(x)  # [B, embed_dim, H//patch_size, W//patch_size]
        x = x.flatten(2)        # [B, embed_dim, num_patches]
        x = x.transpose(1, 2)   # [B, num_patches, embed_dim]
        
        return x

class MultiHeadSelfAttention(nn.Module):
    """
    Multi-Head Self-Attention êµ¬í˜„
    """
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        assert embed_dim % num_heads == 0
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scale = self.head_dim ** -0.5
        
        # Q, K, Vë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ê¸° ìœ„í•œ ì„ í˜• ë ˆì´ì–´
        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, return_attention=False):
        B, N, C = x.shape
        
        # Q, K, V ê³„ì‚°
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, num_heads, N, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Attention ì ìˆ˜ ê³„ì‚°
        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, num_heads, N, N]
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)
        
        # Valueì™€ ê°€ì¤‘í•©
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        
        if return_attention:
            return x, attn
        return x

class TransformerBlock(nn.Module):
    """
    Transformer ë¸”ë¡ (Self-Attention + MLP)
    """
    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)
        
        self.norm2 = nn.LayerNorm(embed_dim)
        mlp_hidden_dim = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, return_attention=False):
        # Self-Attention with residual connection
        if return_attention:
            attn_output, attn_weights = self.attn(self.norm1(x), return_attention=True)
            x = x + attn_output
        else:
            x = x + self.attn(self.norm1(x))
            attn_weights = None
        
        # MLP with residual connection
        x = x + self.mlp(self.norm2(x))
        
        if return_attention:
            return x, attn_weights
        return x

class VisionTransformer(nn.Module):
    """
    ì™„ì „í•œ Vision Transformer êµ¬í˜„
    """
    def __init__(self, 
                 img_size=224,
                 patch_size=16,
                 num_classes=1000,
                 embed_dim=768,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.0,
                 dropout=0.1):
        super().__init__()
        
        self.num_classes = num_classes
        self.embed_dim = embed_dim
        self.num_patches = (img_size // patch_size) ** 2
        
        # 1. íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)
        
        # 2. í´ë˜ìŠ¤ í† í°
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # 3. ìœ„ì¹˜ ì„ë² ë”©
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))
        self.pos_dropout = nn.Dropout(dropout)
        
        # 4. Transformer ë¸”ë¡ë“¤
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)
            for _ in range(depth)
        ])
        
        # 5. ì •ê·œí™” ë° ë¶„ë¥˜ í—¤ë“œ
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.init_weights()
    
    def init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        
        # ë¶„ë¥˜ í—¤ë“œ ì´ˆê¸°í™”
        if isinstance(self.head, nn.Linear):
            nn.init.zeros_(self.head.weight)
            nn.init.zeros_(self.head.bias)
    
    def forward(self, x, return_attention=False):
        B = x.shape[0]
        
        # 1. íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embed(x)  # [B, num_patches, embed_dim]
        
        # 2. í´ë˜ìŠ¤ í† í° ì¶”ê°€
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # [B, num_patches+1, embed_dim]
        
        # 3. ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€
        x = x + self.pos_embed
        x = self.pos_dropout(x)
        
        # 4. Transformer ë¸”ë¡ë“¤ í†µê³¼
        attention_weights = []
        for i, block in enumerate(self.blocks):
            if return_attention and i == len(self.blocks) - 1:  # ë§ˆì§€ë§‰ ë¸”ë¡ì˜ attentionë§Œ ë°˜í™˜
                x, attn = block(x, return_attention=True)
                attention_weights.append(attn)
            else:
                x = block(x)
        
        # 5. ì •ê·œí™”
        x = self.norm(x)
        
        # 6. ë¶„ë¥˜ (CLS í† í° ì‚¬ìš©)
        cls_token_final = x[:, 0]
        logits = self.head(cls_token_final)
        
        if return_attention:
            return logits, attention_weights[0] if attention_weights else None
        return logits

class ViTAnalyzer:
    """
    ViT ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬
    """
    def __init__(self, model):
        self.model = model
        self.model.eval()
    
    def visualize_patches(self, image, patch_size=16):
        """ì´ë¯¸ì§€ íŒ¨ì¹˜ ë¶„í•  ì‹œê°í™”"""
        if isinstance(image, torch.Tensor):
            image = transforms.ToPILImage()(image.squeeze())
        
        img_array = np.array(image)
        H, W = img_array.shape[:2]
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 12))
        
        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0, 0].imshow(img_array)
        axes[0, 0].set_title('ì›ë³¸ ì´ë¯¸ì§€')
        axes[0, 0].axis('off')
        
        # íŒ¨ì¹˜ ê·¸ë¦¬ë“œ í‘œì‹œ
        axes[0, 1].imshow(img_array)
        for i in range(0, H, patch_size):
            axes[0, 1].axhline(y=i, color='red', linewidth=1)
        for j in range(0, W, patch_size):
            axes[0, 1].axvline(x=j, color='red', linewidth=1)
        axes[0, 1].set_title(f'íŒ¨ì¹˜ ê·¸ë¦¬ë“œ ({patch_size}x{patch_size})')
        axes[0, 1].axis('off')
        
        # ê°œë³„ íŒ¨ì¹˜ë“¤ ìƒ˜í”Œ
        num_patches_h = H // patch_size
        num_patches_w = W // patch_size
        
        # ëª‡ ê°œ íŒ¨ì¹˜ë§Œ ì„ íƒí•´ì„œ í‘œì‹œ
        sample_patches = []
        positions = [(2, 2), (2, 8), (8, 2), (8, 8)]  # 4ê°œ ì½”ë„ˆ ê·¼ì²˜
        
        for idx, (i, j) in enumerate(positions):
            if i < num_patches_h and j < num_patches_w:
                patch = img_array[i*patch_size:(i+1)*patch_size, 
                                j*patch_size:(j+1)*patch_size]
                sample_patches.append(patch)
        
        # íŒ¨ì¹˜ë“¤ì„ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ í•©ì¹˜ê¸°
        if sample_patches:
            patch_grid = np.concatenate([
                np.concatenate(sample_patches[:2], axis=1),
                np.concatenate(sample_patches[2:], axis=1)
            ], axis=0)
            
            axes[1, 0].imshow(patch_grid)
            axes[1, 0].set_title('ìƒ˜í”Œ íŒ¨ì¹˜ë“¤')
            axes[1, 0].axis('off')
        
        # íŒ¨ì¹˜ ì„ë² ë”© ì°¨ì› ì •ë³´
        embed_info = f"""íŒ¨ì¹˜ ì •ë³´:
        - ì´ë¯¸ì§€ í¬ê¸°: {H}x{W}
        - íŒ¨ì¹˜ í¬ê¸°: {patch_size}x{patch_size}
        - íŒ¨ì¹˜ ê°œìˆ˜: {num_patches_h}x{num_patches_w} = {num_patches_h * num_patches_w}
        - íŒ¨ì¹˜ ì°¨ì›: {3 * patch_size * patch_size}
        - ì„ë² ë”© ì°¨ì›: {self.model.embed_dim}"""
        
        axes[1, 1].text(0.1, 0.5, embed_info, fontsize=12, 
                        verticalalignment='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('íŒ¨ì¹˜ ì„ë² ë”© ì •ë³´')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def visualize_attention(self, image, layer_idx=-1, head_idx=0):
        """Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”"""
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        if isinstance(image, Image.Image):
            input_tensor = transform(image).unsqueeze(0).to(DEVICE)
        else:
            input_tensor = image.to(DEVICE)
        
        # Attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ
        with torch.no_grad():
            _, attention_weights = self.model(input_tensor, return_attention=True)
        
        if attention_weights is None:
            print("Attention ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # íŠ¹ì • í—¤ë“œì˜ attention ê°€ì¤‘ì¹˜ ì„ íƒ
        attn = attention_weights[0, head_idx].cpu().numpy()  # [seq_len, seq_len]
        
        # CLS í† í°ì— ëŒ€í•œ attention (ì²« ë²ˆì§¸ í–‰)
        cls_attention = attn[0, 1:]  # CLS í† í°ì´ ë‹¤ë¥¸ íŒ¨ì¹˜ë“¤ì— ì£¼ëŠ” attention
        
        # Attentionì„ ì´ë¯¸ì§€ í˜•íƒœë¡œ reshape
        grid_size = int(np.sqrt(len(cls_attention)))
        attention_map = cls_attention.reshape(grid_size, grid_size)
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # ì›ë³¸ ì´ë¯¸ì§€
        if isinstance(image, Image.Image):
            axes[0, 0].imshow(image)
        else:
            # ì •ê·œí™” í•´ì œ
            img_denorm = input_tensor.squeeze().cpu()
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            img_denorm = img_denorm * std + mean
            img_denorm = torch.clamp(img_denorm, 0, 1)
            axes[0, 0].imshow(img_denorm.permute(1, 2, 0))
        
        axes[0, 0].set_title('ì›ë³¸ ì´ë¯¸ì§€')
        axes[0, 0].axis('off')
        
        # Attention íˆíŠ¸ë§µ
        im1 = axes[0, 1].imshow(attention_map, cmap='hot', interpolation='nearest')
        axes[0, 1].set_title(f'CLS Token Attention (Head {head_idx})')
        plt.colorbar(im1, ax=axes[0, 1])
        
        # Attentionì„ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
        from scipy.ndimage import zoom
        attention_upsampled = zoom(attention_map, (224/grid_size, 224/grid_size), order=1)
        
        # ì›ë³¸ ì´ë¯¸ì§€ì™€ ì˜¤ë²„ë ˆì´
        axes[0, 2].imshow(image if isinstance(image, Image.Image) else 
                         img_denorm.permute(1, 2, 0), alpha=0.7)
        im2 = axes[0, 2].imshow(attention_upsampled, cmap='hot', alpha=0.5)
        axes[0, 2].set_title('Attention Overlay')
        axes[0, 2].axis('off')
        
        # ì „ì²´ attention í–‰ë ¬
        im3 = axes[1, 0].imshow(attn, cmap='Blues')
        axes[1, 0].set_title('ì „ì²´ Attention Matrix')
        axes[1, 0].set_xlabel('Key Position')
        axes[1, 0].set_ylabel('Query Position')
        plt.colorbar(im3, ax=axes[1, 0])
        
        # íŒ¨ì¹˜ë³„ attention ë¶„í¬
        patch_attention_avg = np.mean(attn[1:, 1:], axis=0)  # íŒ¨ì¹˜ë“¤ ê°„ì˜ í‰ê·  attention
        patch_attention_2d = patch_attention_avg.reshape(grid_size, grid_size)
        
        im4 = axes[1, 1].imshow(patch_attention_2d, cmap='viridis')
        axes[1, 1].set_title('íŒ¨ì¹˜ ê°„ í‰ê·  Attention')
        plt.colorbar(im4, ax=axes[1, 1])
        
        # Attention í†µê³„
        stats_text = f"""Attention í†µê³„:
        - í—¤ë“œ ê°œìˆ˜: {attention_weights.shape[1]}
        - ì‹œí€€ìŠ¤ ê¸¸ì´: {attn.shape[0]}
        - íŒ¨ì¹˜ ê·¸ë¦¬ë“œ: {grid_size}x{grid_size}
        - CLS attention í‰ê· : {cls_attention.mean():.4f}
        - CLS attention í‘œì¤€í¸ì°¨: {cls_attention.std():.4f}
        - ìµœëŒ€ attention: {cls_attention.max():.4f}
        - ìµœì†Œ attention: {cls_attention.min():.4f}"""
        
        axes[1, 2].text(0.1, 0.5, stats_text, fontsize=10,
                        verticalalignment='center', transform=axes[1, 2].transAxes)
        axes[1, 2].set_title('Attention ë¶„ì„')
        axes[1, 2].axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def analyze_positional_encoding(self):
        """ìœ„ì¹˜ ì„ë² ë”© ë¶„ì„"""
        pos_embed = self.model.pos_embed.data.squeeze().cpu().numpy()  # [num_patches+1, embed_dim]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ìœ„ì¹˜ ì„ë² ë”© íˆíŠ¸ë§µ
        im1 = axes[0, 0].imshow(pos_embed.T, cmap='RdBu', aspect='auto')
        axes[0, 0].set_title('ìœ„ì¹˜ ì„ë² ë”© íˆíŠ¸ë§µ')
        axes[0, 0].set_xlabel('Position')
        axes[0, 0].set_ylabel('Embedding Dimension')
        plt.colorbar(im1, ax=axes[0, 0])
        
        # CLS í† í° vs íŒ¨ì¹˜ ìœ„ì¹˜ ì„ë² ë”© ë¹„êµ
        cls_embed = pos_embed[0]  # CLS í† í° ì„ë² ë”©
        patch_embeds = pos_embed[1:]  # íŒ¨ì¹˜ ì„ë² ë”©ë“¤
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(patch_embeds, cls_embed) / (
            np.linalg.norm(patch_embeds, axis=1) * np.linalg.norm(cls_embed)
        )
        
        grid_size = int(np.sqrt(len(similarities)))
        similarity_map = similarities.reshape(grid_size, grid_size)
        
        im2 = axes[0, 1].imshow(similarity_map, cmap='coolwarm')
        axes[0, 1].set_title('CLS í† í°ê³¼ì˜ ìœ„ì¹˜ ì„ë² ë”© ìœ ì‚¬ë„')
        plt.colorbar(im2, ax=axes[0, 1])
        
        # ì¸ì ‘ íŒ¨ì¹˜ ê°„ ìœ ì‚¬ë„
        adjacent_similarities = []
        for i in range(len(patch_embeds) - 1):
            sim = np.dot(patch_embeds[i], patch_embeds[i+1]) / (
                np.linalg.norm(patch_embeds[i]) * np.linalg.norm(patch_embeds[i+1])
            )
            adjacent_similarities.append(sim)
        
        axes[1, 0].plot(adjacent_similarities)
        axes[1, 0].set_title('ì¸ì ‘ íŒ¨ì¹˜ ê°„ ìœ„ì¹˜ ì„ë² ë”© ìœ ì‚¬ë„')
        axes[1, 0].set_xlabel('íŒ¨ì¹˜ ì¸ë±ìŠ¤')
        axes[1, 0].set_ylabel('ìœ ì‚¬ë„')
        axes[1, 0].grid(True)
        
        # ìœ„ì¹˜ ì„ë² ë”© í†µê³„
        stats_text = f"""ìœ„ì¹˜ ì„ë² ë”© í†µê³„:
        - ì´ ìœ„ì¹˜ ê°œìˆ˜: {pos_embed.shape[0]}
        - ì„ë² ë”© ì°¨ì›: {pos_embed.shape[1]}
        - CLS ì„ë² ë”© norm: {np.linalg.norm(cls_embed):.4f}
        - íŒ¨ì¹˜ ì„ë² ë”© í‰ê·  norm: {np.mean([np.linalg.norm(p) for p in patch_embeds]):.4f}
        - ìµœëŒ€ ìœ ì‚¬ë„: {similarities.max():.4f}
        - ìµœì†Œ ìœ ì‚¬ë„: {similarities.min():.4f}
        - í‰ê·  ì¸ì ‘ ìœ ì‚¬ë„: {np.mean(adjacent_similarities):.4f}"""
        
        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12,
                        verticalalignment='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('ìœ„ì¹˜ ì„ë² ë”© ë¶„ì„')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()

def compare_cnn_vs_vit():
    """CNNê³¼ ViT ì„±ëŠ¥ ë¹„êµ"""
    print("ğŸ”¥ CNN vs ViT ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        # ResNet-50 (CNN)
        import torchvision.models as models
        resnet = models.resnet50(pretrained=True).to(DEVICE)
        resnet.eval()
        
        # ViT-Base (Transformer)
        vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
        vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(DEVICE)
        vit_model.eval()
        
        # ì»¤ìŠ¤í…€ ViT
        custom_vit = VisionTransformer(
            img_size=224, patch_size=16, num_classes=1000,
            embed_dim=768, depth=12, num_heads=12
        ).to(DEVICE)
        custom_vit.eval()
        
        print("âœ… ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
    test_images = torch.randn(10, 3, 224, 224).to(DEVICE)
    
    # ì„±ëŠ¥ ì¸¡ì • í•¨ìˆ˜
    def measure_performance(model, inputs, model_name, num_runs=50):
        model.eval()
        times = []
        
        # Warm-up
        with torch.no_grad():
            for _ in range(5):
                _ = model(inputs[:1])
        
        # ì‹¤ì œ ì¸¡ì •
        with torch.no_grad():
            for _ in range(num_runs):
                start_time = time.time()
                _ = model(inputs)
                end_time = time.time()
                times.append((end_time - start_time) * 1000)  # ms
        
        return {
            'model': model_name,
            'avg_time': np.mean(times),
            'std_time': np.std(times),
            'min_time': np.min(times),
            'max_time': np.max(times)
        }
    
    # ì„±ëŠ¥ ì¸¡ì •
    results = []
    
    print("\nğŸ“Š ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
    
    # ResNet-50
    resnet_result = measure_performance(resnet, test_images, "ResNet-50")
    results.append(resnet_result)
    print(f"ResNet-50: {resnet_result['avg_time']:.2f}ms Â± {resnet_result['std_time']:.2f}ms")
    
    # ViT (HuggingFace)
    vit_result = measure_performance(vit_model, test_images, "ViT-Base (HF)")
    results.append(vit_result)
    print(f"ViT-Base (HF): {vit_result['avg_time']:.2f}ms Â± {vit_result['std_time']:.2f}ms")
    
    # Custom ViT
    custom_vit_result = measure_performance(custom_vit, test_images, "Custom ViT")
    results.append(custom_vit_result)
    print(f"Custom ViT: {custom_vit_result['avg_time']:.2f}ms Â± {custom_vit_result['std_time']:.2f}ms")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (CUDA ì‚¬ìš© ì‹œ)
    if torch.cuda.is_available():
        print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¤‘...")
        
        def measure_memory(model, inputs, model_name):
            torch.cuda.reset_peak_memory_stats()
            with torch.no_grad():
                _ = model(inputs[:1])  # ë‹¨ì¼ ì´ë¯¸ì§€ë¡œ ì¸¡ì •
            memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)
            return memory_mb
        
        for result in results:
            if result['model'] == "ResNet-50":
                result['memory'] = measure_memory(resnet, test_images, "ResNet-50")
            elif result['model'] == "ViT-Base (HF)":
                result['memory'] = measure_memory(vit_model, test_images, "ViT-Base (HF)")
            elif result['model'] == "Custom ViT":
                result['memory'] = measure_memory(custom_vit, test_images, "Custom ViT")
            
            print(f"{result['model']}: {result['memory']:.2f}MB")
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(15, 5))
    
    # ì¶”ë¡  ì‹œê°„ ë¹„êµ
    plt.subplot(1, 3, 1)
    models = [r['model'] for r in results]
    times = [r['avg_time'] for r in results]
    errors = [r['std_time'] for r in results]
    
    bars = plt.bar(models, times, yerr=errors, capsize=5, alpha=0.7, 
                   color=['skyblue', 'lightcoral', 'lightgreen'])
    plt.title('ì¶”ë¡  ì‹œê°„ ë¹„êµ')
    plt.ylabel('ì‹œê°„ (ms)')
    plt.xticks(rotation=45)
    
    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bar, time_val in zip(bars, times):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                f'{time_val:.1f}ms', ha='center', va='bottom')
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ (CUDA ì‚¬ìš© ì‹œ)
    if torch.cuda.is_available() and 'memory' in results[0]:
        plt.subplot(1, 3, 2)
        memories = [r['memory'] for r in results]
        bars = plt.bar(models, memories, alpha=0.7, 
                      color=['skyblue', 'lightcoral', 'lightgreen'])
        plt.title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ')
        plt.ylabel('ë©”ëª¨ë¦¬ (MB)')
        plt.xticks(rotation=45)
        
        for bar, mem_val in zip(bars, memories):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                    f'{mem_val:.1f}MB', ha='center', va='bottom')
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
    plt.subplot(1, 3, 3)
    param_counts = []
    
    for result in results:
        if result['model'] == "ResNet-50":
            params = sum(p.numel() for p in resnet.parameters()) / 1e6
        elif result['model'] == "ViT-Base (HF)":
            params = sum(p.numel() for p in vit_model.parameters()) / 1e6
        elif result['model'] == "Custom ViT":
            params = sum(p.numel() for p in custom_vit.parameters()) / 1e6
        param_counts.append(params)
    
    bars = plt.bar(models, param_counts, alpha=0.7,
                  color=['skyblue', 'lightcoral', 'lightgreen'])
    plt.title('ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜')
    plt.ylabel('íŒŒë¼ë¯¸í„° (M)')
    plt.xticks(rotation=45)
    
    for bar, param_val in zip(bars, param_counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{param_val:.1f}M', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“‹ ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
    print("=" * 50)
    for result in results:
        print(f"{result['model']:15s}: {result['avg_time']:6.2f}ms", end="")
        if 'memory' in result:
            print(f", {result['memory']:6.2f}MB", end="")
        print()
    
    return results

def main():
    """ë©”ì¸ ì‹¤ìŠµ í•¨ìˆ˜"""
    print("ğŸ¤– Week 4: Vision Transformer ì‹¤ìŠµ")
    print("=" * 50)
    
    # 1. ì»¤ìŠ¤í…€ ViT ëª¨ë¸ ìƒì„±
    print("\n1ï¸âƒ£ Vision Transformer ëª¨ë¸ ìƒì„±")
    vit_model = VisionTransformer(
        img_size=224,
        patch_size=16,
        num_classes=1000,
        embed_dim=768,
        depth=12,
        num_heads=12
    ).to(DEVICE)
    
    print(f"âœ… ViT ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in vit_model.parameters()) / 1e6:.1f}M")
    print(f"   - íŒ¨ì¹˜ ê°œìˆ˜: {vit_model.num_patches}")
    print(f"   - ì„ë² ë”© ì°¨ì›: {vit_model.embed_dim}")
    
    # 2. ë¶„ì„ ë„êµ¬ ìƒì„±
    analyzer = ViTAnalyzer(vit_model)
    
    # 3. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ (ë˜ëŠ” ìƒì„±)
    print("\n2ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„")
    try:
        # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
        test_image = Image.new('RGB', (224, 224), color='white')
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ê·¸ë¦¬ê¸°
        from PIL import ImageDraw
        draw = ImageDraw.Draw(test_image)
        
        # ì²´í¬ë³´ë“œ íŒ¨í„´
        for i in range(0, 224, 32):
            for j in range(0, 224, 32):
                if (i//32 + j//32) % 2 == 0:
                    draw.rectangle([i, j, i+32, j+32], fill='black')
        
        # ì¤‘ì•™ì— ì› ê·¸ë¦¬ê¸°
        draw.ellipse([80, 80, 144, 144], fill='red')
        
        print("âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 4. íŒ¨ì¹˜ ë¶„í•  ì‹œê°í™”
    print("\n3ï¸âƒ£ íŒ¨ì¹˜ ë¶„í•  ì‹œê°í™”")
    analyzer.visualize_patches(test_image)
    
    # 5. Attention ì‹œê°í™”
    print("\n4ï¸âƒ£ Self-Attention ì‹œê°í™”")
    analyzer.visualize_attention(test_image, head_idx=0)
    
    # 6. ìœ„ì¹˜ ì„ë² ë”© ë¶„ì„
    print("\n5ï¸âƒ£ ìœ„ì¹˜ ì„ë² ë”© ë¶„ì„")
    analyzer.analyze_positional_encoding()
    
    # 7. CNN vs ViT ì„±ëŠ¥ ë¹„êµ
    print("\n6ï¸âƒ£ CNN vs ViT ì„±ëŠ¥ ë¹„êµ")
    comparison_results = compare_cnn_vs_vit()
    
    print("\nğŸ‰ ëª¨ë“  ì‹¤ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("\nğŸ“š ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
    print("   - ë‹¤ì–‘í•œ íŒ¨ì¹˜ í¬ê¸° ì‹¤í—˜ (8x8, 32x32)")
    print("   - ë‹¤ë¥¸ í—¤ë“œì˜ Attention íŒ¨í„´ ë¶„ì„")
    print("   - ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸")
    print("   - Fine-tuning ì‹¤í—˜")

if __name__ == "__main__":
    main()

