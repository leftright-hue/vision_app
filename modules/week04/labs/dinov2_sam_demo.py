#!/usr/bin/env python3
"""
Week 4 Lab: DINOv2ì™€ SAM ë°ëª¨
ìê¸°ì§€ë„í•™ìŠµ ëª¨ë¸ DINOv2ì™€ Segment Anything Model (SAM) í™œìš© ì‹¤ìŠµ

ì´ ì‹¤ìŠµì—ì„œëŠ”:
1. DINOv2ë¡œ ê³ í’ˆì§ˆ íŠ¹ì§• ì¶”ì¶œ
2. SAMìœ¼ë¡œ ê°ì²´ ì„¸ê·¸ë©˜í…Œì´ì…˜
3. ìê¸°ì§€ë„í•™ìŠµ vs ì§€ë„í•™ìŠµ ë¹„êµ
4. ë©€í‹°ëª¨ë‹¬ íŠ¹ì§• ë¶„ì„
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw
import requests
from io import BytesIO
import cv2
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# ì„¤ì •
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {DEVICE}")

class DINOv2Analyzer:
    """
    DINOv2 ëª¨ë¸ì„ ì‚¬ìš©í•œ íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ì„
    """
    def __init__(self, model_name='dinov2_vitb14'):
        """
        DINOv2 ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  DINOv2 ëª¨ë¸ ('dinov2_vits14', 'dinov2_vitb14', 'dinov2_vitl14', 'dinov2_vitg14')
        """
        self.model_name = model_name
        self.model = None
        self.load_model()
        
    def load_model(self):
        """DINOv2 ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ”„ {self.model_name} ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # PyTorch Hubì—ì„œ DINOv2 ëª¨ë¸ ë¡œë“œ
            self.model = torch.hub.load('facebookresearch/dinov2', self.model_name)
            self.model.eval().to(DEVICE)
            
            print(f"âœ… {self.model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ëª…ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
    
    def preprocess_image(self, image):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        if isinstance(image, str):
            # URL ë˜ëŠ” íŒŒì¼ ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
            if image.startswith('http'):
                response = requests.get(image)
                image = Image.open(BytesIO(response.content)).convert('RGB')
            else:
                image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        
        # DINOv2 ì „ì²˜ë¦¬
        from torchvision import transforms
        
        transform = transforms.Compose([
            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).unsqueeze(0).to(DEVICE)
    
    def extract_features(self, image, return_attention=False):
        """íŠ¹ì§• ì¶”ì¶œ"""
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        input_tensor = self.preprocess_image(image)
        
        with torch.no_grad():
            if return_attention:
                # Attention ê°€ì¤‘ì¹˜ë„ í•¨ê»˜ ì¶”ì¶œ
                features = self.model.forward_features(input_tensor)
                # ë§ˆì§€ë§‰ ë¸”ë¡ì˜ attention ê°€ì ¸ì˜¤ê¸° (êµ¬í˜„ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                attention = None  # DINOv2ì—ì„œëŠ” ì§ì ‘ attentionì„ ê°€ì ¸ì˜¤ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ
                return features, attention
            else:
                features = self.model(input_tensor)  # CLS í† í° íŠ¹ì§•
                return features
    
    def visualize_features(self, images, labels=None):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ì‹œê°í™”"""
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ” íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        features_list = []
        
        for i, image in enumerate(images):
            features = self.extract_features(image)
            if features is not None:
                features_list.append(features.cpu().numpy().flatten())
                print(f"   ì´ë¯¸ì§€ {i+1}/{len(images)} ì²˜ë¦¬ ì™„ë£Œ")
        
        if not features_list:
            print("âŒ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨")
            return
        
        features_array = np.array(features_list)
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ
        print("ğŸ“Š PCA ì°¨ì› ì¶•ì†Œ ì¤‘...")
        pca = PCA(n_components=50)
        features_pca = pca.fit_transform(features_array)
        
        # t-SNEë¡œ 2D ì‹œê°í™”
        print("ğŸ¨ t-SNE ì‹œê°í™” ì¤‘...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(images)-1))
        features_2d = tsne.fit_transform(features_pca)
        
        # ì‹œê°í™”
        plt.figure(figsize=(15, 5))
        
        # PCA ì„¤ëª… ë¶„ì‚°
        plt.subplot(1, 3, 1)
        plt.plot(np.cumsum(pca.explained_variance_ratio_))
        plt.xlabel('ì£¼ì„±ë¶„ ê°œìˆ˜')
        plt.ylabel('ëˆ„ì  ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨')
        plt.title('PCA ì„¤ëª… ë¶„ì‚°')
        plt.grid(True)
        
        # t-SNE ì‹œê°í™”
        plt.subplot(1, 3, 2)
        if labels is not None:
            unique_labels = list(set(labels))
            colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
            
            for label, color in zip(unique_labels, colors):
                mask = np.array(labels) == label
                plt.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                           c=[color], label=label, alpha=0.7)
            plt.legend()
        else:
            plt.scatter(features_2d[:, 0], features_2d[:, 1], alpha=0.7)
        
        plt.xlabel('t-SNE 1')
        plt.ylabel('t-SNE 2')
        plt.title('DINOv2 íŠ¹ì§• t-SNE ì‹œê°í™”')
        
        # íŠ¹ì§• í†µê³„
        plt.subplot(1, 3, 3)
        feature_stats = {
            'í‰ê· ': np.mean(features_array),
            'í‘œì¤€í¸ì°¨': np.std(features_array),
            'ìµœëŒ€ê°’': np.max(features_array),
            'ìµœì†Œê°’': np.min(features_array),
            'íŠ¹ì§• ì°¨ì›': features_array.shape[1]
        }
        
        stats_text = '\n'.join([f'{k}: {v:.4f}' if isinstance(v, float) else f'{k}: {v}' 
                               for k, v in feature_stats.items()])
        plt.text(0.1, 0.5, stats_text, fontsize=12, 
                verticalalignment='center', transform=plt.gca().transAxes)
        plt.title('íŠ¹ì§• í†µê³„')
        plt.axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return features_array, features_2d
    
    def compare_with_supervised(self, images):
        """ìê¸°ì§€ë„í•™ìŠµ vs ì§€ë„í•™ìŠµ íŠ¹ì§• ë¹„êµ"""
        if self.model is None:
            print("âŒ DINOv2 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ì§€ë„í•™ìŠµ ëª¨ë¸ (ResNet) ë¡œë“œ
            import torchvision.models as models
            resnet = models.resnet50(pretrained=True)
            resnet.fc = torch.nn.Identity()  # ë¶„ë¥˜ í—¤ë“œ ì œê±°
            resnet.eval().to(DEVICE)
            
            print("ğŸ”„ íŠ¹ì§• ì¶”ì¶œ ë¹„êµ ì¤‘...")
            
            dinov2_features = []
            resnet_features = []
            
            for i, image in enumerate(images):
                # DINOv2 íŠ¹ì§•
                dino_feat = self.extract_features(image)
                if dino_feat is not None:
                    dinov2_features.append(dino_feat.cpu().numpy().flatten())
                
                # ResNet íŠ¹ì§•
                input_tensor = self.preprocess_image(image)
                with torch.no_grad():
                    resnet_feat = resnet(input_tensor)
                    resnet_features.append(resnet_feat.cpu().numpy().flatten())
                
                print(f"   ì´ë¯¸ì§€ {i+1}/{len(images)} ì²˜ë¦¬ ì™„ë£Œ")
            
            # íŠ¹ì§• ë¹„êµ ì‹œê°í™”
            dinov2_array = np.array(dinov2_features)
            resnet_array = np.array(resnet_features)
            
            # PCA ì ìš©
            pca_dino = PCA(n_components=2)
            pca_resnet = PCA(n_components=2)
            
            dino_2d = pca_dino.fit_transform(dinov2_array)
            resnet_2d = pca_resnet.fit_transform(resnet_array)
            
            # ì‹œê°í™”
            plt.figure(figsize=(15, 5))
            
            plt.subplot(1, 3, 1)
            plt.scatter(dino_2d[:, 0], dino_2d[:, 1], alpha=0.7, label='DINOv2')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.title('DINOv2 íŠ¹ì§• (PCA)')
            plt.legend()
            
            plt.subplot(1, 3, 2)
            plt.scatter(resnet_2d[:, 0], resnet_2d[:, 1], alpha=0.7, color='orange', label='ResNet')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.title('ResNet íŠ¹ì§• (PCA)')
            plt.legend()
            
            # íŠ¹ì§• í†µê³„ ë¹„êµ
            plt.subplot(1, 3, 3)
            comparison_stats = f"""íŠ¹ì§• ë¹„êµ:
            
DINOv2:
- ì°¨ì›: {dinov2_array.shape[1]}
- í‰ê· : {np.mean(dinov2_array):.4f}
- í‘œì¤€í¸ì°¨: {np.std(dinov2_array):.4f}

ResNet:
- ì°¨ì›: {resnet_array.shape[1]}
- í‰ê· : {np.mean(resnet_array):.4f}
- í‘œì¤€í¸ì°¨: {np.std(resnet_array):.4f}

ì„¤ëª… ë¶„ì‚° (PC1+PC2):
- DINOv2: {sum(pca_dino.explained_variance_ratio_):.3f}
- ResNet: {sum(pca_resnet.explained_variance_ratio_):.3f}"""
            
            plt.text(0.1, 0.5, comparison_stats, fontsize=10,
                    verticalalignment='center', transform=plt.gca().transAxes)
            plt.title('íŠ¹ì§• ë¹„êµ')
            plt.axis('off')
            
            plt.tight_layout()
            plt.show()
            
            return dinov2_array, resnet_array
            
        except Exception as e:
            print(f"âŒ ë¹„êµ ì‹¤í—˜ ì‹¤íŒ¨: {e}")

class SAMDemo:
    """
    Segment Anything Model (SAM) ë°ëª¨
    """
    def __init__(self):
        """SAM ëª¨ë¸ ì´ˆê¸°í™”"""
        self.model = None
        self.predictor = None
        self.load_model()
    
    def load_model(self):
        """SAM ëª¨ë¸ ë¡œë“œ"""
        try:
            print("ğŸ”„ SAM ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # SAM ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” segment-anything íŒ¨í‚¤ì§€ í•„ìš”)
            # pip install git+https://github.com/facebookresearch/segment-anything.git
            
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´
            print("ğŸ’¡ SAM ëª¨ë¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")
            print("   ì‹¤ì œ ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” segment-anything íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”")
            
        except Exception as e:
            print(f"âŒ SAM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def segment_image(self, image, points=None, boxes=None):
        """ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì‹œë®¬ë ˆì´ì…˜)"""
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif isinstance(image, torch.Tensor):
            image = transforms.ToPILImage()(image.squeeze())
        
        # ì‹¤ì œ SAM êµ¬í˜„ ëŒ€ì‹  ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
        img_array = np.array(image)
        h, w = img_array.shape[:2]
        
        # ê°€ì§œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros((h, w), dtype=bool)
        
        if points is not None:
            # í¬ì¸íŠ¸ ì£¼ë³€ì— ì›í˜• ë§ˆìŠ¤í¬ ìƒì„±
            for point in points:
                x, y = point
                y_coords, x_coords = np.ogrid[:h, :w]
                mask_circle = (x_coords - x)**2 + (y_coords - y)**2 <= 50**2
                mask = mask | mask_circle
        else:
            # ì¤‘ì•™ì— ì„ì˜ì˜ ë§ˆìŠ¤í¬ ìƒì„±
            center_x, center_y = w//2, h//2
            y_coords, x_coords = np.ogrid[:h, :w]
            mask = (x_coords - center_x)**2 + (y_coords - center_y)**2 <= (min(w, h)//4)**2
        
        return mask
    
    def interactive_segmentation_demo(self, image):
        """ëŒ€í™”í˜• ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ëª¨"""
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        img_array = np.array(image)
        
        # ì‹œë®¬ë ˆì´ì…˜: ì—¬ëŸ¬ í¬ì¸íŠ¸ì—ì„œ ì„¸ê·¸ë©˜í…Œì´ì…˜
        points = [(100, 100), (200, 150), (150, 200)]
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 12))
        
        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0, 0].imshow(img_array)
        axes[0, 0].set_title('ì›ë³¸ ì´ë¯¸ì§€')
        axes[0, 0].axis('off')
        
        # í¬ì¸íŠ¸ í‘œì‹œ
        axes[0, 1].imshow(img_array)
        for i, (x, y) in enumerate(points):
            axes[0, 1].plot(x, y, 'ro', markersize=10)
            axes[0, 1].text(x+10, y-10, f'P{i+1}', color='red', fontsize=12, fontweight='bold')
        axes[0, 1].set_title('í´ë¦­ í¬ì¸íŠ¸')
        axes[0, 1].axis('off')
        
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼
        mask = self.segment_image(image, points)
        axes[1, 0].imshow(mask, cmap='gray')
        axes[1, 0].set_title('ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬')
        axes[1, 0].axis('off')
        
        # ì˜¤ë²„ë ˆì´
        overlay = img_array.copy()
        overlay[mask] = overlay[mask] * 0.5 + np.array([255, 0, 0]) * 0.5
        axes[1, 1].imshow(overlay.astype(np.uint8))
        axes[1, 1].set_title('ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return mask

def create_sample_images():
    """ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±"""
    images = []
    labels = []
    
    # ë‹¤ì–‘í•œ íŒ¨í„´ì˜ ì´ë¯¸ì§€ ìƒì„±
    patterns = [
        ('ì²´í¬ë³´ë“œ', 'checkerboard'),
        ('ì›í˜•', 'circle'),
        ('ì¤„ë¬´ëŠ¬', 'stripes'),
        ('ê·¸ë¼ë””ì–¸íŠ¸', 'gradient'),
        ('ë…¸ì´ì¦ˆ', 'noise')
    ]
    
    for name, pattern in patterns:
        img = Image.new('RGB', (224, 224), color='white')
        draw = ImageDraw.Draw(img)
        
        if pattern == 'checkerboard':
            for i in range(0, 224, 32):
                for j in range(0, 224, 32):
                    if (i//32 + j//32) % 2 == 0:
                        draw.rectangle([i, j, i+32, j+32], fill='black')
        
        elif pattern == 'circle':
            draw.ellipse([50, 50, 174, 174], fill='blue')
            draw.ellipse([80, 80, 144, 144], fill='red')
        
        elif pattern == 'stripes':
            for i in range(0, 224, 20):
                if (i//20) % 2 == 0:
                    draw.rectangle([0, i, 224, i+20], fill='green')
        
        elif pattern == 'gradient':
            img_array = np.zeros((224, 224, 3), dtype=np.uint8)
            for i in range(224):
                img_array[:, i] = [i, 255-i, 128]
            img = Image.fromarray(img_array)
        
        elif pattern == 'noise':
            img_array = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)
            img = Image.fromarray(img_array)
        
        images.append(img)
        labels.append(name)
    
    return images, labels

def multimodal_benchmark():
    """ë©€í‹°ëª¨ë‹¬ API ë²¤ì¹˜ë§ˆí¬ ì‹œë®¬ë ˆì´ì…˜"""
    print("ğŸš€ ë©€í‹°ëª¨ë‹¬ API ì„±ëŠ¥ ë¹„êµ")
    print("=" * 50)
    
    # API ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
    api_performance = {
        'Gemini Vision': {
            'response_time': np.random.normal(1200, 200, 10),  # ms
            'accuracy': 0.95,
            'cost_per_1k': 0.0,  # ë¬´ë£Œ
            'rate_limit': '60 RPM',
            'features': ['ì´ë¯¸ì§€ ë¶„ì„', 'í…ìŠ¤íŠ¸ ì¶”ì¶œ', 'OCR', 'ê°ì²´ ì¸ì‹']
        },
        'GPT-4V': {
            'response_time': np.random.normal(2100, 300, 10),
            'accuracy': 0.92,
            'cost_per_1k': 0.01,
            'rate_limit': '100 RPM',
            'features': ['ì´ë¯¸ì§€ ë¶„ì„', 'ìƒì„¸ ì„¤ëª…', 'ì¶”ë¡ ', 'ì°½ì‘']
        },
        'Llama Vision': {
            'response_time': np.random.normal(1800, 250, 10),
            'accuracy': 0.88,
            'cost_per_1k': 0.0,  # 3ê°œì›” ë¬´ë£Œ
            'rate_limit': '50 RPM',
            'features': ['ì˜¤í”ˆì†ŒìŠ¤', 'ì»¤ìŠ¤í„°ë§ˆì´ì§•', 'ë¡œì»¬ ì‹¤í–‰']
        },
        'Claude Vision': {
            'response_time': np.random.normal(1600, 180, 10),
            'accuracy': 0.90,
            'cost_per_1k': 0.008,
            'rate_limit': '50 RPM',
            'features': ['ì•ˆì „ì„±', 'ìœ¤ë¦¬ì  AI', 'ê¸´ ì»¨í…ìŠ¤íŠ¸']
        }
    }
    
    # ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # ì‘ë‹µ ì‹œê°„ ë¹„êµ
    apis = list(api_performance.keys())
    response_times = [np.mean(data['response_time']) for data in api_performance.values()]
    response_stds = [np.std(data['response_time']) for data in api_performance.values()]
    
    bars1 = axes[0, 0].bar(apis, response_times, yerr=response_stds, capsize=5, alpha=0.7)
    axes[0, 0].set_title('í‰ê·  ì‘ë‹µ ì‹œê°„')
    axes[0, 0].set_ylabel('ì‹œê°„ (ms)')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    for bar, time_val in zip(bars1, response_times):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,
                       f'{time_val:.0f}ms', ha='center', va='bottom')
    
    # ì •í™•ë„ ë¹„êµ
    accuracies = [data['accuracy'] for data in api_performance.values()]
    bars2 = axes[0, 1].bar(apis, accuracies, alpha=0.7, color='lightcoral')
    axes[0, 1].set_title('ì •í™•ë„ ë¹„êµ')
    axes[0, 1].set_ylabel('ì •í™•ë„')
    axes[0, 1].set_ylim(0.8, 1.0)
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    for bar, acc_val in zip(bars2, accuracies):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                       f'{acc_val:.2f}', ha='center', va='bottom')
    
    # ë¹„ìš© ë¹„êµ
    costs = [data['cost_per_1k'] for data in api_performance.values()]
    bars3 = axes[1, 0].bar(apis, costs, alpha=0.7, color='lightgreen')
    axes[1, 0].set_title('1K ìš”ì²­ë‹¹ ë¹„ìš©')
    axes[1, 0].set_ylabel('ë¹„ìš© ($)')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    for bar, cost_val in zip(bars3, costs):
        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0005,
                       f'${cost_val:.3f}' if cost_val > 0 else 'Free', ha='center', va='bottom')
    
    # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
    # ì ìˆ˜ = (1/ì‘ë‹µì‹œê°„) * 0.3 + ì •í™•ë„ * 0.4 + (1/ë¹„ìš©+0.001) * 0.3
    composite_scores = []
    for api, data in api_performance.items():
        time_score = 1 / (np.mean(data['response_time']) / 1000)  # ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰
        acc_score = data['accuracy']
        cost_score = 1 / (data['cost_per_1k'] + 0.001)  # ë¹„ìš© ì—­ìˆ˜
        
        composite = time_score * 0.3 + acc_score * 0.4 + (cost_score / 1000) * 0.3
        composite_scores.append(composite)
    
    bars4 = axes[1, 1].bar(apis, composite_scores, alpha=0.7, color='gold')
    axes[1, 1].set_title('ì¢…í•© ì„±ëŠ¥ ì ìˆ˜')
    axes[1, 1].set_ylabel('ì ìˆ˜')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    for bar, score_val in zip(bars4, composite_scores):
        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{score_val:.2f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # ìƒì„¸ ë¹„êµ í…Œì´ë¸”
    print("\nğŸ“Š API ìƒì„¸ ë¹„êµ")
    print("-" * 80)
    print(f"{'API':<15} {'ì‘ë‹µì‹œê°„(ms)':<12} {'ì •í™•ë„':<8} {'ë¹„ìš©/1K':<10} {'ì œí•œ':<10}")
    print("-" * 80)
    
    for api, data in api_performance.items():
        avg_time = np.mean(data['response_time'])
        cost_str = f"${data['cost_per_1k']:.3f}" if data['cost_per_1k'] > 0 else "Free"
        print(f"{api:<15} {avg_time:<12.0f} {data['accuracy']:<8.2f} {cost_str:<10} {data['rate_limit']:<10}")
    
    # ì„ íƒ ê°€ì´ë“œ
    print("\nğŸ¯ API ì„ íƒ ê°€ì´ë“œ")
    print("-" * 50)
    print("ğŸ’° ë¹„ìš© ìµœìš°ì„ : Gemini Vision ë˜ëŠ” Llama Vision")
    print("ğŸ¯ ì •í™•ë„ ìµœìš°ì„ : Gemini Vision")
    print("âš¡ ì†ë„ ìµœìš°ì„ : Gemini Vision")
    print("ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•: Llama Vision")
    print("ğŸ›¡ï¸ ì•ˆì „ì„±: Claude Vision")
    
    return api_performance

def main():
    """ë©”ì¸ ì‹¤ìŠµ í•¨ìˆ˜"""
    print("ğŸ¤– Week 4: DINOv2 & SAM ë°ëª¨")
    print("=" * 50)
    
    # 1. ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±
    print("\n1ï¸âƒ£ ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±")
    sample_images, labels = create_sample_images()
    print(f"âœ… {len(sample_images)}ê°œ ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
    
    # ìƒ˜í”Œ ì´ë¯¸ì§€ í‘œì‹œ
    fig, axes = plt.subplots(1, len(sample_images), figsize=(15, 3))
    for i, (img, label) in enumerate(zip(sample_images, labels)):
        axes[i].imshow(img)
        axes[i].set_title(label)
        axes[i].axis('off')
    plt.tight_layout()
    plt.show()
    
    # 2. DINOv2 ë¶„ì„
    print("\n2ï¸âƒ£ DINOv2 íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ì„")
    dinov2_analyzer = DINOv2Analyzer('dinov2_vitb14')
    
    if dinov2_analyzer.model is not None:
        # íŠ¹ì§• ì‹œê°í™”
        features_array, features_2d = dinov2_analyzer.visualize_features(sample_images, labels)
        
        # ìê¸°ì§€ë„í•™ìŠµ vs ì§€ë„í•™ìŠµ ë¹„êµ
        print("\n3ï¸âƒ£ ìê¸°ì§€ë„í•™ìŠµ vs ì§€ë„í•™ìŠµ ë¹„êµ")
        dinov2_analyzer.compare_with_supervised(sample_images[:3])  # ì²˜ìŒ 3ê°œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
    
    # 4. SAM ë°ëª¨
    print("\n4ï¸âƒ£ SAM ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ëª¨")
    sam_demo = SAMDemo()
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ëª¨
    mask = sam_demo.interactive_segmentation_demo(sample_images[1])
    
    # 5. ë©€í‹°ëª¨ë‹¬ API ë²¤ì¹˜ë§ˆí¬
    print("\n5ï¸âƒ£ ë©€í‹°ëª¨ë‹¬ API ì„±ëŠ¥ ë¹„êµ")
    api_performance = multimodal_benchmark()
    
    print("\nğŸ‰ ëª¨ë“  ë°ëª¨ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("\nğŸ“š ì¶”ê°€ ì‹¤í—˜ ì•„ì´ë””ì–´:")
    print("   - ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ DINOv2 íŠ¹ì§• ë¶„ì„")
    print("   - SAMì„ í™œìš©í•œ ìë™ ë¼ë²¨ë§ ì‹œìŠ¤í…œ êµ¬ì¶•")
    print("   - ë‹¤ì–‘í•œ DINOv2 ëª¨ë¸ í¬ê¸° ë¹„êµ (ViT-S, B, L, G)")
    print("   - ì‹¤ì œ APIë¥¼ ì‚¬ìš©í•œ ë©€í‹°ëª¨ë‹¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main()
