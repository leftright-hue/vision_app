# ğŸ¤– Week 4: Vision Transformerì™€ ìµœì‹  ëª¨ë¸ ë¹„êµ

## ğŸ“Œ í•™ìŠµ ëª©í‘œ

ì´ë²ˆ ì£¼ì°¨ì—ì„œëŠ” ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¾¼ Vision Transformer(ViT)ì™€ ìµœì‹  ìê¸°ì§€ë„í•™ìŠµ ëª¨ë¸ë“¤ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**í•µì‹¬ í•™ìŠµ ë‚´ìš©:**
- ğŸ§  Self-Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ì›ë¦¬ì™€ êµ¬í˜„
- ğŸ” Vision Transformer (ViT) ì•„í‚¤í…ì²˜ ì™„ì „ ë¶„ì„
- ğŸ¯ DINOì™€ ìê¸°ì§€ë„í•™ìŠµì˜ í˜ì‹ 
- ğŸš€ ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

---

## 1. Transformerì˜ ë“±ì¥ê³¼ ì»´í“¨í„° ë¹„ì „ í˜ëª…

### 1.1 Transformerì˜ ì—­ì‚¬

#### NLPì—ì„œ ì‹œì‘ëœ í˜ëª…
- **2017ë…„ "Attention Is All You Need"** ë…¼ë¬¸ìœ¼ë¡œ ì‹œì‘
- RNN/LSTMì˜ ìˆœì°¨ ì²˜ë¦¬ í•œê³„ ê·¹ë³µ
- ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ Self-Attention ë©”ì»¤ë‹ˆì¦˜ ë„ì…
- BERT, GPT ë“± ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ë°˜

#### ì»´í“¨í„° ë¹„ì „ìœ¼ë¡œì˜ í™•ì¥
- **2020ë…„ "An Image is Worth 16x16 Words"** (ViT ë…¼ë¬¸)
- CNNì˜ ê·€ë‚©ì  í¸í–¥(inductive bias) ì—†ì´ë„ ìš°ìˆ˜í•œ ì„±ëŠ¥
- ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ CNNì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ ë‹¬ì„±
- ë©€í‹°ëª¨ë‹¬ AIì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### 1.2 CNN vs Transformer íŒ¨ëŸ¬ë‹¤ì„ ë¹„êµ

#### CNNì˜ íŠ¹ì§•
```python
# CNNì˜ ì§€ì—­ì  ì²˜ë¦¬
def cnn_processing(image):
    # ì‘ì€ í•„í„°ë¡œ ì§€ì—­ì  íŠ¹ì§• ì¶”ì¶œ
    conv1 = conv2d(image, kernel_3x3)
    # ê³„ì¸µì ìœ¼ë¡œ íŠ¹ì§• ì¡°í•©
    conv2 = conv2d(conv1, kernel_3x3)
    # ê³µê°„ ì •ë³´ ì ì§„ì  ì¶•ì†Œ
    pooled = max_pool(conv2)
    return pooled
```

**ì¥ì :**
- ì§€ì—­ì  íŠ¹ì§• ì¶”ì¶œì— ìµœì í™”
- ì ì€ ë§¤ê°œë³€ìˆ˜ë¡œ íš¨ìœ¨ì 
- ì´ë¯¸ì§€ì˜ ê³µê°„ì  êµ¬ì¡° í™œìš©

**í•œê³„:**
- ì¥ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°© ì–´ë ¤ì›€
- ê³ ì •ëœ ìˆ˜ìš© ì˜ì—­(receptive field)
- ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ì¸í•œ ë³‘ë ¬í™” ì œì•½

#### Transformerì˜ íŠ¹ì§•
```python
# Transformerì˜ ì „ì—­ì  ì²˜ë¦¬
def transformer_processing(image_patches):
    # ëª¨ë“  íŒ¨ì¹˜ ê°„ ê´€ê³„ ë™ì‹œ ê³„ì‚°
    attention_weights = self_attention(image_patches)
    # ì „ì—­ì  ë§¥ë½ ì •ë³´ í™œìš©
    attended_features = apply_attention(image_patches, attention_weights)
    return attended_features
```

**ì¥ì :**
- ì „ì—­ì  ë§¥ë½ ì •ë³´ í™œìš©
- ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
- ì¥ê±°ë¦¬ ì˜ì¡´ì„± ìì—°ìŠ¤ëŸ½ê²Œ í¬ì°©
- ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ì— ì ìš© ê°€ëŠ¥

**í•œê³„:**
- ëŒ€ìš©ëŸ‰ ë°ì´í„° í•„ìš”
- ë†’ì€ ê³„ì‚° ë³µì¡ë„
- ì´ë¯¸ì§€ êµ¬ì¡°ì— ëŒ€í•œ ì‚¬ì „ ì§€ì‹ ë¶€ì¡±

---

## 2. Self-Attention ë©”ì»¤ë‹ˆì¦˜ ì™„ì „ ë¶„ì„

### 2.1 Attentionì˜ ê¸°ë³¸ ê°œë…

#### Attentionì˜ ì§ê´€ì  ì´í•´
Self-Attentionì€ "ì–´ë–¤ ë¶€ë¶„ì— ì§‘ì¤‘í•  ê²ƒì¸ê°€?"ë¥¼ í•™ìŠµí•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

```
ì…ë ¥: "The cat sat on the mat"
Query: "cat"ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ê³  ì‹¶ìŒ
Key: ê° ë‹¨ì–´ë“¤ ["The", "cat", "sat", "on", "the", "mat"]
Value: ê° ë‹¨ì–´ì˜ ì˜ë¯¸ í‘œí˜„

ê²°ê³¼: "cat"ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ "sat", "mat" ë“±ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
```

#### ìˆ˜í•™ì  ì •ì˜
Self-Attentionì€ ë‹¤ìŒ ê³µì‹ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```

ì—¬ê¸°ì„œ:
- Q (Query): ì§ˆì˜ í–‰ë ¬
- K (Key): í‚¤ í–‰ë ¬  
- V (Value): ê°’ í–‰ë ¬
- d_k: í‚¤ ë²¡í„°ì˜ ì°¨ì›

### 2.2 Self-Attention ë‹¨ê³„ë³„ êµ¬í˜„

#### Step 1: ì…ë ¥ ì„ë² ë”© ìƒì„±
```python
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # Q, K, V ë³€í™˜ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        
        # ì¶œë ¥ ë³€í™˜
        self.output = nn.Linear(embed_dim, embed_dim)
        
    def forward(self, x):
        batch_size, seq_len, embed_dim = x.shape
        
        # Step 1: Q, K, V ê³„ì‚°
        Q = self.query(x)  # [batch, seq_len, embed_dim]
        K = self.key(x)    # [batch, seq_len, embed_dim]
        V = self.value(x)  # [batch, seq_len, embed_dim]
        
        # Step 2: Multi-headë¥¼ ìœ„í•œ reshape
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Step 3: Attention ì ìˆ˜ ê³„ì‚°
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # Step 4: Softmaxë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜
        attention_weights = torch.softmax(attention_scores, dim=-1)
        
        # Step 5: Valueì™€ ê°€ì¤‘í•©
        attended_values = torch.matmul(attention_weights, V)
        
        # Step 6: Multi-head ê²°ê³¼ í•©ì¹˜ê¸°
        attended_values = attended_values.transpose(1, 2).contiguous().view(
            batch_size, seq_len, embed_dim
        )
        
        # Step 7: ìµœì¢… ì¶œë ¥ ë³€í™˜
        output = self.output(attended_values)
        
        return output, attention_weights
```

#### Step 2: Attention ì‹œê°í™”
```python
def visualize_attention(attention_weights, tokens):
    """
    Attention ê°€ì¤‘ì¹˜ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # ì²« ë²ˆì§¸ í—¤ë“œì˜ attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ
    attn = attention_weights[0, 0].detach().numpy()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(attn, 
                xticklabels=tokens, 
                yticklabels=tokens,
                cmap='Blues',
                annot=True,
                fmt='.2f')
    plt.title('Self-Attention Weights')
    plt.xlabel('Key Tokens')
    plt.ylabel('Query Tokens')
    plt.show()
```

### 2.3 Multi-Head Attentionì˜ í•„ìš”ì„±

#### ë‹¨ì¼ í—¤ë“œì˜ í•œê³„
- í•˜ë‚˜ì˜ ê´€ì ì—ì„œë§Œ ê´€ê³„ íŒŒì•…
- ë‹¤ì–‘í•œ ìœ í˜•ì˜ ê´€ê³„ ë™ì‹œ í¬ì°© ì–´ë ¤ì›€
- í‘œí˜„ë ¥ ì œí•œ

#### Multi-Head Attentionì˜ ì¥ì 
```python
# 8ê°œ í—¤ë“œê°€ ì„œë¡œ ë‹¤ë¥¸ ê´€ê³„ë¥¼ í•™ìŠµ
Head 1: ê³µê°„ì  ì¸ì ‘ì„± (spatial proximity)
Head 2: ì˜ë¯¸ì  ìœ ì‚¬ì„± (semantic similarity)  
Head 3: ìƒ‰ìƒ ê´€ê³„ (color relationships)
Head 4: í…ìŠ¤ì²˜ íŒ¨í„´ (texture patterns)
Head 5: ê°ì²´ ë¶€ë¶„-ì „ì²´ ê´€ê³„ (part-whole)
Head 6: ì‹œê°„ì  ì—°ê´€ì„± (temporal associations)
Head 7: ê¸°í•˜í•™ì  ë³€í™˜ (geometric transformations)
Head 8: ë§¥ë½ì  ì •ë³´ (contextual information)
```

#### êµ¬í˜„ ì˜ˆì œ
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # ëª¨ë“  í—¤ë“œë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ê¸° ìœ„í•œ í° í–‰ë ¬
        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)
        self.output = nn.Linear(embed_dim, embed_dim)
        
    def forward(self, x):
        batch_size, seq_len, embed_dim = x.shape
        
        # Q, K, Vë¥¼ í•œ ë²ˆì— ê³„ì‚°
        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch, heads, seq_len, head_dim]
        
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Scaled Dot-Product Attention
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        
        # í—¤ë“œë“¤ì„ ë‹¤ì‹œ í•©ì¹˜ê¸°
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)
        
        return self.output(attn_output)
```

---

## 3. Vision Transformer (ViT) ì•„í‚¤í…ì²˜

### 3.1 ViTì˜ í•µì‹¬ ì•„ì´ë””ì–´

#### "An Image is Worth 16x16 Words"
ViTëŠ” ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ì²˜ëŸ¼ ì²˜ë¦¬í•˜ëŠ” í˜ì‹ ì  ì ‘ê·¼ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

```python
# ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •
def image_to_patches(image, patch_size=16):
    """
    ì´ë¯¸ì§€ë¥¼ ê³ ì • í¬ê¸° íŒ¨ì¹˜ë“¤ë¡œ ë¶„í• 
    
    Args:
        image: [B, C, H, W] í˜•íƒœì˜ ì´ë¯¸ì§€
        patch_size: íŒ¨ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 16x16)
    
    Returns:
        patches: [B, num_patches, patch_dim] í˜•íƒœì˜ íŒ¨ì¹˜ë“¤
    """
    B, C, H, W = image.shape
    
    # íŒ¨ì¹˜ ê°œìˆ˜ ê³„ì‚°
    num_patches_h = H // patch_size
    num_patches_w = W // patch_size
    num_patches = num_patches_h * num_patches_w
    
    # íŒ¨ì¹˜ ì°¨ì› ê³„ì‚°
    patch_dim = C * patch_size * patch_size
    
    # ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í• 
    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
    patches = patches.contiguous().view(B, C, num_patches_h, num_patches_w, patch_size, patch_size)
    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()
    patches = patches.view(B, num_patches, patch_dim)
    
    return patches

# ì˜ˆì‹œ: 224x224 ì´ë¯¸ì§€ë¥¼ 16x16 íŒ¨ì¹˜ë¡œ ë¶„í• 
image = torch.randn(1, 3, 224, 224)  # [ë°°ì¹˜, ì±„ë„, ë†’ì´, ë„ˆë¹„]
patches = image_to_patches(image, patch_size=16)
print(f"íŒ¨ì¹˜ í˜•íƒœ: {patches.shape}")  # [1, 196, 768]
# 196 = (224/16) Ã— (224/16) = 14 Ã— 14 íŒ¨ì¹˜
# 768 = 3 Ã— 16 Ã— 16 (ì±„ë„ Ã— íŒ¨ì¹˜ ë†’ì´ Ã— íŒ¨ì¹˜ ë„ˆë¹„)
```

### 3.2 ViT ì „ì²´ ì•„í‚¤í…ì²˜

#### ì™„ì „í•œ ViT êµ¬í˜„
```python
class VisionTransformer(nn.Module):
    def __init__(self, 
                 img_size=224,
                 patch_size=16, 
                 num_classes=1000,
                 embed_dim=768,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.0,
                 dropout=0.1):
        super().__init__()
        
        # ê¸°ë³¸ ì„¤ì •
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.embed_dim = embed_dim
        
        # 1. íŒ¨ì¹˜ ì„ë² ë”©
        self.patch_embed = PatchEmbedding(
            img_size=img_size,
            patch_size=patch_size,
            embed_dim=embed_dim
        )
        
        # 2. í´ë˜ìŠ¤ í† í° (CLS token)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # 3. ìœ„ì¹˜ ì„ë² ë”©
        self.pos_embed = nn.Parameter(
            torch.zeros(1, self.num_patches + 1, embed_dim)
        )
        
        # 4. ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
        # 5. Transformer ë¸”ë¡ë“¤
        self.blocks = nn.ModuleList([
            TransformerBlock(
                embed_dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                dropout=dropout
            ) for _ in range(depth)
        ])
        
        # 6. ë ˆì´ì–´ ì •ê·œí™”
        self.norm = nn.LayerNorm(embed_dim)
        
        # 7. ë¶„ë¥˜ í—¤ë“œ
        self.head = nn.Linear(embed_dim, num_classes)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.init_weights()
    
    def init_weights(self):
        # ìœ„ì¹˜ ì„ë² ë”© ì´ˆê¸°í™”
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        
        # ë¶„ë¥˜ í—¤ë“œ ì´ˆê¸°í™”
        nn.init.zeros_(self.head.weight)
        nn.init.zeros_(self.head.bias)
    
    def forward(self, x):
        B = x.shape[0]
        
        # 1. íŒ¨ì¹˜ ì„ë² ë”©
        x = self.patch_embed(x)  # [B, num_patches, embed_dim]
        
        # 2. í´ë˜ìŠ¤ í† í° ì¶”ê°€
        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, embed_dim]
        x = torch.cat([cls_tokens, x], dim=1)  # [B, num_patches+1, embed_dim]
        
        # 3. ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€
        x = x + self.pos_embed
        x = self.dropout(x)
        
        # 4. Transformer ë¸”ë¡ë“¤ í†µê³¼
        for block in self.blocks:
            x = block(x)
        
        # 5. ì •ê·œí™”
        x = self.norm(x)
        
        # 6. í´ë˜ìŠ¤ í† í°ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜
        cls_token_final = x[:, 0]  # [B, embed_dim]
        
        # 7. ë¶„ë¥˜ ê²°ê³¼
        logits = self.head(cls_token_final)  # [B, num_classes]
        
        return logits

class PatchEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        # ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ íŒ¨ì¹˜ ì„ë² ë”© êµ¬í˜„
        self.projection = nn.Conv2d(
            in_channels, embed_dim, 
            kernel_size=patch_size, 
            stride=patch_size
        )
        
    def forward(self, x):
        # x: [B, C, H, W]
        x = self.projection(x)  # [B, embed_dim, H//patch_size, W//patch_size]
        x = x.flatten(2)        # [B, embed_dim, num_patches]
        x = x.transpose(1, 2)   # [B, num_patches, embed_dim]
        return x

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        
        # Multi-Head Self-Attention
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadAttention(embed_dim, num_heads)
        
        # MLP (Feed Forward)
        self.norm2 = nn.LayerNorm(embed_dim)
        mlp_hidden_dim = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden_dim, embed_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        # Self-Attention with residual connection
        x = x + self.attn(self.norm1(x))
        
        # MLP with residual connection  
        x = x + self.mlp(self.norm2(x))
        
        return x
```

### 3.3 ViTì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ë¶„ì„

#### 1. íŒ¨ì¹˜ ì„ë² ë”© (Patch Embedding)
```python
# íŒ¨ì¹˜ ì„ë² ë”©ì˜ ìƒì„¸ êµ¬í˜„
class DetailedPatchEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        
        # íŒ¨ì¹˜ ì •ë³´ ê³„ì‚°
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.patch_dim = in_channels * patch_size * patch_size
        
        # ì„ í˜• ë³€í™˜ ë°©ì‹
        self.linear_projection = nn.Linear(self.patch_dim, embed_dim)
        
        # ì»¨ë³¼ë£¨ì…˜ ë°©ì‹ (ë” íš¨ìœ¨ì )
        self.conv_projection = nn.Conv2d(
            in_channels, embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )
    
    def forward_linear(self, x):
        """ì„ í˜• ë³€í™˜ì„ ì‚¬ìš©í•œ íŒ¨ì¹˜ ì„ë² ë”©"""
        B, C, H, W = x.shape
        
        # íŒ¨ì¹˜ë¡œ ë¶„í• 
        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        patches = patches.contiguous().view(B, C, -1, self.patch_size, self.patch_size)
        patches = patches.permute(0, 2, 1, 3, 4).contiguous()
        patches = patches.view(B, self.num_patches, self.patch_dim)
        
        # ì„ í˜• ë³€í™˜
        embeddings = self.linear_projection(patches)
        
        return embeddings
    
    def forward_conv(self, x):
        """ì»¨ë³¼ë£¨ì…˜ì„ ì‚¬ìš©í•œ íŒ¨ì¹˜ ì„ë² ë”© (ë” íš¨ìœ¨ì )"""
        # ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ í•œ ë²ˆì— ì²˜ë¦¬
        x = self.conv_projection(x)  # [B, embed_dim, H//patch_size, W//patch_size]
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]
        
        return x
```

#### 2. ìœ„ì¹˜ ì„ë² ë”© (Positional Embedding)
```python
class PositionalEmbedding(nn.Module):
    def __init__(self, num_patches, embed_dim, dropout=0.1):
        super().__init__()
        
        # í•™ìŠµ ê°€ëŠ¥í•œ ìœ„ì¹˜ ì„ë² ë”©
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.dropout = nn.Dropout(dropout)
        
        # ì´ˆê¸°í™”
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
    
    def forward(self, x):
        # x: [B, num_patches+1, embed_dim] (CLS í† í° í¬í•¨)
        x = x + self.pos_embed
        return self.dropout(x)

# 2D ìœ„ì¹˜ ì„ë² ë”© (ë” ì •êµí•œ ë°©ì‹)
class Position2DEmbedding(nn.Module):
    def __init__(self, img_size, patch_size, embed_dim):
        super().__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = img_size // patch_size
        
        # í–‰ê³¼ ì—´ì— ëŒ€í•œ ë³„ë„ ì„ë² ë”©
        self.row_embed = nn.Parameter(torch.zeros(self.grid_size, embed_dim // 2))
        self.col_embed = nn.Parameter(torch.zeros(self.grid_size, embed_dim // 2))
        
        self.init_weights()
    
    def init_weights(self):
        nn.init.trunc_normal_(self.row_embed, std=0.02)
        nn.init.trunc_normal_(self.col_embed, std=0.02)
    
    def forward(self, x):
        B, num_patches, embed_dim = x.shape
        
        # 2D ê·¸ë¦¬ë“œ ìœ„ì¹˜ ìƒì„±
        pos_embed = torch.zeros(1, num_patches, embed_dim, device=x.device)
        
        for i in range(self.grid_size):
            for j in range(self.grid_size):
                patch_idx = i * self.grid_size + j
                pos_embed[0, patch_idx, :embed_dim//2] = self.row_embed[i]
                pos_embed[0, patch_idx, embed_dim//2:] = self.col_embed[j]
        
        return x + pos_embed
```

#### 3. í´ë˜ìŠ¤ í† í° (CLS Token)
```python
class CLSToken(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        
        # í•™ìŠµ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ í† í°
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # ì´ˆê¸°í™”
        nn.init.trunc_normal_(self.cls_token, std=0.02)
    
    def forward(self, x):
        B = x.shape[0]
        
        # ë°°ì¹˜ í¬ê¸°ë§Œí¼ í´ë˜ìŠ¤ í† í° ë³µì œ
        cls_tokens = self.cls_token.expand(B, -1, -1)
        
        # íŒ¨ì¹˜ ì„ë² ë”© ì•ì— ì¶”ê°€
        x = torch.cat([cls_tokens, x], dim=1)
        
        return x
```

---

## 4. DINOì™€ ìê¸°ì§€ë„í•™ìŠµ

### 4.1 ìê¸°ì§€ë„í•™ìŠµì˜ ê°œë…

#### ì§€ë„í•™ìŠµ vs ìê¸°ì§€ë„í•™ìŠµ
```python
# ì§€ë„í•™ìŠµ (Supervised Learning)
def supervised_learning():
    for image, label in dataset:
        prediction = model(image)
        loss = cross_entropy(prediction, label)  # ì •ë‹µ ë¼ë²¨ í•„ìš”
        loss.backward()

# ìê¸°ì§€ë„í•™ìŠµ (Self-Supervised Learning)  
def self_supervised_learning():
    for image in dataset:  # ë¼ë²¨ ë¶ˆí•„ìš”!
        # ì´ë¯¸ì§€ ìì²´ì—ì„œ í•™ìŠµ ì‹ í˜¸ ìƒì„±
        augmented1, augmented2 = augment(image), augment(image)
        
        # ê°™ì€ ì´ë¯¸ì§€ì˜ ë‹¤ë¥¸ ë·°ëŠ” ìœ ì‚¬í•´ì•¼ í•¨
        embedding1 = model(augmented1)
        embedding2 = model(augmented2)
        
        loss = similarity_loss(embedding1, embedding2)
        loss.backward()
```

#### ìê¸°ì§€ë„í•™ìŠµì˜ ì¥ì 
- **ë¼ë²¨ ì—†ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„° í™œìš© ê°€ëŠ¥**
- **ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ**: ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ì „ì´ ê°€ëŠ¥
- **ë¹„ìš© ì ˆê°**: ë¼ë²¨ë§ ë¹„ìš© ë¶ˆí•„ìš”
- **í¸í–¥ ê°ì†Œ**: ì¸ê°„ì˜ ë¼ë²¨ë§ í¸í–¥ ì œê±°

### 4.2 DINO (Self-Distillation with No Labels)

#### DINOì˜ í•µì‹¬ ì•„ì´ë””ì–´
DINOëŠ” "êµì‚¬-í•™ìƒ" êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¼ë²¨ ì—†ì´ Vision Transformerë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

```python
class DINO(nn.Module):
    def __init__(self, backbone, embed_dim=768, out_dim=65536):
        super().__init__()
        
        # í•™ìƒ ë„¤íŠ¸ì›Œí¬ (ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸)
        self.student = backbone
        self.student_head = DINOHead(embed_dim, out_dim)
        
        # êµì‚¬ ë„¤íŠ¸ì›Œí¬ (ì²œì²œíˆ ì—…ë°ì´íŠ¸)
        self.teacher = copy.deepcopy(backbone)
        self.teacher_head = DINOHead(embed_dim, out_dim)
        
        # êµì‚¬ ë„¤íŠ¸ì›Œí¬ëŠ” gradient ê³„ì‚° ì•ˆ í•¨
        for param in self.teacher.parameters():
            param.requires_grad = False
        for param in self.teacher_head.parameters():
            param.requires_grad = False
    
    def forward(self, x1, x2):
        # í•™ìƒ ë„¤íŠ¸ì›Œí¬ forward
        student_output1 = self.student_head(self.student(x1))
        student_output2 = self.student_head(self.student(x2))
        
        # êµì‚¬ ë„¤íŠ¸ì›Œí¬ forward (gradient ì—†ìŒ)
        with torch.no_grad():
            teacher_output1 = self.teacher_head(self.teacher(x1))
            teacher_output2 = self.teacher_head(self.teacher(x2))
        
        return student_output1, student_output2, teacher_output1, teacher_output2

class DINOHead(nn.Module):
    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=256):
        super().__init__()
        
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, bottleneck_dim)
        )
        
        # L2 ì •ê·œí™” í›„ ì„ í˜• ë³€í™˜
        self.last_layer = nn.Linear(bottleneck_dim, out_dim, bias=False)
        
    def forward(self, x):
        x = self.mlp(x)
        x = F.normalize(x, dim=-1, p=2)  # L2 ì •ê·œí™”
        x = self.last_layer(x)
        return x
```

#### DINO í•™ìŠµ ê³¼ì •
```python
def dino_training_step(model, images, optimizer, temperature_student=0.1, temperature_teacher=0.04):
    # 1. ì´ë¯¸ì§€ ì¦ê°•
    global_crops, local_crops = multi_crop_augmentation(images)
    
    # 2. í•™ìƒê³¼ êµì‚¬ ë„¤íŠ¸ì›Œí¬ forward
    student_outputs = []
    teacher_outputs = []
    
    # Global crops (í° ì´ë¯¸ì§€)
    for crop in global_crops:
        s_out, _, t_out, _ = model(crop, crop)
        student_outputs.append(s_out)
        teacher_outputs.append(t_out)
    
    # Local crops (ì‘ì€ ì´ë¯¸ì§€) - í•™ìƒë§Œ
    for crop in local_crops:
        s_out, _, _, _ = model(crop, crop)
        student_outputs.append(s_out)
    
    # 3. ì†ì‹¤ ê³„ì‚°
    loss = 0
    n_loss_terms = 0
    
    for i, teacher_out in enumerate(teacher_outputs):
        for j, student_out in enumerate(student_outputs):
            if i != j:  # ê°™ì€ ì´ë¯¸ì§€ì˜ ë‹¤ë¥¸ ë·°ë§Œ ë¹„êµ
                loss += dino_loss(
                    student_out, teacher_out,
                    temperature_student, temperature_teacher
                )
                n_loss_terms += 1
    
    loss = loss / n_loss_terms
    
    # 4. ì—­ì „íŒŒ
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 5. êµì‚¬ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (EMA)
    update_teacher_network(model.student, model.teacher, momentum=0.996)
    
    return loss.item()

def dino_loss(student_output, teacher_output, temp_s, temp_t):
    """
    DINO ì†ì‹¤ í•¨ìˆ˜: Cross-entropy between student and teacher
    """
    student_prob = F.log_softmax(student_output / temp_s, dim=-1)
    teacher_prob = F.softmax(teacher_output / temp_t, dim=-1)
    
    return -(teacher_prob * student_prob).sum(dim=-1).mean()

def update_teacher_network(student, teacher, momentum):
    """
    Exponential Moving Averageë¡œ êµì‚¬ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
    """
    for param_s, param_t in zip(student.parameters(), teacher.parameters()):
        param_t.data = momentum * param_t.data + (1 - momentum) * param_s.data
```

### 4.3 DINOv2ì˜ ê°œì„ ì‚¬í•­

#### DINOv2ì˜ ì£¼ìš” í˜ì‹ 
```python
class DINOv2Improvements:
    """
    DINOv2ì˜ ì£¼ìš” ê°œì„ ì‚¬í•­ë“¤
    """
    
    def __init__(self):
        # 1. ë” í° ë°ì´í„°ì…‹ (142M ì´ë¯¸ì§€)
        self.dataset_size = "142M curated images"
        
        # 2. ê°œì„ ëœ ë°ì´í„° íë ˆì´ì…˜
        self.data_curation = {
            "deduplication": "ì¤‘ë³µ ì´ë¯¸ì§€ ì œê±°",
            "retrieval_augmentation": "ê²€ìƒ‰ ê¸°ë°˜ ì¦ê°•",
            "quality_filtering": "í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§"
        }
        
        # 3. ì•ˆì •í™”ëœ í•™ìŠµ
        self.training_stability = {
            "koleo_regularization": "íŠ¹ì§• ë¶•ê´´ ë°©ì§€",
            "improved_augmentation": "ê°œì„ ëœ ë°ì´í„° ì¦ê°•",
            "better_initialization": "ë” ë‚˜ì€ ì´ˆê¸°í™”"
        }
        
        # 4. ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°
        self.model_variants = {
            "ViT-S/14": "Small model, 14x14 patches",
            "ViT-B/14": "Base model, 14x14 patches", 
            "ViT-L/14": "Large model, 14x14 patches",
            "ViT-g/14": "Giant model, 14x14 patches"
        }

# DINOv2 ì‚¬ìš© ì˜ˆì œ
def use_dinov2():
    from transformers import Dinov2Model, Dinov2Processor
    
    # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = Dinov2Processor.from_pretrained('facebook/dinov2-base')
    model = Dinov2Model.from_pretrained('facebook/dinov2-base')
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬
    image = Image.open("sample.jpg")
    inputs = processor(images=image, return_tensors="pt")
    
    # íŠ¹ì§• ì¶”ì¶œ
    with torch.no_grad():
        outputs = model(**inputs)
        features = outputs.last_hidden_state
        cls_token = features[:, 0]  # CLS í† í°
    
    return cls_token
```

---

## 5. ìµœì‹  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

### 5.1 ì£¼ìš” Vision ëª¨ë¸ë“¤ì˜ íŠ¹ì„± ë¹„êµ

#### ëª¨ë¸ë³„ ìƒì„¸ ë¶„ì„
```python
class ModelComparison:
    def __init__(self):
        self.models = {
            "ResNet-50": {
                "type": "CNN",
                "params": "25.6M",
                "accuracy": "76.1%",
                "inference_speed": "ë§¤ìš° ë¹ ë¦„",
                "memory": "ë‚®ìŒ",
                "strengths": ["íš¨ìœ¨ì„±", "ì•ˆì •ì„±", "ì‘ì€ ë°ì´í„°ì…‹"],
                "weaknesses": ["ì¥ê±°ë¦¬ ì˜ì¡´ì„±", "í™•ì¥ì„±"]
            },
            
            "EfficientNet-B7": {
                "type": "CNN",
                "params": "66.3M", 
                "accuracy": "84.4%",
                "inference_speed": "ë³´í†µ",
                "memory": "ë³´í†µ",
                "strengths": ["íš¨ìœ¨ì„±", "ì •í™•ë„", "ëª¨ë°”ì¼ ìµœì í™”"],
                "weaknesses": ["ë³µì¡í•œ êµ¬ì¡°", "í•™ìŠµ ì‹œê°„"]
            },
            
            "ViT-Base/16": {
                "type": "Transformer",
                "params": "86.6M",
                "accuracy": "81.8%",
                "inference_speed": "ë³´í†µ",
                "memory": "ë†’ìŒ",
                "strengths": ["í™•ì¥ì„±", "ì „ì´í•™ìŠµ", "í•´ì„ê°€ëŠ¥ì„±"],
                "weaknesses": ["ëŒ€ìš©ëŸ‰ ë°ì´í„° í•„ìš”", "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"]
            },
            
            "DeiT-Base": {
                "type": "Transformer",
                "params": "86.6M",
                "accuracy": "81.8%",
                "inference_speed": "ë³´í†µ", 
                "memory": "ë†’ìŒ",
                "strengths": ["ì§€ì‹ ì¦ë¥˜", "íš¨ìœ¨ì  í•™ìŠµ"],
                "weaknesses": ["ë³µì¡í•œ í•™ìŠµ ê³¼ì •"]
            },
            
            "Swin-Base": {
                "type": "Hierarchical Transformer",
                "params": "88.0M",
                "accuracy": "83.3%",
                "inference_speed": "ë³´í†µ",
                "memory": "ë³´í†µ",
                "strengths": ["ê³„ì¸µì  êµ¬ì¡°", "ë‹¤ì–‘í•œ í•´ìƒë„"],
                "weaknesses": ["ë³µì¡ì„±", "êµ¬í˜„ ë‚œì´ë„"]
            },
            
            "ConvNeXt-Base": {
                "type": "Modern CNN",
                "params": "89.0M",
                "accuracy": "83.8%",
                "inference_speed": "ë¹ ë¦„",
                "memory": "ë³´í†µ",
                "strengths": ["CNN+Transformer ì¥ì ", "íš¨ìœ¨ì„±"],
                "weaknesses": ["ìƒëŒ€ì ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ë¸"]
            },
            
            "DINOv2-Base": {
                "type": "Self-Supervised ViT",
                "params": "86.6M",
                "accuracy": "82.1%",
                "inference_speed": "ë³´í†µ",
                "memory": "ë†’ìŒ",
                "strengths": ["ìê¸°ì§€ë„í•™ìŠµ", "ì¼ë°˜í™”", "íŠ¹ì§• í’ˆì§ˆ"],
                "weaknesses": ["ë¼ë²¨ ë°ì´í„° ë¯¸í™œìš©"]
            }
        }
```

### 5.2 íƒœìŠ¤í¬ë³„ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

#### ê²°ì • íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
```python
def select_model(task_type, data_size, compute_budget, accuracy_requirement):
    """
    íƒœìŠ¤í¬ íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì¶”ì²œ
    """
    
    if task_type == "image_classification":
        if data_size == "small" and compute_budget == "low":
            return "ResNet-50 (Transfer Learning)"
        elif data_size == "large" and accuracy_requirement == "high":
            return "ViT-Large ë˜ëŠ” Swin-Large"
        else:
            return "EfficientNet-B4 ë˜ëŠ” ConvNeXt-Base"
    
    elif task_type == "object_detection":
        if compute_budget == "low":
            return "YOLO + ResNet backbone"
        else:
            return "DETR + ViT backbone"
    
    elif task_type == "semantic_segmentation":
        if accuracy_requirement == "high":
            return "Segformer ë˜ëŠ” SegViT"
        else:
            return "DeepLabV3+ + EfficientNet"
    
    elif task_type == "feature_extraction":
        return "DINOv2 (ìµœê³  í’ˆì§ˆ íŠ¹ì§•)"
    
    elif task_type == "zero_shot_classification":
        return "CLIP"
    
    else:
        return "ViT-Base (ë²”ìš©ì„±)"

# ì‚¬ìš© ì˜ˆì œ
recommendation = select_model(
    task_type="image_classification",
    data_size="medium",
    compute_budget="medium", 
    accuracy_requirement="high"
)
print(f"ì¶”ì²œ ëª¨ë¸: {recommendation}")
```

### 5.3 ì‹¤ì œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

#### ì¢…í•© ë²¤ì¹˜ë§ˆí¬ êµ¬í˜„
```python
import time
import torch
from torchvision import models, transforms
from transformers import ViTModel, DeiTModel

class ModelBenchmark:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def load_models(self):
        """ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ ë¡œë“œ"""
        models_dict = {}
        
        # CNN ëª¨ë¸ë“¤
        models_dict['resnet50'] = models.resnet50(pretrained=True)
        models_dict['efficientnet_b4'] = models.efficientnet_b4(pretrained=True)
        
        # Transformer ëª¨ë¸ë“¤
        models_dict['vit_base'] = ViTModel.from_pretrained('google/vit-base-patch16-224')
        models_dict['deit_base'] = DeiTModel.from_pretrained('facebook/deit-base-patch16-224')
        
        # ëª¨ë“  ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        for model in models_dict.values():
            model.eval().to(self.device)
        
        return models_dict
    
    def measure_inference_time(self, model, input_tensor, num_runs=100):
        """ì¶”ë¡  ì‹œê°„ ì¸¡ì •"""
        model.eval()
        
        # Warm-up
        with torch.no_grad():
            for _ in range(10):
                _ = model(input_tensor)
        
        # ì‹¤ì œ ì¸¡ì •
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(input_tensor)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        return avg_time * 1000  # ms ë‹¨ìœ„
    
    def measure_memory_usage(self, model, input_tensor):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            
            with torch.no_grad():
                _ = model(input_tensor)
            
            memory_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)
            return memory_mb
        else:
            return "N/A (CPU mode)"
    
    def run_comprehensive_benchmark(self):
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        models_dict = self.load_models()
        input_tensor = torch.randn(1, 3, 224, 224).to(self.device)
        
        results = {}
        
        for model_name, model in models_dict.items():
            print(f"\në²¤ì¹˜ë§ˆí‚¹: {model_name}")
            
            # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            inference_time = self.measure_inference_time(model, input_tensor)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_usage = self.measure_memory_usage(model, input_tensor)
            
            # ëª¨ë¸ í¬ê¸° ê³„ì‚°
            param_count = sum(p.numel() for p in model.parameters())
            model_size_mb = param_count * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
            
            results[model_name] = {
                'inference_time_ms': round(inference_time, 2),
                'memory_usage_mb': round(memory_usage, 2) if isinstance(memory_usage, float) else memory_usage,
                'model_size_mb': round(model_size_mb, 2),
                'parameters': f"{param_count / 1e6:.1f}M"
            }
        
        return results
    
    def print_benchmark_results(self, results):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*80)
        
        print(f"{'ëª¨ë¸ëª…':<20} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ë©”ëª¨ë¦¬(MB)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'íŒŒë¼ë¯¸í„°':<15}")
        print("-" * 80)
        
        for model_name, metrics in results.items():
            print(f"{model_name:<20} {metrics['inference_time_ms']:<15} "
                  f"{metrics['memory_usage_mb']:<15} {metrics['model_size_mb']:<15} "
                  f"{metrics['parameters']:<15}")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
if __name__ == "__main__":
    benchmark = ModelBenchmark()
    results = benchmark.run_comprehensive_benchmark()
    benchmark.print_benchmark_results(results)
```

### 5.4 ë©€í‹°ëª¨ë‹¬ API ì„±ëŠ¥ ë¹„êµ

#### API ì‘ë‹µ ì‹œê°„ ë° ì •í™•ë„ ë¹„êµ
```python
import asyncio
import aiohttp
import time
from typing import Dict, List

class MultimodalAPIBenchmark:
    def __init__(self):
        self.apis = {
            "gemini": {
                "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent",
                "headers": {"Content-Type": "application/json"},
                "free_tier": "ë§¤ìš° ê´€ëŒ€í•¨"
            },
            "gpt4v": {
                "endpoint": "https://api.openai.com/v1/chat/completions", 
                "headers": {"Content-Type": "application/json"},
                "free_tier": "ì œí•œì "
            },
            "llama_vision": {
                "endpoint": "https://api.together.xyz/inference",
                "headers": {"Content-Type": "application/json"},
                "free_tier": "3ê°œì›” ë¬´ë£Œ"
            },
            "claude_vision": {
                "endpoint": "https://api.anthropic.com/v1/messages",
                "headers": {"Content-Type": "application/json"},
                "free_tier": "ì œí•œì "
            }
        }
    
    async def test_api_response_time(self, api_name: str, image_path: str, prompt: str) -> Dict:
        """API ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            # ì‹¤ì œ API í˜¸ì¶œ (ì˜ì‚¬ì½”ë“œ)
            response = await self.call_api(api_name, image_path, prompt)
            end_time = time.time()
            
            return {
                "api": api_name,
                "response_time": round((end_time - start_time) * 1000, 2),  # ms
                "success": True,
                "response_length": len(response.get("text", "")),
                "error": None
            }
        
        except Exception as e:
            end_time = time.time()
            return {
                "api": api_name,
                "response_time": round((end_time - start_time) * 1000, 2),
                "success": False,
                "response_length": 0,
                "error": str(e)
            }
    
    async def run_comprehensive_api_test(self, test_images: List[str], prompts: List[str]):
        """ì¢…í•© API í…ŒìŠ¤íŠ¸"""
        results = []
        
        for image_path in test_images:
            for prompt in prompts:
                for api_name in self.apis.keys():
                    result = await self.test_api_response_time(api_name, image_path, prompt)
                    result["image"] = image_path
                    result["prompt"] = prompt
                    results.append(result)
        
        return results
    
    def analyze_results(self, results: List[Dict]) -> Dict:
        """ê²°ê³¼ ë¶„ì„"""
        analysis = {}
        
        for api_name in self.apis.keys():
            api_results = [r for r in results if r["api"] == api_name and r["success"]]
            
            if api_results:
                response_times = [r["response_time"] for r in api_results]
                analysis[api_name] = {
                    "avg_response_time": round(sum(response_times) / len(response_times), 2),
                    "min_response_time": min(response_times),
                    "max_response_time": max(response_times),
                    "success_rate": len(api_results) / len([r for r in results if r["api"] == api_name]) * 100,
                    "avg_response_length": round(sum(r["response_length"] for r in api_results) / len(api_results), 2)
                }
            else:
                analysis[api_name] = {
                    "avg_response_time": "N/A",
                    "success_rate": 0,
                    "error": "ëª¨ë“  ìš”ì²­ ì‹¤íŒ¨"
                }
        
        return analysis

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ (ì˜ì‚¬ì½”ë“œ)
"""
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
benchmark = MultimodalAPIBenchmark()
test_images = ["test1.jpg", "test2.jpg", "test3.jpg"]
prompts = ["ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”", "ì´ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”"]

results = await benchmark.run_comprehensive_api_test(test_images, prompts)
analysis = benchmark.analyze_results(results)

# ê²°ê³¼ ì¶œë ¥
for api, metrics in analysis.items():
    print(f"{api}: í‰ê·  ì‘ë‹µì‹œê°„ {metrics['avg_response_time']}ms, "
          f"ì„±ê³µë¥  {metrics['success_rate']}%")
"""
```

---

## 6. ì‹¤ìŠµ í”„ë¡œì íŠ¸: ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ ì•±

### 6.1 í”„ë¡œì íŠ¸ ê°œìš”

#### ëª©í‘œ
ë‹¤ì–‘í•œ Vision ëª¨ë¸ê³¼ ë©€í‹°ëª¨ë‹¬ APIì˜ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

#### ì£¼ìš” ê¸°ëŠ¥
1. **ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**: ViT, ResNet, EfficientNet ë“±
2. **API ì‘ë‹µ ë¹„êµ**: Gemini, GPT-4V, Llama Vision
3. **ì‹¤ì‹œê°„ ë²¤ì¹˜ë§ˆí‚¹**: ì¶”ë¡  ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
4. **ì‹œê°ì  ê²°ê³¼ í‘œì‹œ**: ì°¨íŠ¸ì™€ ê·¸ë˜í”„ë¡œ ê²°ê³¼ ì‹œê°í™”

### 6.2 í†µí•© ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ êµ¬í˜„

```python
import gradio as gr
import torch
import time
import matplotlib.pyplot as plt
import pandas as pd
from PIL import Image
import io
import base64

class UnifiedBenchmarkSystem:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.models = {}
        self.load_all_models()
    
    def load_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        try:
            # CNN ëª¨ë¸ë“¤
            self.models['ResNet-50'] = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)
            self.models['EfficientNet-B4'] = torch.hub.load('rwightman/gen-efficientnet-pytorch', 'efficientnet_b4', pretrained=True)
            
            # Transformer ëª¨ë¸ë“¤
            from transformers import ViTModel, ViTImageProcessor
            self.models['ViT-Base'] = ViTModel.from_pretrained('google/vit-base-patch16-224')
            self.vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
            
            # DINOv2
            self.models['DINOv2'] = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
            
            # ëª¨ë“  ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            for model in self.models.values():
                model.eval().to(self.device)
                
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def benchmark_single_model(self, model_name, image, num_runs=10):
        """ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        if model_name not in self.models:
            return {"error": f"ëª¨ë¸ {model_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        model = self.models[model_name]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if model_name == 'ViT-Base':
            inputs = self.vit_processor(images=image, return_tensors="pt")
            input_tensor = inputs['pixel_values'].to(self.device)
        else:
            # í‘œì¤€ ImageNet ì „ì²˜ë¦¬
            from torchvision import transforms
            transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            input_tensor = transform(image).unsqueeze(0).to(self.device)
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        times = []
        memory_usage = []
        
        for _ in range(num_runs):
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
            
            start_time = time.time()
            
            with torch.no_grad():
                output = model(input_tensor)
            
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
            
            if torch.cuda.is_available():
                memory_usage.append(torch.cuda.max_memory_allocated() / (1024 * 1024))  # MB
        
        return {
            "model": model_name,
            "avg_inference_time": round(sum(times) / len(times), 2),
            "std_inference_time": round(pd.Series(times).std(), 2),
            "avg_memory_usage": round(sum(memory_usage) / len(memory_usage), 2) if memory_usage else "N/A",
            "min_time": round(min(times), 2),
            "max_time": round(max(times), 2)
        }
    
    def compare_all_models(self, image):
        """ëª¨ë“  ëª¨ë¸ ë¹„êµ"""
        results = []
        
        for model_name in self.models.keys():
            try:
                result = self.benchmark_single_model(model_name, image)
                if "error" not in result:
                    results.append(result)
            except Exception as e:
                print(f"{model_name} ë²¤ì¹˜ë§ˆí¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return results
    
    def create_comparison_chart(self, results):
        """ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        if not results:
            return None
        
        df = pd.DataFrame(results)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ì¶”ë¡  ì‹œê°„ ë¹„êµ
        ax1.bar(df['model'], df['avg_inference_time'], color='skyblue', alpha=0.7)
        ax1.set_title('í‰ê·  ì¶”ë¡  ì‹œê°„ ë¹„êµ')
        ax1.set_ylabel('ì‹œê°„ (ms)')
        ax1.tick_params(axis='x', rotation=45)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ (CUDA ì‚¬ìš© ì‹œë§Œ)
        if df['avg_memory_usage'].dtype != 'object':
            ax2.bar(df['model'], df['avg_memory_usage'], color='lightcoral', alpha=0.7)
            ax2.set_title('í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ')
            ax2.set_ylabel('ë©”ëª¨ë¦¬ (MB)')
            ax2.tick_params(axis='x', rotation=45)
        else:
            ax2.text(0.5, 0.5, 'GPU ë©”ëª¨ë¦¬ ì •ë³´ ì—†ìŒ\n(CPU ëª¨ë“œ)', 
                    ha='center', va='center', transform=ax2.transAxes)
            ax2.set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰')
        
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ë¡œ ë³€í™˜
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
        buf.seek(0)
        plt.close()
        
        return Image.open(buf)
    
    def test_multimodal_apis(self, image, prompt="ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"):
        """ë©€í‹°ëª¨ë‹¬ API í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° APIë¥¼ í˜¸ì¶œ
        api_results = {
            "Gemini Vision": {
                "response_time": 1200,  # ms
                "response": "ì´ ì´ë¯¸ì§€ëŠ” ê³ ì–‘ì´ê°€ ì†ŒíŒŒì— ì•‰ì•„ìˆëŠ” ëª¨ìŠµì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
                "confidence": 0.95
            },
            "GPT-4V": {
                "response_time": 2100,
                "response": "ì‚¬ì§„ì—ëŠ” í„¸ì´ ë³µìŠ¬ë³µìŠ¬í•œ ê³ ì–‘ì´ê°€ í¸ì•ˆí•˜ê²Œ ì†ŒíŒŒì— ì•‰ì•„ìˆìŠµë‹ˆë‹¤.",
                "confidence": 0.92
            },
            "Llama Vision": {
                "response_time": 1800,
                "response": "ì†ŒíŒŒ ìœ„ì— ì•‰ì€ ê³ ì–‘ì´ì˜ ì´ë¯¸ì§€ì…ë‹ˆë‹¤. ê³ ì–‘ì´ëŠ” ì¹´ë©”ë¼ë¥¼ ë°”ë¼ë³´ê³  ìˆìŠµë‹ˆë‹¤.",
                "confidence": 0.88
            }
        }
        
        return api_results

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    benchmark_system = UnifiedBenchmarkSystem()
    
    def run_benchmark(image):
        if image is None:
            return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", None, "ê²°ê³¼ ì—†ìŒ"
        
        # ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = benchmark_system.compare_all_models(image)
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        if results:
            df = pd.DataFrame(results)
            table_html = df.to_html(index=False, classes='benchmark-table')
            
            # ì°¨íŠ¸ ìƒì„±
            chart = benchmark_system.create_comparison_chart(results)
            
            # API í…ŒìŠ¤íŠ¸
            api_results = benchmark_system.test_multimodal_apis(image)
            api_text = "\n".join([f"{api}: {data['response']} (ì‘ë‹µì‹œê°„: {data['response_time']}ms)" 
                                 for api, data in api_results.items()])
            
            return table_html, chart, api_text
        else:
            return "ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", None, "API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
    with gr.Blocks(title="ğŸš€ Vision Model & API ë²¤ì¹˜ë§ˆí¬", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸš€ Vision Model & Multimodal API ë²¤ì¹˜ë§ˆí¬
        
        ë‹¤ì–‘í•œ Vision ëª¨ë¸ê³¼ ë©€í‹°ëª¨ë‹¬ APIì˜ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¹„êµí•´ë³´ì„¸ìš”!
        
        ## ì§€ì› ëª¨ë¸:
        - **CNN**: ResNet-50, EfficientNet-B4
        - **Transformer**: ViT-Base, DINOv2
        
        ## ì¸¡ì • ì§€í‘œ:
        - ì¶”ë¡  ì‹œê°„ (ms)
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
        - API ì‘ë‹µ ì‹œê°„
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                image_input = gr.Image(
                    type="pil", 
                    label="í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                    height=300
                )
                
                benchmark_btn = gr.Button(
                    "ğŸ”¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰", 
                    variant="primary",
                    size="lg"
                )
            
            with gr.Column(scale=2):
                with gr.Tab("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"):
                    results_table = gr.HTML(label="ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
                    performance_chart = gr.Image(label="ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸")
                
                with gr.Tab("ğŸ¤– API ì‘ë‹µ ë¹„êµ"):
                    api_results = gr.Textbox(
                        label="ë©€í‹°ëª¨ë‹¬ API ì‘ë‹µ",
                        lines=10,
                        placeholder="API í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤..."
                    )
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        benchmark_btn.click(
            fn=run_benchmark,
            inputs=[image_input],
            outputs=[results_table, performance_chart, api_results]
        )
        
        # ì˜ˆì œ ì´ë¯¸ì§€ ì¶”ê°€
        gr.Examples(
            examples=[
                ["examples/cat.jpg"],
                ["examples/dog.jpg"], 
                ["examples/car.jpg"]
            ],
            inputs=[image_input]
        )
    
    return demo

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        debug=True
    )
```

### 6.3 HuggingFace Space ë°°í¬

#### ë°°í¬ìš© íŒŒì¼ êµ¬ì„±
```python
# app.py - ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# requirements.txt - ì˜ì¡´ì„± íŒ¨í‚¤ì§€
# README.md - í”„ë¡œì íŠ¸ ì„¤ëª…

# requirements.txt ë‚´ìš©:
"""
gradio>=4.0.0
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
pillow>=9.0.0
matplotlib>=3.5.0
pandas>=1.5.0
numpy>=1.21.0
"""

# README.md ë‚´ìš©:
"""
# ğŸš€ Vision Model & Multimodal API Benchmark

ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ì–‘í•œ Vision ëª¨ë¸ê³¼ ë©€í‹°ëª¨ë‹¬ APIì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.

## ê¸°ëŠ¥
- Vision Transformer, CNN ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- ì¶”ë¡  ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
- ë©€í‹°ëª¨ë‹¬ API ì‘ë‹µ ì‹œê°„ ë¹„êµ
- ì‹¤ì‹œê°„ ì°¨íŠ¸ ë° ì‹œê°í™”

## ì‚¬ìš©ë²•
1. ì´ë¯¸ì§€ ì—…ë¡œë“œ
2. 'ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰' ë²„íŠ¼ í´ë¦­
3. ê²°ê³¼ í™•ì¸

## ì§€ì› ëª¨ë¸
- ResNet-50, EfficientNet-B4
- ViT-Base, DINOv2
- Gemini Vision, GPT-4V, Llama Vision
"""
```

---

## 7. ì‹¤ìŠµ ê³¼ì œ ë° í‰ê°€

### 7.1 ì‹¤ìŠµ ê³¼ì œ

#### ê³¼ì œ 1: ViT êµ¬í˜„ ë° ë¶„ì„
```python
# ê³¼ì œ ìš”êµ¬ì‚¬í•­
class ViTImplementationTask:
    """
    Vision Transformer êµ¬í˜„ ê³¼ì œ
    """
    
    def requirements(self):
        return {
            "basic_implementation": {
                "patch_embedding": "íŒ¨ì¹˜ ì„ë² ë”© êµ¬í˜„",
                "self_attention": "Self-Attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„", 
                "transformer_block": "Transformer ë¸”ë¡ êµ¬í˜„",
                "classification_head": "ë¶„ë¥˜ í—¤ë“œ êµ¬í˜„"
            },
            
            "analysis": {
                "attention_visualization": "Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”",
                "feature_analysis": "ì¤‘ê°„ íŠ¹ì§• ë¶„ì„",
                "performance_comparison": "CNNê³¼ ì„±ëŠ¥ ë¹„êµ"
            },
            
            "optimization": {
                "efficiency_improvement": "íš¨ìœ¨ì„± ê°œì„ ",
                "memory_optimization": "ë©”ëª¨ë¦¬ ìµœì í™”",
                "inference_speed": "ì¶”ë¡  ì†ë„ í–¥ìƒ"
            }
        }
    
    def evaluation_criteria(self):
        return {
            "correctness": "êµ¬í˜„ ì •í™•ì„± (30%)",
            "analysis_quality": "ë¶„ì„ í’ˆì§ˆ (25%)",
            "optimization": "ìµœì í™” ì •ë„ (20%)",
            "documentation": "ë¬¸ì„œí™” (15%)",
            "creativity": "ì°½ì˜ì„± (10%)"
        }
```

#### ê³¼ì œ 2: DINO ìê¸°ì§€ë„í•™ìŠµ ì‹¤í—˜
```python
class DINOExperimentTask:
    """
    DINO ìê¸°ì§€ë„í•™ìŠµ ì‹¤í—˜ ê³¼ì œ
    """
    
    def experiment_design(self):
        return {
            "data_preparation": {
                "dataset_selection": "ì ì ˆí•œ ë°ì´í„°ì…‹ ì„ íƒ",
                "augmentation_strategy": "ë°ì´í„° ì¦ê°• ì „ëµ ì„¤ê³„",
                "preprocessing": "ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
            },
            
            "model_training": {
                "hyperparameter_tuning": "í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •",
                "training_monitoring": "í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§",
                "convergence_analysis": "ìˆ˜ë ´ ë¶„ì„"
            },
            
            "evaluation": {
                "feature_quality": "íŠ¹ì§• í’ˆì§ˆ í‰ê°€",
                "downstream_tasks": "ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ ì„±ëŠ¥",
                "comparison_study": "ë‹¤ë¥¸ ë°©ë²•ê³¼ ë¹„êµ"
            }
        }
```

### 7.2 í”„ë¡œì íŠ¸ í‰ê°€ ê¸°ì¤€

#### ì¢…í•© í‰ê°€ ë§¤íŠ¸ë¦­ìŠ¤
```python
class ProjectEvaluation:
    def __init__(self):
        self.criteria = {
            "technical_implementation": {
                "weight": 0.4,
                "subcriteria": {
                    "code_quality": 0.3,
                    "algorithm_correctness": 0.4,
                    "efficiency": 0.3
                }
            },
            
            "analysis_depth": {
                "weight": 0.25,
                "subcriteria": {
                    "theoretical_understanding": 0.4,
                    "experimental_design": 0.3,
                    "result_interpretation": 0.3
                }
            },
            
            "innovation": {
                "weight": 0.2,
                "subcriteria": {
                    "novel_approaches": 0.5,
                    "creative_solutions": 0.5
                }
            },
            
            "presentation": {
                "weight": 0.15,
                "subcriteria": {
                    "documentation": 0.4,
                    "visualization": 0.3,
                    "user_interface": 0.3
                }
            }
        }
    
    def calculate_score(self, scores_dict):
        """ì ìˆ˜ ê³„ì‚°"""
        total_score = 0
        
        for main_criterion, main_data in self.criteria.items():
            main_weight = main_data["weight"]
            subcriteria = main_data["subcriteria"]
            
            main_score = 0
            for sub_criterion, sub_weight in subcriteria.items():
                sub_score = scores_dict.get(f"{main_criterion}_{sub_criterion}", 0)
                main_score += sub_score * sub_weight
            
            total_score += main_score * main_weight
        
        return round(total_score, 2)
```

---

## 8. ë‹¤ìŒ ì£¼ì°¨ ì˜ˆê³  ë° ì—°ê³„ì„±

### 8.1 Week 5 ì˜ˆê³ : ê°ì²´ íƒì§€ ì´ë¡  + YOLO ì‹¤ìŠµ

#### ì—°ê³„ í•™ìŠµ í¬ì¸íŠ¸
```python
class Week5Preview:
    def __init__(self):
        self.connection_points = {
            "from_week4": {
                "attention_mechanism": "DETRì˜ Transformer ê¸°ë°˜ ê°ì²´ íƒì§€",
                "feature_extraction": "ViT backboneì„ ì‚¬ìš©í•œ ê°ì²´ íƒì§€",
                "self_supervised_features": "DINO íŠ¹ì§•ì„ í™œìš©í•œ íƒì§€ ì„±ëŠ¥ í–¥ìƒ"
            },
            
            "new_concepts": {
                "object_detection": "ê°ì²´ íƒì§€ ê¸°ë³¸ ê°œë…",
                "yolo_architecture": "YOLO ì•„í‚¤í…ì²˜ ë¶„ì„",
                "anchor_boxes": "ì•µì»¤ ë°•ìŠ¤ì™€ NMS",
                "loss_functions": "ê°ì²´ íƒì§€ ì†ì‹¤ í•¨ìˆ˜"
            },
            
            "practical_applications": {
                "real_time_detection": "ì‹¤ì‹œê°„ ê°ì²´ íƒì§€",
                "custom_dataset": "ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í•™ìŠµ",
                "deployment": "ëª¨ë°”ì¼/ì›¹ ë°°í¬"
            }
        }
```

### 8.2 í•™ìŠµ ì—°ì†ì„± í™•ë³´

#### ì§€ì‹ ëˆ„ì  ì²´ê³„
```python
def knowledge_accumulation_system():
    """
    ì£¼ì°¨ë³„ ì§€ì‹ ëˆ„ì  ì²´ê³„
    """
    
    cumulative_knowledge = {
        "week1": ["Google AI Studio", "ê¸°ë³¸ ì´ë¯¸ì§€ ë¶„ì„"],
        "week2": ["CNN ê¸°ì´ˆ", "HuggingFace", "ì´ë¯¸ì§€ ì²˜ë¦¬"],
        "week3": ["Transfer Learning", "CLIP", "ë©€í‹°ëª¨ë‹¬ API"],
        "week4": ["Vision Transformer", "Self-Attention", "DINO", "ìê¸°ì§€ë„í•™ìŠµ"],
        "week5": ["ê°ì²´ íƒì§€", "YOLO", "ì‹¤ì‹œê°„ ì²˜ë¦¬"],  # ì˜ˆì •
        "week6": ["ì„¸ê·¸ë©˜í…Œì´ì…˜", "SAM", "í”½ì…€ ë‹¨ìœ„ ë¶„ì„"],  # ì˜ˆì •
    }
    
    integration_projects = {
        "week4_integration": {
            "models": ["ViT", "DINO", "ResNet", "EfficientNet"],
            "apis": ["Gemini", "GPT-4V", "Llama Vision"],
            "techniques": ["Self-Attention", "Transfer Learning", "Self-Supervised Learning"]
        }
    }
    
    return cumulative_knowledge, integration_projects
```

---

## ğŸ“š ì°¸ê³  ìë£Œ ë° ì¶”ê°€ í•™ìŠµ

### ë…¼ë¬¸ ë° ë¬¸ì„œ
- **Vision Transformer**: "An Image is Worth 16x16 Words" (Dosovitskiy et al., 2020)
- **DINO**: "Emerging Properties in Self-Supervised Vision Transformers" (Caron et al., 2021)
- **DINOv2**: "DINOv2: Learning Robust Visual Features without Supervision" (Oquab et al., 2023)
- **Attention Is All You Need**: Transformerì˜ ì›ì¡° ë…¼ë¬¸ (Vaswani et al., 2017)

### ì‹¤ìŠµ ì½”ë“œ ë° íŠœí† ë¦¬ì–¼
- [HuggingFace Transformers ë¬¸ì„œ](https://huggingface.co/docs/transformers)
- [PyTorch Vision Transformer íŠœí† ë¦¬ì–¼](https://pytorch.org/vision/stable/models.html#vision-transformer)
- [DINO ê³µì‹ êµ¬í˜„](https://github.com/facebookresearch/dino)
- [DINOv2 ê³µì‹ êµ¬í˜„](https://github.com/facebookresearch/dinov2)

### ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤
- [Papers With Code - Vision Transformer](https://paperswithcode.com/method/vision-transformer)
- [Distill.pub - Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

---

## ğŸ¯ ì´ë²ˆ ì£¼ì°¨ í•µì‹¬ ì •ë¦¬

### í•™ìŠµ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

âœ… **Self-Attention ë©”ì»¤ë‹ˆì¦˜ ì´í•´**
- ìˆ˜í•™ì  ì›ë¦¬ì™€ êµ¬í˜„ ë°©ë²•
- Multi-Head Attentionì˜ í•„ìš”ì„±
- ì‹œê°í™”ë¥¼ í†µí•œ ì‘ë™ ì›ë¦¬ í™•ì¸

âœ… **Vision Transformer (ViT) ë§ˆìŠ¤í„°**
- íŒ¨ì¹˜ ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©
- Transformer ë¸”ë¡ êµ¬ì¡°
- ë¶„ë¥˜ë¥¼ ìœ„í•œ CLS í† í° í™œìš©

âœ… **DINO ìê¸°ì§€ë„í•™ìŠµ ì´í•´**
- êµì‚¬-í•™ìƒ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
- Contrastive Learning ì›ë¦¬
- DINOv2ì˜ ê°œì„ ì‚¬í•­

âœ… **ìµœì‹  ëª¨ë¸ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ**
- íƒœìŠ¤í¬ë³„ ìµœì  ëª¨ë¸ ì„ íƒ
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë°©ë²•ë¡ 
- ë©€í‹°ëª¨ë‹¬ API í™œìš©ë²•

âœ… **í†µí•© ë²¤ì¹˜ë§ˆí¬ ì•± êµ¬ì¶•**
- ì‹¤ì‹œê°„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- ì‹œê°ì  ê²°ê³¼ í‘œì‹œ
- HuggingFace Space ë°°í¬

**ğŸš€ ì´ì œ ì—¬ëŸ¬ë¶„ì€ ìµœì‹  Vision AI ê¸°ìˆ ì˜ í•µì‹¬ì„ ì™„ì „íˆ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤!**

ë‹¤ìŒ ì£¼ì—ëŠ” ì´ëŸ¬í•œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ê°ì²´ íƒì§€ì™€ YOLO ì‹¤ìŠµì„ ì§„í–‰í•˜ì—¬, ì‹¤ì‹œê°„ ì»´í“¨í„° ë¹„ì „ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶• ëŠ¥ë ¥ì„ í‚¤ì›Œë³´ê² ìŠµë‹ˆë‹¤.

