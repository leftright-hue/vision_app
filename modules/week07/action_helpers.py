"""
í–‰ë™ì¸ì‹ í—¬í¼ í´ë˜ìŠ¤
3-tier fallback ì „ëµ: HuggingFace Transformers â†’ OpenCV â†’ Simulation
"""

import numpy as np
from PIL import Image
import streamlit as st
from typing import Optional, List, Dict, Tuple, Any, Union
import warnings
import sys
import os
import traceback

# BaseImageProcessor import
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from core.base_processor import BaseImageProcessor

# HuggingFace ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
MODEL_REGISTRY = {
    'videomae': 'MCG-NJU/videomae-base-finetuned-kinetics',
    'timesformer': 'facebook/timesformer-base-finetuned-k400',
    'xclip': 'microsoft/xclip-base-patch32',
}


class VideoHelper(BaseImageProcessor):
    """
    ë¹„ë””ì˜¤ í–‰ë™ì¸ì‹ í—¬í¼ í´ë˜ìŠ¤
    - 1ìˆœìœ„: HuggingFace Transformers (transformers íŒ¨í‚¤ì§€ + VideoMAE/TimeSformer ëª¨ë¸)
    - 2ìˆœìœ„: OpenCV only (ë¹„ë””ì˜¤ ì²˜ë¦¬, Optical Flowë§Œ ê°€ëŠ¥, ML ëª¨ë¸ ì—†ìŒ)
    - 3ìˆœìœ„: Simulation mode (ê¸°ë³¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜)
    """

    def __init__(self):
        """
        VideoHelper ì´ˆê¸°í™”
        - 3-tier fallbackìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“œ ê°ì§€
        - ë””ë°”ì´ìŠ¤ ê°ì§€ (CPU/GPU)
        """
        super().__init__()
        self.mode = None  # 'transformers', 'opencv', 'simulation'
        self.device = None  # 'cuda', 'cpu', None
        self.model = None
        self.processor = None
        self.pipeline = None

        self._initialize()

    def _initialize(self):
        """
        3-tier fallbackìœ¼ë¡œ ì´ˆê¸°í™”
        """
        # Tier 1: Try HuggingFace Transformers
        if self._try_transformers():
            self.mode = 'transformers'
            st.success("âœ… í–‰ë™ì¸ì‹ ì¤€ë¹„ ì™„ë£Œ (HuggingFace Transformers)")
            return

        # Tier 2: Try OpenCV only
        if self._try_opencv():
            self.mode = 'opencv'
            st.info("â„¹ï¸ OpenCV ëª¨ë“œ í™œì„±í™” (ë¹„ë””ì˜¤ ì²˜ë¦¬ ê°€ëŠ¥, ML ëª¨ë¸ ë¯¸ì‚¬ìš©)\n\n"
                   "í–‰ë™ ë¶„ë¥˜ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´:\n"
                   "```bash\n"
                   "pip install transformers torch\n"
                   "```")
            return

        # Tier 3: Fallback to simulation
        self.mode = 'simulation'
        st.warning("âš ï¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ì‹¤ì œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë¯¸ì‚¬ìš©)\n\n"
                  "ì‹¤ì œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´:\n"
                  "```bash\n"
                  "pip install opencv-python transformers torch\n"
                  "```")

    def _try_transformers(self) -> bool:
        """
        HuggingFace Transformersë¡œ ë¡œë“œ ì‹œë„
        VideoMAE, TimeSformer, X-CLIP ë“± ë¹„ë””ì˜¤ ëª¨ë¸ ì§€ì›
        """
        try:
            import transformers
            import torch

            # ë””ë°”ì´ìŠ¤ ê°ì§€
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            if self.device == "cpu":
                st.info("â„¹ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. (ë¹„ë””ì˜¤ ì²˜ë¦¬ëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ)")

            # í•„ìš”í•œ ëª¨ë“ˆì´ ìˆëŠ”ì§€ë§Œ í™•ì¸ (ì‹¤ì œ ëª¨ë¸ì€ ë‚˜ì¤‘ì— ë¡œë“œ)
            return True

        except ImportError:
            return False
        except Exception as e:
            st.error(f"Transformers ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _try_opencv(self) -> bool:
        """
        OpenCV ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        ë¹„ë””ì˜¤ ì²˜ë¦¬ì™€ Optical FlowëŠ” ê°€ëŠ¥í•˜ì§€ë§Œ ML ëª¨ë¸ì€ ì—†ìŒ
        """
        try:
            import cv2
            return True
        except ImportError:
            return False
        except Exception as e:
            return False

    def _detect_device(self) -> Optional[str]:
        """
        CUDA ë””ë°”ì´ìŠ¤ ê°ì§€
        Returns:
            'cuda', 'cpu', or None
        """
        try:
            import torch
            return "cuda" if torch.cuda.is_available() else "cpu"
        except ImportError:
            return None

    def get_mode(self) -> str:
        """
        í˜„ì¬ ë™ì‘ ëª¨ë“œ ë°˜í™˜
        Returns:
            'transformers', 'opencv', 'simulation'
        """
        return self.mode

    def get_device(self) -> Optional[str]:
        """
        í˜„ì¬ ë””ë°”ì´ìŠ¤ ë°˜í™˜
        Returns:
            'cuda', 'cpu', or None
        """
        return self.device

    def is_available(self, feature: str) -> bool:
        """
        íŠ¹ì • ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

        Args:
            feature: 'video_processing', 'optical_flow', 'action_classification'

        Returns:
            bool: ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        """
        if feature == 'video_processing':
            return self.mode in ['transformers', 'opencv']
        elif feature == 'optical_flow':
            return self.mode in ['transformers', 'opencv']
        elif feature == 'action_classification':
            return self.mode == 'transformers'
        else:
            return False

    def extract_frames(
        self,
        video_path: str,
        sample_rate: int = 30,
        max_frames: int = 100,
        target_size: Tuple[int, int] = (224, 224)
    ) -> List[np.ndarray]:
        """
        ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ì˜ˆ: 30ì´ë©´ 30í”„ë ˆì„ë‹¹ 1ê°œ ì¶”ì¶œ)
            max_frames: ìµœëŒ€ í”„ë ˆì„ ìˆ˜ (ë©”ëª¨ë¦¬ ì œí•œ)
            target_size: ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸° (width, height)

        Returns:
            List[np.ndarray]: í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸, ê° í”„ë ˆì„ shape (H, W, 3) RGB
        """
        # Simulation mode: ëœë¤ í”„ë ˆì„ ìƒì„±
        if self.mode == 'simulation':
            st.info("â„¹ï¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: ëœë¤ í”„ë ˆì„ ìƒì„±")
            frames = []
            for _ in range(min(10, max_frames)):
                frame = np.random.randint(0, 255, (target_size[1], target_size[0], 3), dtype=np.uint8)
                frames.append(frame)
            return frames

        # OpenCV or Transformers mode
        try:
            import cv2

            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

            # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))

            # ì‹¤ì œ ìƒ˜í”Œë§ ë ˆì´íŠ¸ ê³„ì‚° (max_frames ì œí•œ ê³ ë ¤)
            actual_sample_rate = max(sample_rate, total_frames // max_frames) if total_frames > max_frames else sample_rate

            st.info(f"ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´: {total_frames}í”„ë ˆì„, {fps}fps\n"
                   f"ìƒ˜í”Œë§: ë§¤ {actual_sample_rate}í”„ë ˆì„ë‹¹ 1ê°œ ì¶”ì¶œ")

            frames = []
            frame_idx = 0

            # í”„ë ˆì„ ì¶”ì¶œ
            while len(frames) < max_frames:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()

                if not ret:
                    break

                # BGR â†’ RGB ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # ë¦¬ì‚¬ì´ì¦ˆ
                frame_resized = cv2.resize(frame_rgb, target_size, interpolation=cv2.INTER_LINEAR)

                frames.append(frame_resized)
                frame_idx += actual_sample_rate

            cap.release()

            st.success(f"âœ… {len(frames)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")
            return frames

        except ImportError:
            st.error("âŒ OpenCVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        except Exception as e:
            st.error(f"âŒ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def compute_optical_flow(
        self,
        frame1: np.ndarray,
        frame2: np.ndarray
    ) -> np.ndarray:
        """
        ë‘ í”„ë ˆì„ ê°„ Optical Flow ê³„ì‚° (Farneback ì•Œê³ ë¦¬ì¦˜)

        Args:
            frame1: ì²« ë²ˆì§¸ í”„ë ˆì„ (H, W, 3) RGB
            frame2: ë‘ ë²ˆì§¸ í”„ë ˆì„ (H, W, 3) RGB

        Returns:
            np.ndarray: Optical flow (H, W, 2) - [dx, dy] ëª¨ì…˜ ë²¡í„°
        """
        # Simulation mode: ëœë¤ flow ìƒì„±
        if self.mode == 'simulation':
            h, w = frame1.shape[:2]
            flow = np.random.randn(h, w, 2).astype(np.float32) * 2
            return flow

        # OpenCV or Transformers mode
        try:
            import cv2

            # RGB â†’ Grayscale
            gray1 = cv2.cvtColor(frame1, cv2.COLOR_RGB2GRAY)
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_RGB2GRAY)

            # Farneback Optical Flow ê³„ì‚°
            flow = cv2.calcOpticalFlowFarneback(
                gray1, gray2,
                None,
                pyr_scale=0.5,    # í”¼ë¼ë¯¸ë“œ ìŠ¤ì¼€ì¼
                levels=3,          # í”¼ë¼ë¯¸ë“œ ë ˆë²¨
                winsize=15,        # ìœˆë„ìš° í¬ê¸°
                iterations=3,      # ë°˜ë³µ íšŸìˆ˜
                poly_n=5,          # ë‹¤í•­ì‹ í™•ì¥
                poly_sigma=1.2,    # ê°€ìš°ì‹œì•ˆ ì‹œê·¸ë§ˆ
                flags=0
            )

            return flow

        except ImportError:
            st.error("âŒ OpenCVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return np.zeros((frame1.shape[0], frame1.shape[1], 2), dtype=np.float32)
        except Exception as e:
            st.error(f"âŒ Optical Flow ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.zeros((frame1.shape[0], frame1.shape[1], 2), dtype=np.float32)

    def visualize_flow(
        self,
        flow: np.ndarray
    ) -> np.ndarray:
        """
        Optical Flowë¥¼ HSV ìƒ‰ìƒ ê³µê°„ìœ¼ë¡œ ì‹œê°í™”

        Args:
            flow: Optical flow (H, W, 2) - [dx, dy] ëª¨ì…˜ ë²¡í„°

        Returns:
            np.ndarray: RGB ì´ë¯¸ì§€ (H, W, 3)
        """
        try:
            import cv2

            h, w = flow.shape[:2]

            # HSV ì´ë¯¸ì§€ ìƒì„±
            hsv = np.zeros((h, w, 3), dtype=np.uint8)
            hsv[..., 1] = 255  # Saturationì„ ìµœëŒ€ë¡œ

            # Magnitude (í¬ê¸°)ì™€ Angle (ê°ë„) ê³„ì‚°
            magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])

            # Hue: ë°©í–¥ (0-180 ë²”ìœ„)
            hsv[..., 0] = angle * 180 / np.pi / 2

            # Value: í¬ê¸° (ì •ê·œí™”)
            hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)

            # HSV â†’ RGB ë³€í™˜
            rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)

            return rgb

        except ImportError:
            st.error("âŒ OpenCVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)
        except Exception as e:
            st.error(f"âŒ Flow ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)

    def classify_action(
        self,
        video_path: str,
        model_name: str = 'videomae',
        top_k: int = 5
    ) -> List[Tuple[str, float]]:
        """
        ë¹„ë””ì˜¤ì—ì„œ í–‰ë™ ë¶„ë¥˜ ìˆ˜í–‰

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ('videomae', 'timesformer', 'xclip')
            top_k: ë°˜í™˜í•  ìƒìœ„ ì˜ˆì¸¡ ê°œìˆ˜

        Returns:
            List[Tuple[str, float]]: [(í–‰ë™ëª…, í™•ë¥ )] ë¦¬ìŠ¤íŠ¸
        """
        if self.mode == 'transformers':
            return self._classify_with_transformers(video_path, model_name, top_k)
        elif self.mode == 'opencv':
            return self._classify_with_opencv(video_path, top_k)
        else:  # simulation
            return self._simulate_classification(top_k)

    def _classify_with_transformers(
        self,
        video_path: str,
        model_name: str,
        top_k: int
    ) -> List[Tuple[str, float]]:
        """
        HuggingFace Transformersë¡œ í–‰ë™ ë¶„ë¥˜ ìˆ˜í–‰

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            model_name: ëª¨ë¸ ì´ë¦„
            top_k: ìƒìœ„ Kê°œ ì˜ˆì¸¡

        Returns:
            List[Tuple[str, float]]: ì˜ˆì¸¡ ê²°ê³¼
        """
        try:
            from transformers import AutoImageProcessor, AutoModelForVideoClassification
            import torch

            # ëª¨ë¸ ID ê°€ì ¸ì˜¤ê¸°
            if model_name not in MODEL_REGISTRY:
                st.warning(f"âš ï¸ '{model_name}' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. videomaeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                model_name = 'videomae'

            model_id = MODEL_REGISTRY[model_name]

            st.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... ({model_id})")

            # í”„ë ˆì„ ì¶”ì¶œ (VideoMAEëŠ” 16í”„ë ˆì„ ì‚¬ìš©)
            frames = self.extract_frames(
                video_path,
                sample_rate=2,
                max_frames=16,
                target_size=(224, 224)
            )

            if len(frames) == 0:
                return [('error', 0.0)]

            # ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
            processor = AutoImageProcessor.from_pretrained(model_id)
            model = AutoModelForVideoClassification.from_pretrained(model_id).to(self.device)
            model.eval()

            # í”„ë ˆì„ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pil_frames = [Image.fromarray(frame) for frame in frames]

            # ì „ì²˜ë¦¬
            inputs = processor(pil_frames, return_tensors="pt").to(self.device)

            # ì¶”ë¡  (íƒ€ì„ì•„ì›ƒ 30ì´ˆ)
            with st.spinner("ğŸ¬ í–‰ë™ ë¶„ë¥˜ ì¤‘..."):
                with torch.no_grad():
                    outputs = model(**inputs)
                    logits = outputs.logits

            # Softmaxë¡œ í™•ë¥  ë³€í™˜
            probs = torch.nn.functional.softmax(logits, dim=-1)[0]

            # Top-K ì˜ˆì¸¡
            top_probs, top_indices = torch.topk(probs, min(top_k, len(probs)))

            results = []
            for prob, idx in zip(top_probs, top_indices):
                label = model.config.id2label[idx.item()]
                score = prob.item()
                results.append((label, score))

            st.success(f"âœ… ë¶„ë¥˜ ì™„ë£Œ: {results[0][0]} ({results[0][1]:.2%})")
            return results

        except ImportError:
            st.error("âŒ transformers ë˜ëŠ” torchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return [('error', 0.0)]
        except Exception as e:
            st.error(f"âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return [('error', 0.0)]

    def _classify_with_opencv(
        self,
        video_path: str,
        top_k: int
    ) -> List[Tuple[str, float]]:
        """
        OpenCV ê¸°ë°˜ ê°„ë‹¨í•œ í–‰ë™ ë¶„ë¥˜ (ì›€ì§ì„ ê°•ë„ ê¸°ë°˜)

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            top_k: ìƒìœ„ Kê°œ ì˜ˆì¸¡

        Returns:
            List[Tuple[str, float]]: ì˜ˆì¸¡ ê²°ê³¼
        """
        try:
            st.info("â„¹ï¸ OpenCV ëª¨ë“œ: ì›€ì§ì„ ê°•ë„ ê¸°ë°˜ ê°„ë‹¨ ë¶„ë¥˜")

            # í”„ë ˆì„ ì¶”ì¶œ
            frames = self.extract_frames(video_path, sample_rate=10, max_frames=20)

            if len(frames) < 2:
                return [('static', 0.9)]

            # ì—°ì† í”„ë ˆì„ ê°„ ì›€ì§ì„ ê³„ì‚°
            motion_scores = []
            for i in range(len(frames) - 1):
                flow = self.compute_optical_flow(frames[i], frames[i + 1])
                magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
                motion_score = np.mean(magnitude)
                motion_scores.append(motion_score)

            avg_motion = np.mean(motion_scores)

            # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜
            if avg_motion > 10.0:
                return [
                    ('high_activity', 0.8),
                    ('running', 0.6),
                    ('jumping', 0.4),
                    ('walking', 0.2),
                    ('static', 0.1)
                ][:top_k]
            elif avg_motion > 5.0:
                return [
                    ('moderate_activity', 0.8),
                    ('walking', 0.7),
                    ('gesturing', 0.5),
                    ('running', 0.3),
                    ('static', 0.2)
                ][:top_k]
            else:
                return [
                    ('low_activity', 0.9),
                    ('static', 0.7),
                    ('sitting', 0.6),
                    ('standing', 0.4),
                    ('walking', 0.1)
                ][:top_k]

        except Exception as e:
            st.error(f"âŒ OpenCV ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return [('error', 0.0)]

    def _simulate_classification(self, top_k: int) -> List[Tuple[str, float]]:
        """
        ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: ë”ë¯¸ ë¶„ë¥˜ ê²°ê³¼ ë°˜í™˜

        Args:
            top_k: ìƒìœ„ Kê°œ ì˜ˆì¸¡

        Returns:
            List[Tuple[str, float]]: ë”ë¯¸ ì˜ˆì¸¡ ê²°ê³¼
        """
        st.info("â„¹ï¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: ë”ë¯¸ ë¶„ë¥˜ ê²°ê³¼")

        dummy_results = [
            ('walking', 0.85),
            ('running', 0.10),
            ('jumping', 0.03),
            ('sitting', 0.01),
            ('standing', 0.01)
        ]

        return dummy_results[:top_k]

    def count_exercise_reps(
        self,
        video_path: str,
        exercise_type: str = 'pushup'
    ) -> Dict[str, Any]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ìš´ë™ ë°˜ë³µ íšŸìˆ˜ ì¹´ìš´íŠ¸ (MediaPipe Pose ì‚¬ìš©)

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            exercise_type: ìš´ë™ ì¢…ë¥˜ ('pushup', 'squat', 'jumping_jack')

        Returns:
            Dict: {'count': int, 'angle_history': List[float], 'confidence': float}
        """
        # Simulation mode
        if self.mode == 'simulation':
            st.info(f"â„¹ï¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {exercise_type} ì¹´ìš´íŠ¸")
            count = np.random.randint(5, 20)
            angles = np.random.uniform(60, 160, size=count*2).tolist()
            return {
                'count': count,
                'angle_history': angles,
                'confidence': 0.85
            }

        # MediaPipe ì‚¬ìš© ì‹œë„
        try:
            import mediapipe as mp

            st.info(f"ğŸ‹ï¸ MediaPipe Poseë¡œ {exercise_type} ì¹´ìš´íŠ¸ ì¤‘...")

            mp_pose = mp.solutions.pose
            pose = mp_pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )

            # í”„ë ˆì„ ì¶”ì¶œ
            frames = self.extract_frames(video_path, sample_rate=2, max_frames=50)

            angles = []
            count = 0
            prev_angle = None
            in_down_position = False

            for frame in frames:
                # RGBë¡œ ë³€í™˜ (MediaPipeëŠ” RGB ì…ë ¥)
                results = pose.process(frame)

                if results.pose_landmarks:
                    landmarks = results.pose_landmarks.landmark

                    # ìš´ë™ë³„ ê°ë„ ê³„ì‚°
                    if exercise_type == 'pushup':
                        # íŒ”ê¿ˆì¹˜ ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
                        shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
                        elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW]
                        wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]

                        angle = self._calculate_angle(
                            (shoulder.x, shoulder.y),
                            (elbow.x, elbow.y),
                            (wrist.x, wrist.y)
                        )

                    elif exercise_type == 'squat':
                        # ë¬´ë¦ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
                        hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP]
                        knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE]
                        ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]

                        angle = self._calculate_angle(
                            (hip.x, hip.y),
                            (knee.x, knee.y),
                            (ankle.x, ankle.y)
                        )

                    else:  # jumping_jack
                        # íŒ” ê°ë„ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª©)
                        shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]
                        elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW]
                        wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]

                        angle = self._calculate_angle(
                            (shoulder.x, shoulder.y),
                            (elbow.x, elbow.y),
                            (wrist.x, wrist.y)
                        )

                    angles.append(angle)

                    # ë°˜ë³µ ì¹´ìš´íŠ¸ ë¡œì§ (ê°ë„ ë³€í™” ê°ì§€)
                    if prev_angle is not None:
                        # Down position ê°ì§€ (ê°ë„ê°€ ì‘ì•„ì§)
                        if angle < 100 and not in_down_position:
                            in_down_position = True
                        # Up position ê°ì§€ (ê°ë„ê°€ ì»¤ì§) â†’ 1íšŒ ì¹´ìš´íŠ¸
                        elif angle > 140 and in_down_position:
                            count += 1
                            in_down_position = False

                    prev_angle = angle

            pose.close()

            st.success(f"âœ… {exercise_type} {count}íšŒ ì¹´ìš´íŠ¸ ì™„ë£Œ")

            return {
                'count': count,
                'angle_history': angles,
                'confidence': 0.9 if len(angles) > 10 else 0.5
            }

        except ImportError:
            st.warning("âš ï¸ MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            count = np.random.randint(5, 15)
            angles = np.random.uniform(60, 160, size=count*2).tolist()
            return {
                'count': count,
                'angle_history': angles,
                'confidence': 0.5
            }
        except Exception as e:
            st.error(f"âŒ ìš´ë™ ì¹´ìš´íŠ¸ ì‹¤íŒ¨: {e}")
            return {'count': 0, 'angle_history': [], 'confidence': 0.0}

    def _calculate_angle(
        self,
        point1: Tuple[float, float],
        point2: Tuple[float, float],
        point3: Tuple[float, float]
    ) -> float:
        """
        3ê°œ ì ìœ¼ë¡œ ê°ë„ ê³„ì‚°

        Args:
            point1: ì²« ë²ˆì§¸ ì  (x, y)
            point2: ì¤‘ê°„ ì  (x, y) - ê°ë„ì˜ ê¼­ì§€ì 
            point3: ì„¸ ë²ˆì§¸ ì  (x, y)

        Returns:
            float: ê°ë„ (ë„ ë‹¨ìœ„, 0-180)
        """
        # ë²¡í„° ê³„ì‚°
        vector1 = np.array([point1[0] - point2[0], point1[1] - point2[1]])
        vector2 = np.array([point3[0] - point2[0], point3[1] - point2[1]])

        # ë‚´ì ê³¼ normìœ¼ë¡œ ê°ë„ ê³„ì‚°
        cosine_angle = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2) + 1e-6)
        angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))

        return np.degrees(angle)

    def get_video_info(self, video_path: str) -> Dict[str, Any]:
        """
        ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            Dict: {'fps': float, 'duration': float, 'resolution': Tuple[int, int], 'frame_count': int}
        """
        if self.mode == 'simulation':
            return {
                'fps': 30.0,
                'duration': 10.0,
                'resolution': (1920, 1080),
                'frame_count': 300
            }

        try:
            import cv2

            cap = cv2.VideoCapture(video_path)

            if not cap.isOpened():
                raise ValueError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = frame_count / fps if fps > 0 else 0.0

            cap.release()

            return {
                'fps': fps,
                'duration': duration,
                'resolution': (width, height),
                'frame_count': frame_count
            }

        except Exception as e:
            st.error(f"âŒ ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'fps': 0.0,
                'duration': 0.0,
                'resolution': (0, 0),
                'frame_count': 0
            }

    def save_temp_video(self, uploaded_bytes: bytes) -> Optional[str]:
        """
        ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥

        Args:
            uploaded_bytes: ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ ë°”ì´íŠ¸

        Returns:
            Optional[str]: ì„ì‹œ íŒŒì¼ ê²½ë¡œ, ì‹¤íŒ¨ ì‹œ None
        """
        try:
            import tempfile

            # ì„ì‹œ íŒŒì¼ ìƒì„± (ìë™ ì‚­ì œí•˜ì§€ ì•ŠìŒ)
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                tmp_file.write(uploaded_bytes)
                tmp_path = tmp_file.name

            st.success(f"âœ… ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {os.path.basename(tmp_path)}")
            return tmp_path

        except Exception as e:
            st.error(f"âŒ ë¹„ë””ì˜¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


@st.cache_resource
def get_video_helper() -> VideoHelper:
    """
    ìºì‹œëœ VideoHelper ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)

    Returns:
        VideoHelper: ìºì‹œëœ í—¬í¼ ì¸ìŠ¤í„´ìŠ¤
    """
    return VideoHelper()
