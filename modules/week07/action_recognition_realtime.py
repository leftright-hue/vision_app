"""
Week 7: Real-time Action Recognition Module
MediaPipe (Open Source) and Google Video Intelligence API (Cloud) Implementation
"""

import streamlit as st
import numpy as np
import cv2
import tempfile
import os
import json
import time
from typing import List, Dict, Any, Optional, Tuple
import io
import base64

# Core imports
from core.base_processor import BaseImageProcessor


class RealtimeActionRecognitionModule(BaseImageProcessor):
    """Week 7: ì‹¤ì‹œê°„ í–‰ë™ì¸ì‹ ëª¨ë“ˆ"""

    def __init__(self):
        super().__init__()
        self.name = "Week 7: Realtime Action Recognition"
        self.mediapipe_available = self._check_mediapipe()
        self.google_cloud_available = self._check_google_cloud()

    def _check_mediapipe(self) -> bool:
        """MediaPipe ì„¤ì¹˜ í™•ì¸"""
        try:
            import mediapipe as mp
            return True
        except ImportError:
            return False

    def _check_google_cloud(self) -> bool:
        """Google Cloud Video Intelligence API ì„¤ì¹˜ í™•ì¸"""
        try:
            from google.cloud import videointelligence
            return True
        except ImportError:
            return False

    def render(self):
        """ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜"""
        st.title("ğŸ¬ Week 7: ì‹¤ì‹œê°„ í–‰ë™ì¸ì‹ (Real-time Action Recognition)")

        st.markdown("""
        ## í•™ìŠµ ëª©í‘œ
        - **Open Source**: MediaPipeë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ í–‰ë™ ì¸ì‹
        - **Cloud API**: Google Video Intelligence APIë¥¼ í™œìš©í•œ ë¹„ë””ì˜¤ ë¶„ì„
        - **ì‹¤ìŠµ**: ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì„ í†µí•œ í–‰ë™ ì¸ì‹ êµ¬í˜„
        """)

        # í™˜ê²½ ì²´í¬
        self._check_environment()

        # 2ê°œ íƒ­ êµ¬ì„±
        tabs = st.tabs([
            "ğŸ”§ Open Source: MediaPipe",
            "â˜ï¸ Cloud: Google Video Intelligence"
        ])

        with tabs[0]:
            self.render_mediapipe_tab()

        with tabs[1]:
            self.render_google_cloud_tab()

    def _check_environment(self):
        """í™˜ê²½ ì²´í¬ ë° ì„¤ì •"""
        with st.expander("ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸", expanded=False):
            st.markdown("""
            ### í•„ìš”í•œ íŒ¨í‚¤ì§€

            **MediaPipe (Open Source)**:
            ```bash
            pip install mediapipe opencv-python numpy
            ```

            **Google Cloud Video Intelligence**:
            ```bash
            pip install google-cloud-videointelligence
            # Google Cloud ì¸ì¦ ì„¤ì • í•„ìš”
            ```
            """)

            col1, col2 = st.columns(2)

            with col1:
                st.subheader("MediaPipe Status")
                if self.mediapipe_available:
                    st.success("âœ… MediaPipe ì„¤ì¹˜ë¨")
                else:
                    st.error("âŒ MediaPipe ë¯¸ì„¤ì¹˜")

            with col2:
                st.subheader("Google Cloud Status")
                if self.google_cloud_available:
                    st.success("âœ… Google Cloud SDK ì„¤ì¹˜ë¨")
                else:
                    st.warning("âš ï¸ Google Cloud SDK ë¯¸ì„¤ì¹˜")

    # ==================== MediaPipe Tab ====================

    def render_mediapipe_tab(self):
        """MediaPipeë¥¼ ì´ìš©í•œ í–‰ë™ ì¸ì‹"""
        st.header("ğŸ”§ Open Source: MediaPipe Action Recognition")

        st.markdown("""
        ### MediaPipeë€?
        Googleì—ì„œ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ë¡œ, ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ë¶„ì„ì„ ìœ„í•œ ë‹¤ì–‘í•œ ML ì†”ë£¨ì…˜ ì œê³µ

        ### ì£¼ìš” ê¸°ëŠ¥
        - **Pose Detection**: 33ê°œ ì‹ ì²´ ëœë“œë§ˆí¬ ì¶”ì 
        - **Hand Tracking**: 21ê°œ ì† ëœë“œë§ˆí¬ ì¶”ì 
        - **Face Detection**: 468ê°œ ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì 
        - **Holistic**: Pose + Hand + Face í†µí•© ì¶”ì 

        ### í–‰ë™ ì¸ì‹ ì‘ìš©
        - ìš´ë™ ìì„¸ ë¶„ì„ (ìŠ¤ì¿¼íŠ¸, í‘¸ì‹œì—… ì¹´ìš´íŒ…)
        - ì œìŠ¤ì²˜ ì¸ì‹ (ìˆ˜í™”, ì†ë™ì‘ ëª…ë ¹)
        - ë„˜ì–´ì§ ê°ì§€ (ì•ˆì „ ëª¨ë‹ˆí„°ë§)
        - ìŠ¤í¬ì¸  ë™ì‘ ë¶„ì„
        """)

        if not self.mediapipe_available:
            st.error("""
            âŒ MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!

            ì„¤ì¹˜ ë°©ë²•:
            ```bash
            pip install mediapipe opencv-python
            ```
            """)
            return

        # MediaPipe ëª¨ë“œ ì„ íƒ
        detection_mode = st.selectbox(
            "ê²€ì¶œ ëª¨ë“œ ì„ íƒ",
            ["Pose Detection (ì „ì‹  í¬ì¦ˆ)", "Hand Tracking (ì† ì œìŠ¤ì²˜)", "Holistic (í†µí•©)"]
        )

        # í–‰ë™ ìœ í˜• ì„ íƒ
        if detection_mode == "Pose Detection (ì „ì‹  í¬ì¦ˆ)":
            action_type = st.selectbox(
                "ì¸ì‹í•  í–‰ë™",
                ["ìš´ë™ ì¹´ìš´íŒ… (ìŠ¤ì¿¼íŠ¸/í‘¸ì‹œì—…)", "ë„˜ì–´ì§ ê°ì§€", "ìš”ê°€ ìì„¸ ì¸ì‹"]
            )
        elif detection_mode == "Hand Tracking (ì† ì œìŠ¤ì²˜)":
            action_type = st.selectbox(
                "ì¸ì‹í•  ì œìŠ¤ì²˜",
                ["ê¸°ë³¸ ì œìŠ¤ì²˜ (ì£¼ë¨¹/ê°€ìœ„/ë°”ìœ„)", "ìˆ«ì ì¹´ìš´íŒ… (1-5)", "ë°©í–¥ ì§€ì‹œ"]
            )
        else:
            action_type = "í†µí•© ë¶„ì„"

        # ë¹„ë””ì˜¤ ì…ë ¥
        st.subheader("ğŸ“¹ ë¹„ë””ì˜¤ ì…ë ¥")
        input_source = st.radio(
            "ì…ë ¥ ì†ŒìŠ¤ ì„ íƒ",
            ["íŒŒì¼ ì—…ë¡œë“œ", "ìƒ˜í”Œ ë¹„ë””ì˜¤", "ì›¹ìº  (ë³„ë„ ì‹¤í–‰)"]
        )

        if input_source == "íŒŒì¼ ì—…ë¡œë“œ":
            uploaded_file = st.file_uploader(
                "ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ",
                type=['mp4', 'avi', 'mov'],
                key="mediapipe_upload"
            )

            if uploaded_file:
                # ì„ì‹œ íŒŒì¼ ì €ì¥
                tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
                tfile.write(uploaded_file.read())
                video_path = tfile.name

                # ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸°
                st.video(uploaded_file)

                if st.button("ğŸ¬ MediaPipe ë¶„ì„ ì‹œì‘", type="primary", key="mp_analyze"):
                    self._process_with_mediapipe(video_path, detection_mode, action_type)

        elif input_source == "ìƒ˜í”Œ ë¹„ë””ì˜¤":
            st.info("ìƒ˜í”Œ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            if st.button("ğŸ¥ ìƒ˜í”Œ ë¹„ë””ì˜¤ ìƒì„± ë° ë¶„ì„", key="mp_sample"):
                video_path = self._create_sample_video()
                if video_path:
                    self._process_with_mediapipe(video_path, detection_mode, action_type)

        else:
            st.markdown("""
            ### ì›¹ìº  ì‹¤ì‹œê°„ ì²˜ë¦¬

            ì›¹ìº ì„ ì´ìš©í•œ ì‹¤ì‹œê°„ ì²˜ë¦¬ëŠ” ë³„ë„ Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:

            ```python
            import cv2
            import mediapipe as mp

            mp_pose = mp.solutions.pose
            pose = mp_pose.Pose()
            mp_drawing = mp.solutions.drawing_utils

            cap = cv2.VideoCapture(0)

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                # MediaPipe ì²˜ë¦¬
                results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

                # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                if results.pose_landmarks:
                    mp_drawing.draw_landmarks(
                        frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)

                cv2.imshow('MediaPipe Pose', frame)

                if cv2.waitKey(1) & 0xFF == 27:  # ESC
                    break

            cap.release()
            cv2.destroyAllWindows()
            ```
            """)

    def _process_with_mediapipe(self, video_path: str, detection_mode: str, action_type: str):
        """MediaPipeë¡œ ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        import mediapipe as mp
        import cv2

        st.info(f"ğŸ”„ ì²˜ë¦¬ ì¤‘... ëª¨ë“œ: {detection_mode}, í–‰ë™: {action_type}")

        # MediaPipe ì´ˆê¸°í™”
        mp_drawing = mp.solutions.drawing_utils

        if "Pose" in detection_mode:
            mp_pose = mp.solutions.pose
            detector = mp_pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            connections = mp_pose.POSE_CONNECTIONS
        elif "Hand" in detection_mode:
            mp_hands = mp.solutions.hands
            detector = mp_hands.Hands(
                static_image_mode=False,
                max_num_hands=2,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            connections = mp_hands.HAND_CONNECTIONS
        else:
            mp_holistic = mp.solutions.holistic
            detector = mp_holistic.Holistic(
                static_image_mode=False,
                model_complexity=1,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )
            connections = None

        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        cap = cv2.VideoCapture(video_path)

        # í”„ë ˆì„ ìƒ˜í”Œë§ (ë§¤ 5í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬)
        frame_count = 0
        processed_frames = []
        landmarks_history = []
        action_counts = {"count": 0, "state": "neutral"}

        progress_bar = st.progress(0)
        status_text = st.empty()

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # í”„ë ˆì„ ìƒ˜í”Œë§
            if frame_count % 5 != 0:
                continue

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = min(frame_count / total_frames, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"í”„ë ˆì„ {frame_count}/{total_frames} ì²˜ë¦¬ ì¤‘...")

            # RGB ë³€í™˜
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # MediaPipe ì²˜ë¦¬
            results = detector.process(rgb_frame)

            # ê²°ê³¼ ì‹œê°í™”
            annotated_frame = frame.copy()

            if "Pose" in detection_mode and results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    annotated_frame, results.pose_landmarks, connections)
                landmarks_history.append(results.pose_landmarks)

                # ìš´ë™ ì¹´ìš´íŒ… ë¡œì§
                if "ìš´ë™" in action_type:
                    action_counts = self._count_exercise(
                        results.pose_landmarks, action_counts)

            elif "Hand" in detection_mode and results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        annotated_frame, hand_landmarks, connections)
                landmarks_history.append(hand_landmarks)

            elif "Holistic" in detection_mode:
                if results.pose_landmarks:
                    mp_drawing.draw_landmarks(
                        annotated_frame, results.pose_landmarks,
                        mp.solutions.holistic.POSE_CONNECTIONS)
                if results.left_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        annotated_frame, results.left_hand_landmarks,
                        mp.solutions.holistic.HAND_CONNECTIONS)
                if results.right_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        annotated_frame, results.right_hand_landmarks,
                        mp.solutions.holistic.HAND_CONNECTIONS)

            # ì¼ë¶€ í”„ë ˆì„ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if len(processed_frames) < 10:
                processed_frames.append(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))

        cap.release()
        detector.close()

        # ê²°ê³¼ í‘œì‹œ
        st.success("âœ… MediaPipe ë¶„ì„ ì™„ë£Œ!")

        # ë©”íŠ¸ë¦­ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì²˜ë¦¬ëœ í”„ë ˆì„", f"{frame_count}")
        with col2:
            st.metric("ê²€ì¶œëœ ëœë“œë§ˆí¬", f"{len(landmarks_history)}")
        with col3:
            if "ìš´ë™" in action_type:
                st.metric("ìš´ë™ íšŸìˆ˜", f"{action_counts['count']}íšŒ")

        # ì²˜ë¦¬ëœ í”„ë ˆì„ í‘œì‹œ
        if processed_frames:
            st.subheader("ğŸ“¸ ì²˜ë¦¬ëœ í”„ë ˆì„ ìƒ˜í”Œ")
            cols = st.columns(3)
            for i, frame in enumerate(processed_frames[:6]):
                with cols[i % 3]:
                    st.image(frame, caption=f"Frame {i+1}", use_container_width=True)

        # ìƒì„¸ ë¶„ì„ ê²°ê³¼
        with st.expander("ğŸ“Š ìƒì„¸ ë¶„ì„ ê²°ê³¼"):
            st.json({
                "detection_mode": detection_mode,
                "action_type": action_type,
                "total_frames": total_frames,
                "processed_frames": frame_count // 5,
                "landmarks_detected": len(landmarks_history),
                "action_counts": action_counts if "ìš´ë™" in action_type else "N/A"
            })

    def _count_exercise(self, landmarks, counts):
        """ìš´ë™ ì¹´ìš´íŒ… ë¡œì§ (ê°„ë‹¨í•œ ì˜ˆì‹œ)"""
        # ë¬´ë¦ ê°ë„ ê³„ì‚° (ìŠ¤ì¿¼íŠ¸ ì˜ˆì‹œ)
        import math

        def calculate_angle(a, b, c):
            """ì„¸ ì  ì‚¬ì´ì˜ ê°ë„ ê³„ì‚°"""
            ang = math.degrees(
                math.atan2(c.y - b.y, c.x - b.x) -
                math.atan2(a.y - b.y, a.x - b.x)
            )
            return ang + 360 if ang < 0 else ang

        # ì™¼ìª½ ë¬´ë¦ ê°ë„ (ì—‰ë©ì´-ë¬´ë¦-ë°œëª©)
        hip = landmarks.landmark[23]  # LEFT_HIP
        knee = landmarks.landmark[25]  # LEFT_KNEE
        ankle = landmarks.landmark[27]  # LEFT_ANKLE

        angle = calculate_angle(hip, knee, ankle)

        # ìŠ¤ì¿¼íŠ¸ ì¹´ìš´íŒ… ë¡œì§
        if angle < 90:  # ë¬´ë¦ì´ 90ë„ ì´í•˜ë¡œ êµ½í˜€ì§
            if counts["state"] == "up":
                counts["count"] += 1
                counts["state"] = "down"
        elif angle > 160:  # ë¬´ë¦ì´ ê±°ì˜ í´ì§
            counts["state"] = "up"

        return counts

    # ==================== Google Cloud Tab ====================

    def render_google_cloud_tab(self):
        """Google Video Intelligence APIë¥¼ ì´ìš©í•œ í–‰ë™ ì¸ì‹"""
        st.header("â˜ï¸ Cloud: Google Video Intelligence API")

        st.markdown("""
        ### Google Video Intelligence APIë€?
        Google Cloudì˜ ë¹„ë””ì˜¤ ë¶„ì„ ì„œë¹„ìŠ¤ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ ë¹„ë””ì˜¤ ì½˜í…ì¸ ë¥¼ ìë™ìœ¼ë¡œ ë¶„ì„

        ### ì£¼ìš” ê¸°ëŠ¥
        - **Label Detection**: ë¹„ë””ì˜¤ ë‚´ ê°ì²´, ì¥ì†Œ, í™œë™ ê°ì§€
        - **Shot Change Detection**: ì¥ë©´ ì „í™˜ ê°ì§€
        - **Explicit Content Detection**: ë¶€ì ì ˆí•œ ì½˜í…ì¸  ê°ì§€
        - **Speech Transcription**: ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜
        - **Object Tracking**: ê°ì²´ ì¶”ì 
        - **Face Detection**: ì–¼êµ´ ê°ì§€ ë° ê°ì • ë¶„ì„
        - **Person Detection**: ì‚¬ëŒ ê°ì§€ ë° ì¶”ì 
        - **Logo Recognition**: ë¡œê³  ì¸ì‹

        ### í–‰ë™ ì¸ì‹ ê´€ë ¨ ê¸°ëŠ¥
        - 400+ ê°€ì§€ ì‚¬ì „ ì •ì˜ëœ í–‰ë™ ë ˆì´ë¸”
        - ì‹œê°„ë³„ í–‰ë™ êµ¬ê°„ ê°ì§€
        - ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ
        """)

        # API ì„¤ì • ì•ˆë‚´
        with st.expander("ğŸ”‘ API ì„¤ì • ê°€ì´ë“œ", expanded=False):
            st.markdown("""
            ### 1. Google Cloud í”„ë¡œì íŠ¸ ì„¤ì •
            1. [Google Cloud Console](https://console.cloud.google.com) ì ‘ì†
            2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„± ë˜ëŠ” ê¸°ì¡´ í”„ë¡œì íŠ¸ ì„ íƒ
            3. Video Intelligence API í™œì„±í™”

            ### 2. ì¸ì¦ ì„¤ì •
            ```bash
            # ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ìƒì„± í›„
            export GOOGLE_APPLICATION_CREDENTIALS="path/to/key.json"

            # ë˜ëŠ” Python ì½”ë“œì—ì„œ
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/key.json'
            ```

            ### 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜
            ```bash
            pip install google-cloud-videointelligence
            ```

            ### 4. ë¬´ë£Œ í•œë„
            - ë§¤ì›” ì²˜ìŒ 1000ë¶„ ë¬´ë£Œ
            - ì´í›„ ë¶„ë‹¹ $0.10 ~ $0.15
            """)

        # API í‚¤ ì…ë ¥
        st.subheader("ğŸ”‘ API ì¸ì¦")

        auth_method = st.radio(
            "ì¸ì¦ ë°©ë²•",
            ["í™˜ê²½ ë³€ìˆ˜ (ê¶Œì¥)", "ì§ì ‘ ì…ë ¥", "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ"]
        )

        api_ready = False

        if auth_method == "í™˜ê²½ ë³€ìˆ˜ (ê¶Œì¥)":
            if os.environ.get('GOOGLE_APPLICATION_CREDENTIALS'):
                st.success("âœ… í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¸ì¦ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                api_ready = True
            else:
                st.warning("âš ï¸ GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        elif auth_method == "ì§ì ‘ ì…ë ¥":
            api_key_file = st.file_uploader(
                "ì„œë¹„ìŠ¤ ê³„ì • í‚¤ JSON íŒŒì¼ ì—…ë¡œë“œ",
                type=['json'],
                key="gcp_key"
            )
            if api_key_file:
                # ì„ì‹œë¡œ ì €ì¥
                key_path = tempfile.NamedTemporaryFile(delete=False, suffix='.json')
                key_path.write(api_key_file.read())
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_path.name
                st.success("âœ… API í‚¤ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                api_ready = True

        else:  # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
            st.info("ğŸ“ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: ì‹¤ì œ API í˜¸ì¶œ ì—†ì´ ì˜ˆì‹œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
            api_ready = True

        # ë¶„ì„ ê¸°ëŠ¥ ì„ íƒ
        st.subheader("ğŸ¯ ë¶„ì„ ê¸°ëŠ¥ ì„ íƒ")

        features = st.multiselect(
            "ë¶„ì„í•  ê¸°ëŠ¥ ì„ íƒ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
            [
                "LABEL_DETECTION (í–‰ë™/ê°ì²´ ë ˆì´ë¸”)",
                "SHOT_CHANGE_DETECTION (ì¥ë©´ ì „í™˜)",
                "EXPLICIT_CONTENT_DETECTION (ë¶€ì ì ˆ ì½˜í…ì¸ )",
                "PERSON_DETECTION (ì‚¬ëŒ ê°ì§€)",
                "FACE_DETECTION (ì–¼êµ´ ê°ì§€)",
                "OBJECT_TRACKING (ê°ì²´ ì¶”ì )"
            ],
            default=["LABEL_DETECTION (í–‰ë™/ê°ì²´ ë ˆì´ë¸”)"]
        )

        # ë¹„ë””ì˜¤ ì…ë ¥
        st.subheader("ğŸ“¹ ë¹„ë””ì˜¤ ì…ë ¥")

        input_method = st.radio(
            "ì…ë ¥ ë°©ë²•",
            ["íŒŒì¼ ì—…ë¡œë“œ", "Google Cloud Storage URI", "YouTube URL (ì œí•œì )"]
        )

        video_input = None

        if input_method == "íŒŒì¼ ì—…ë¡œë“œ":
            uploaded_file = st.file_uploader(
                "ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ",
                type=['mp4', 'avi', 'mov'],
                key="gcp_upload"
            )
            if uploaded_file:
                st.video(uploaded_file)
                video_input = uploaded_file

        elif input_method == "Google Cloud Storage URI":
            gcs_uri = st.text_input(
                "GCS URI ì…ë ¥",
                placeholder="gs://bucket-name/video-file.mp4"
            )
            if gcs_uri:
                video_input = gcs_uri

        else:
            st.info("YouTube URL ë¶„ì„ì€ ì œí•œì ì…ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ í›„ ì—…ë¡œë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")

        # ë¶„ì„ ì‹œì‘
        if video_input and api_ready and features:
            if st.button("ğŸš€ Google Video Intelligence ë¶„ì„ ì‹œì‘", type="primary", key="gcp_analyze"):
                if auth_method == "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ":
                    self._simulate_google_analysis(features)
                else:
                    self._process_with_google_api(video_input, features, input_method)

    def _simulate_google_analysis(self, features):
        """Google Video Intelligence API ì‹œë®¬ë ˆì´ì…˜"""
        st.info("ğŸ”„ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë¶„ì„ ì¤‘...")

        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
        progress_bar = st.progress(0)
        for i in range(100):
            progress_bar.progress(i + 1)
            time.sleep(0.02)

        st.success("âœ… ë¶„ì„ ì™„ë£Œ (ì‹œë®¬ë ˆì´ì…˜)")

        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
        if "LABEL_DETECTION" in str(features):
            st.subheader("ğŸ·ï¸ ë ˆì´ë¸” ê°ì§€ ê²°ê³¼")

            # ìƒ˜í”Œ ë ˆì´ë¸”
            labels = [
                {"name": "walking", "confidence": 0.92, "segments": [(0, 5), (10, 15)]},
                {"name": "running", "confidence": 0.87, "segments": [(5, 10)]},
                {"name": "jumping", "confidence": 0.75, "segments": [(15, 18)]},
                {"name": "person", "confidence": 0.95, "segments": [(0, 20)]},
                {"name": "outdoor", "confidence": 0.88, "segments": [(0, 20)]}
            ]

            for label in labels:
                with st.expander(f"{label['name']} (ì‹ ë¢°ë„: {label['confidence']:.1%})"):
                    st.write(f"ê°ì§€ëœ êµ¬ê°„: {label['segments']}")

                    # íƒ€ì„ë¼ì¸ ì‹œê°í™”
                    import matplotlib.pyplot as plt
                    fig, ax = plt.subplots(figsize=(10, 1))
                    for seg in label['segments']:
                        ax.barh(0, seg[1] - seg[0], left=seg[0], height=0.5,
                               color='blue', alpha=0.6)
                    ax.set_xlim(0, 20)
                    ax.set_ylim(-0.5, 0.5)
                    ax.set_xlabel("ì‹œê°„ (ì´ˆ)")
                    ax.set_yticks([])
                    ax.set_title(f"{label['name']} íƒ€ì„ë¼ì¸")
                    st.pyplot(fig)
                    plt.close()

        if "SHOT_CHANGE_DETECTION" in str(features):
            st.subheader("ğŸ¬ ì¥ë©´ ì „í™˜ ê°ì§€")
            st.write("ê°ì§€ëœ ì¥ë©´ ì „í™˜ ì‹œì : 3.5ì´ˆ, 7.2ì´ˆ, 12.1ì´ˆ, 16.8ì´ˆ")

        if "PERSON_DETECTION" in str(features):
            st.subheader("ğŸ‘¤ ì‚¬ëŒ ê°ì§€")
            st.write("ê°ì§€ëœ ì‚¬ëŒ ìˆ˜: 2ëª…")
            st.write("ì¶”ì  ID: Person_1 (0-20ì´ˆ), Person_2 (5-15ì´ˆ)")

    def _process_with_google_api(self, video_input, features, input_method):
        """ì‹¤ì œ Google Video Intelligence API í˜¸ì¶œ"""
        if not self.google_cloud_available:
            st.error("Google Cloud Video Intelligence íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        try:
            from google.cloud import videointelligence

            st.info("ğŸ”„ Google Video Intelligence API í˜¸ì¶œ ì¤‘...")

            # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            video_client = videointelligence.VideoIntelligenceServiceClient()

            # ê¸°ëŠ¥ ë§¤í•‘
            feature_map = {
                "LABEL_DETECTION": videointelligence.Feature.LABEL_DETECTION,
                "SHOT_CHANGE_DETECTION": videointelligence.Feature.SHOT_CHANGE_DETECTION,
                "EXPLICIT_CONTENT_DETECTION": videointelligence.Feature.EXPLICIT_CONTENT_DETECTION,
                "PERSON_DETECTION": videointelligence.Feature.PERSON_DETECTION,
                "FACE_DETECTION": videointelligence.Feature.FACE_DETECTION,
                "OBJECT_TRACKING": videointelligence.Feature.OBJECT_TRACKING
            }

            selected_features = []
            for f in features:
                key = f.split(" ")[0]
                if key in feature_map:
                    selected_features.append(feature_map[key])

            # ì…ë ¥ ì¤€ë¹„
            if input_method == "Google Cloud Storage URI":
                input_uri = video_input
            else:
                # íŒŒì¼ ì—…ë¡œë“œì˜ ê²½ìš° ë°”ì´íŠ¸ë¡œ ë³€í™˜
                input_content = video_input.read()

            # API í˜¸ì¶œ
            if input_method == "Google Cloud Storage URI":
                operation = video_client.annotate_video(
                    request={
                        "features": selected_features,
                        "input_uri": input_uri
                    }
                )
            else:
                operation = video_client.annotate_video(
                    request={
                        "features": selected_features,
                        "input_content": input_content
                    }
                )

            st.info("â³ ë¶„ì„ ì¤‘... (1-2ë¶„ ì†Œìš”)")

            # ê²°ê³¼ ëŒ€ê¸°
            result = operation.result(timeout=180)

            st.success("âœ… ë¶„ì„ ì™„ë£Œ!")

            # ê²°ê³¼ íŒŒì‹± ë° í‘œì‹œ
            self._display_google_results(result)

        except Exception as e:
            st.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            st.info("API í‚¤ ì„¤ì •ê³¼ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")

    def _display_google_results(self, result):
        """Google API ê²°ê³¼ í‘œì‹œ"""
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì£¼ì„
        for annotation in result.annotation_results:

            # ë ˆì´ë¸” ê°ì§€ ê²°ê³¼
            if annotation.segment_label_annotations:
                st.subheader("ğŸ·ï¸ ê°ì§€ëœ í–‰ë™/ê°ì²´ ë ˆì´ë¸”")

                for label in annotation.segment_label_annotations[:10]:  # ìƒìœ„ 10ê°œ
                    entity = label.entity
                    confidence = label.segments[0].confidence if label.segments else 0

                    with st.expander(f"{entity.description} (ì‹ ë¢°ë„: {confidence:.1%})"):
                        st.write(f"Entity ID: {entity.entity_id}")

                        # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
                        for segment in label.segments:
                            start_time = segment.segment.start_time_offset.total_seconds()
                            end_time = segment.segment.end_time_offset.total_seconds()
                            st.write(f"ì‹œê°„: {start_time:.1f}ì´ˆ - {end_time:.1f}ì´ˆ")

            # ì¥ë©´ ì „í™˜ ê°ì§€
            if annotation.shot_annotations:
                st.subheader("ğŸ¬ ì¥ë©´ ì „í™˜")
                shot_times = []
                for shot in annotation.shot_annotations:
                    start = shot.start_time_offset.total_seconds()
                    end = shot.end_time_offset.total_seconds()
                    shot_times.append((start, end))
                st.write(f"ì´ {len(shot_times)}ê°œ ì¥ë©´ ê°ì§€")
                st.write(shot_times[:5])  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ

            # ëª…ì‹œì  ì½˜í…ì¸  ê°ì§€
            if annotation.explicit_annotation:
                st.subheader("ğŸ” ëª…ì‹œì  ì½˜í…ì¸ ")
                for frame in annotation.explicit_annotation.frames[:5]:
                    time = frame.time_offset.total_seconds()
                    level = frame.pornography_likelihood.name
                    st.write(f"{time:.1f}ì´ˆ: {level}")

    def _create_sample_video(self):
        """ê°„ë‹¨í•œ ìƒ˜í”Œ ë¹„ë””ì˜¤ ìƒì„±"""
        try:
            import cv2
            import numpy as np

            # ì„ì‹œ íŒŒì¼ ìƒì„±
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
            temp_path = temp_file.name

            # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_path, fourcc, 20.0, (640, 480))

            # ê°„ë‹¨í•œ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± (ì›€ì§ì´ëŠ” ì›)
            for i in range(100):
                frame = np.ones((480, 640, 3), dtype=np.uint8) * 255

                # ì›€ì§ì´ëŠ” ì› ê·¸ë¦¬ê¸°
                x = int(320 + 200 * np.sin(i * 0.1))
                y = int(240 + 100 * np.cos(i * 0.1))
                cv2.circle(frame, (x, y), 30, (255, 0, 0), -1)

                # í…ìŠ¤íŠ¸ ì¶”ê°€
                cv2.putText(frame, f"Frame {i}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)

                out.write(frame)

            out.release()
            st.success(f"âœ… ìƒ˜í”Œ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {temp_path}")
            return temp_path

        except Exception as e:
            st.error(f"ìƒ˜í”Œ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None


# Streamlit ì•± ì‹¤í–‰ì„ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜
def main():
    module = RealtimeActionRecognitionModule()
    module.render()


if __name__ == "__main__":
    main()