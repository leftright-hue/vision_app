"""
Week 7: í–‰ë™ì¸ì‹ (Action Recognition)
"""

import streamlit as st
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional, Dict, Any
import io
import os

from core.base_processor import BaseImageProcessor
from .action_helpers import get_video_helper


class ActionRecognitionModule(BaseImageProcessor):
    """Week 7: í–‰ë™ì¸ì‹ ëª¨ë“ˆ"""

    def __init__(self):
        super().__init__()
        self.name = "Week 7: Action Recognition"

    def render(self):
        """ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜"""
        st.title("ğŸ¬ Week 7: í–‰ë™ì¸ì‹ (Action Recognition)")

        st.markdown("""
        ## í•™ìŠµ ëª©í‘œ
        - **ì´ë¡ **: í–‰ë™ì¸ì‹ ê°œë…, 3D CNN, Two-Stream, Transformer ì•„í‚¤í…ì²˜ ì´í•´
        - **ì‹¤ìŠµ**: ë¹„ë””ì˜¤ ì²˜ë¦¬, Optical Flow, HuggingFace ëª¨ë¸ í™œìš©
        - **ì‘ìš©**: ì‹¤ì‹œê°„ í–‰ë™ ì¸ì‹, ìš´ë™ ì¹´ìš´í„° ì œì‘
        - **ì‹¤ì „**: MediaPipeì™€ Google Video Intelligence API í™œìš©
        """)

        # í™˜ê²½ ì²´í¬
        self._check_environment()

        # 7ê°œ íƒ­ êµ¬ì„±
        tabs = st.tabs([
            "ğŸ“š ê°œë… ì†Œê°œ",
            "ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ê¸°ì´ˆ",
            "ğŸ¤– ì‚¬ì „í›ˆë ¨ ëª¨ë¸",
            "ğŸ“¹ ì‹¤ì‹œê°„ ì¸ì‹",
            "ğŸ’¼ ì‹¤ì „ ì‘ìš©",
            "ğŸ”§ MediaPipe (Open Source)",
            "â˜ï¸ Google Video Intelligence (Cloud)"
        ])

        with tabs[0]:
            self.render_theory()

        with tabs[1]:
            self.render_video_basics()

        with tabs[2]:
            self.render_pretrained_models()

        with tabs[3]:
            self.render_realtime()

        with tabs[4]:
            self.render_applications()

        with tabs[5]:
            self.render_mediapipe_realtime()

        with tabs[6]:
            self.render_google_video_intelligence()

    def _check_environment(self):
        """í™˜ê²½ ì²´í¬ ë° ì„¤ì •"""
        with st.expander("ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸", expanded=False):
            st.markdown("""
            ### í•„ìš”í•œ íŒ¨í‚¤ì§€
            - `transformers`: HuggingFace ëª¨ë¸ (VideoMAE, TimeSformer)
            - `torch`: PyTorch ë°±ì—”ë“œ
            - `opencv-python`: ë¹„ë””ì˜¤ ì²˜ë¦¬, Optical Flow
            - `mediapipe`: ìš´ë™ ì¹´ìš´í„° (ì„ íƒì )

            ### 3-Tier Fallback ì „ëµ
            1. **Transformers** (ê¶Œì¥): `pip install transformers torch`
            2. **OpenCV only**: `pip install opencv-python`
            3. **Simulation Mode**: íŒ¨í‚¤ì§€ ì—†ì´ ê¸°ë³¸ ê¸°ëŠ¥ ì‚¬ìš©
            """)

            issues = []

            # Check transformers
            try:
                import transformers
                st.success(f"âœ… transformers {transformers.__version__}")
            except ImportError:
                issues.append("transformers")
                st.warning("âš ï¸ transformers ë¯¸ì„¤ì¹˜")

            # Check torch
            try:
                import torch
                device = "GPU" if torch.cuda.is_available() else "CPU"
                st.success(f"âœ… torch {torch.__version__} ({device})")
            except ImportError:
                issues.append("torch")
                st.warning("âš ï¸ torch ë¯¸ì„¤ì¹˜")

            # Check opencv
            try:
                import cv2
                st.success(f"âœ… opencv-python {cv2.__version__}")
            except ImportError:
                issues.append("opencv-python")
                st.warning("âš ï¸ opencv-python ë¯¸ì„¤ì¹˜")

            # Check mediapipe
            try:
                import mediapipe
                st.success(f"âœ… mediapipe {mediapipe.__version__}")
            except ImportError:
                st.info("â„¹ï¸ mediapipe ë¯¸ì„¤ì¹˜ (ìš´ë™ ì¹´ìš´í„° ê¸°ëŠ¥ ì œí•œ)")

            if issues:
                st.info(f"""
                ### ğŸ”§ ì„¤ì¹˜ ë°©ë²•
                ```bash
                pip install transformers torch opencv-python mediapipe
                ```

                ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œì—ì„œë„ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
                """)

    # ==================== Tab 1: ê°œë… ì†Œê°œ ====================

    def render_theory(self):
        """í–‰ë™ì¸ì‹ ì´ë¡  ì„¤ëª…"""
        st.header("ğŸ“š í–‰ë™ì¸ì‹ ê°œë… ì†Œê°œ")

        st.markdown("""
        ## 1. í–‰ë™ì¸ì‹ì´ë€?

        **í–‰ë™ì¸ì‹(Action Recognition)**ì€ ë¹„ë””ì˜¤ì—ì„œ ì‚¬ëŒì´ë‚˜ ê°ì²´ì˜ í–‰ë™ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

        **ë¹„ìœ **: í–‰ë™ì¸ì‹ì€ "ì˜í™” ê°ë…ì´ ì¥ë©´ì„ ë³´ê³  ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒ"ê³¼ ê°™ì•„ìš”!

        ì¼ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” ì‚¬ì§„ í•œ ì¥ë§Œ ë³´ì§€ë§Œ, í–‰ë™ì¸ì‹ì€ **ì—°ì†ëœ í”„ë ˆì„ë“¤ì„ ë³´ê³  ì‹œê°„ì  ë³€í™”ë¥¼ ì´í•´**í•©ë‹ˆë‹¤.

        ---

        ### 1.1 ì´ë¯¸ì§€ vs ë¹„ë””ì˜¤ ë°ì´í„°

        | íŠ¹ì„± | ì´ë¯¸ì§€ | ë¹„ë””ì˜¤ |
        |------|--------|--------|
        | **ì°¨ì›** | 2D (H Ã— W Ã— C) | 3D (T Ã— H Ã— W Ã— C) |
        | **ì •ë³´** | ê³µê°„ (Spatial) | ê³µê°„ + ì‹œê°„ (Spatiotemporal) |
        | **ì˜ˆì‹œ** | "ê³ ì–‘ì´" | "ê³ ì–‘ì´ê°€ ë›°ì–´ì˜¤ë¥¸ë‹¤" |
        | **í¬ê¸°** | ì‘ìŒ (~MB) | í¼ (~GB) |

        ğŸ’¡ *T = ì‹œê°„(í”„ë ˆì„ ìˆ˜), H = ë†’ì´, W = ë„ˆë¹„, C = ì±„ë„(RGB)*

        ---

        ## 2. ì£¼ìš” ì•„í‚¤í…ì²˜

        ### 2.1 3D CNN (C3D)

        **ì•„ì´ë””ì–´**: Conv2Dë¥¼ Conv3Dë¡œ í™•ì¥í•˜ì—¬ ì‹œê°„ ì°¨ì›ë„ í•¨ê»˜ ì²˜ë¦¬

        **ë¹„ìœ **: 2DëŠ” ì‚¬ì§„ í•œ ì¥ì„ ë³´ëŠ” ê²ƒ, 3DëŠ” ì—°ì†ëœ ì‚¬ì§„ë“¤(ì˜í™”)ì„ í•œë²ˆì— ë³´ëŠ” ê²ƒ

        **íŠ¹ì§•**:
        - ì…ë ¥: (T Ã— H Ã— W) ë¹„ë””ì˜¤ í´ë¦½
        - Conv3D í•„í„°: (t Ã— h Ã— w) í¬ê¸°
        - ì‹œê³µê°„ íŠ¹ì§• ë™ì‹œ ì¶”ì¶œ

        **ì¥ì **: ê°„ë‹¨í•˜ê³  íš¨ê³¼ì 
        **ë‹¨ì **: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ

        ---

        ### 2.2 Two-Stream Networks

        **ì•„ì´ë””ì–´**: ê³µê°„ ì •ë³´(RGB)ì™€ ì‹œê°„ ì •ë³´(Optical Flow)ë¥¼ ë³„ë„ ì²˜ë¦¬ í›„ ê²°í•©

        **ë¹„ìœ **: í•œ ëˆˆìœ¼ë¡œëŠ” "ë¬´ì—‡ì´ ìˆëŠ”ì§€"(ì™¸í˜•), ë‹¤ë¥¸ ëˆˆìœ¼ë¡œëŠ” "ì–´ë–»ê²Œ ì›€ì§ì´ëŠ”ì§€"(ë™ì‘)ë¥¼ ë³¸ë‹¤

        **êµ¬ì¡°**:
        1. **Spatial Stream (ê³µê°„)**: RGB í”„ë ˆì„ â†’ ì™¸í˜• ì¸ì‹
        2. **Temporal Stream (ì‹œê°„)**: Optical Flow â†’ ì›€ì§ì„ ì¸ì‹
        3. **Fusion**: ë‘ ìŠ¤íŠ¸ë¦¼ ê²°ê³¼ ê²°í•©

        **ì¥ì **: ì™¸í˜•ê³¼ ë™ì‘ ëª…í™•íˆ ë¶„ë¦¬
        **ë‹¨ì **: Optical Flow ê³„ì‚° ë¹„ìš© ë†’ìŒ

        ---

        ### 2.3 Video Transformer

        **ì•„ì´ë””ì–´**: Transformerë¥¼ ë¹„ë””ì˜¤ì— ì ìš©í•˜ì—¬ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°©

        **ëŒ€í‘œ ëª¨ë¸**:

        #### **TimeSformer** (Facebook AI, 2021)
        - **Divided Space-Time Attention**
        - ê³µê°„ Attention + ì‹œê°„ Attention ë¶„ë¦¬
        - íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥

        #### **VideoMAE** (2022)
        - **Masked Autoencoding**
        - í”„ë ˆì„ ì¼ë¶€ë¥¼ ê°€ë¦¬ê³  ë³µì›í•˜ë©° í•™ìŠµ
        - ì ì€ ë°ì´í„°ë¡œë„ ê³ ì„±ëŠ¥

        #### **X-CLIP** (Microsoft, 2022)
        - CLIPì„ ë¹„ë””ì˜¤ë¡œ í™•ì¥
        - í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ì •ë ¬
        - Zero-shot í–‰ë™ ì¸ì‹ ê°€ëŠ¥

        **ì¥ì **:
        - ì¥ê±°ë¦¬ ì˜ì¡´ì„± í¬ì°© (ë¨¼ í”„ë ˆì„ ê°„ ê´€ê³„)
        - Pre-trainingìœ¼ë¡œ ê³ ì„±ëŠ¥

        **ë‹¨ì **:
        - ê³„ì‚° ë¹„ìš© ë†’ìŒ
        - ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”

        ---

        ## 3. Optical Flow (ê´‘í•™ íë¦„)

        **ì •ì˜**: ì—°ì†ëœ ë‘ í”„ë ˆì„ ì‚¬ì´ì˜ í”½ì…€ ì´ë™ ë²¡í„°

        **ë¹„ìœ **: ë¬¼ê²°ì´ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ íë¥´ëŠ”ì§€ í™”ì‚´í‘œë¡œ í‘œì‹œí•˜ëŠ” ê²ƒê³¼ ê°™ì•„ìš”

        ### 3.1 Farneback ì•Œê³ ë¦¬ì¦˜

        **íŠ¹ì§•**:
        - Dense Optical Flow (ëª¨ë“  í”½ì…€ì˜ ì›€ì§ì„ ê³„ì‚°)
        - ë‹¤í•­ì‹ ê·¼ì‚¬ ì‚¬ìš©
        - OpenCVë¡œ ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥

        **ì‹œê°í™”**:
        - **Hue (ìƒ‰ìƒ)**: ì›€ì§ì„ ë°©í–¥
        - **Value (ë°ê¸°)**: ì›€ì§ì„ í¬ê¸°
        - **ê²°ê³¼**: ì»¬ëŸ¬í’€í•œ ì›€ì§ì„ ë§µ

        ğŸ’¡ *ë¹¨ê°•=ì˜¤ë¥¸ìª½, íŒŒë‘=ì™¼ìª½, ì´ˆë¡=ì•„ë˜, ë…¸ë‘=ìœ„ ì›€ì§ì„*

        ---

        ## 4. í–‰ë™ì¸ì‹ì˜ ì‘ìš©

        ### 4.1 ìŠ¤í¬ì¸  ë¶„ì„
        - **ì˜ˆì‹œ**: ì¶•êµ¬ í•˜ì´ë¼ì´íŠ¸ ìë™ ìƒì„±, ê³¨ ì¥ë©´ ê°ì§€
        - **ì‚¬ìš©ì**: ë°©ì†¡ì‚¬, ìŠ¤í¬ì¸  íŒ€

        ### 4.2 ë³´ì•ˆ ë° ê°ì‹œ
        - **ì˜ˆì‹œ**: ì´ìƒ í–‰ë™ ê°ì§€ (ì‹¸ì›€, ì“°ëŸ¬ì§)
        - **ì‚¬ìš©ì**: ë³´ì•ˆ ì—…ì²´, ê³µí•­, ë³‘ì›

        ### 4.3 HCI (Human-Computer Interaction)
        - **ì˜ˆì‹œ**: ì œìŠ¤ì²˜ ì¸ì‹, ìˆ˜í™” ë²ˆì—­
        - **ì‚¬ìš©ì**: ê²Œì„, VR/AR, ì ‘ê·¼ì„± ë„êµ¬

        ### 4.4 ììœ¨ì£¼í–‰
        - **ì˜ˆì‹œ**: ë³´í–‰ì í–‰ë™ ì˜ˆì¸¡ (ê±´ë„ˆë ¤ëŠ”ì§€, ë©ˆì¶œì§€)
        - **ì‚¬ìš©ì**: ìë™ì°¨ íšŒì‚¬

        ### 4.5 í—¬ìŠ¤ì¼€ì–´
        - **ì˜ˆì‹œ**: ìš´ë™ ìì„¸ êµì •, ì¬í™œ ëª¨ë‹ˆí„°ë§
        - **ì‚¬ìš©ì**: í”¼íŠ¸ë‹ˆìŠ¤ ì•±, ë³‘ì›

        ### 4.6 ì½˜í…ì¸  ì œì‘
        - **ì˜ˆì‹œ**: ì˜ìƒ ìë™ í¸ì§‘, í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
        - **ì‚¬ìš©ì**: ìœ íŠœë²„, ë°©ì†¡ PD

        ---

        ## ì°¸ê³  ìë£Œ
        - [C3D ë…¼ë¬¸](https://arxiv.org/abs/1412.0767)
        - [Two-Stream ë…¼ë¬¸](https://arxiv.org/abs/1406.2199)
        - [TimeSformer ë…¼ë¬¸](https://arxiv.org/abs/2102.05095)
        - [VideoMAE ë…¼ë¬¸](https://arxiv.org/abs/2203.12602)
        """)

    # ==================== Tab 2: ë¹„ë””ì˜¤ ì²˜ë¦¬ ê¸°ì´ˆ ====================

    def render_video_basics(self):
        """ë¹„ë””ì˜¤ ì²˜ë¦¬ ê¸°ì´ˆ"""
        st.header("ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ê¸°ì´ˆ")

        st.markdown("""
        ì´ íƒ­ì—ì„œëŠ” ë¹„ë””ì˜¤ ì²˜ë¦¬ì˜ ê¸°ì´ˆë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤:
        1. **í”„ë ˆì„ ì¶”ì¶œ**: ë¹„ë””ì˜¤ â†’ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤
        2. **Optical Flow**: í”„ë ˆì„ ê°„ ì›€ì§ì„ ê³„ì‚°
        3. **ì‹œê°í™”**: ì›€ì§ì„ì„ ìƒ‰ìƒìœ¼ë¡œ í‘œí˜„
        """)

        # VideoHelper ë¡œë“œ
        helper = get_video_helper()
        st.markdown(f"**í˜„ì¬ ëª¨ë“œ**: `{helper.get_mode()}`")

        # ë¹„ë””ì˜¤ ì—…ë¡œë“œ
        uploaded = st.file_uploader(
            "ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ (.mp4, .avi)",
            type=['mp4', 'avi', 'mov'],
            key="video_basics_upload"
        )

        if uploaded:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            video_bytes = uploaded.read()
            temp_path = helper.save_temp_video(video_bytes)

            if temp_path:
                # ë¹„ë””ì˜¤ ì •ë³´
                info = helper.get_video_info(temp_path)

                col1, col2, col3, col4 = st.columns(4)
                col1.metric("FPS", f"{info['fps']:.1f}")
                col2.metric("Duration", f"{info['duration']:.1f}s")
                col3.metric("Resolution", f"{info['resolution'][0]}Ã—{info['resolution'][1]}")
                col4.metric("Frames", info['frame_count'])

                # ê¸°ëŠ¥ ì„ íƒ
                demo_type = st.radio(
                    "ë°ëª¨ ì„ íƒ",
                    ["í”„ë ˆì„ ì¶”ì¶œ", "Optical Flow"],
                    horizontal=True
                )

                if demo_type == "í”„ë ˆì„ ì¶”ì¶œ":
                    self._demo_frame_extraction(temp_path, helper)
                else:
                    self._demo_optical_flow(temp_path, helper)

    def _demo_frame_extraction(self, video_path: str, helper):
        """í”„ë ˆì„ ì¶”ì¶œ ë°ëª¨"""
        st.subheader("ğŸ“¸ í”„ë ˆì„ ì¶”ì¶œ")

        st.markdown("""
        **íŒŒë¼ë¯¸í„°**:
        - **sample_rate**: ë§¤ N í”„ë ˆì„ë‹¹ 1ê°œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        - **max_frames**: ìµœëŒ€ í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸ 100ê°œ)
        """)

        col1, col2 = st.columns(2)
        with col1:
            sample_rate = st.slider("Sample Rate", 1, 60, 30,
                                   help="ë†’ì„ìˆ˜ë¡ ì ê²Œ ì¶”ì¶œ (ë¹ ë¦„)")
        with col2:
            max_frames = st.slider("Max Frames", 10, 200, 50,
                                  help="ë©”ëª¨ë¦¬ ì œí•œ")

        if st.button("ğŸ¬ í”„ë ˆì„ ì¶”ì¶œ", type="primary"):
            with st.spinner("í”„ë ˆì„ ì¶”ì¶œ ì¤‘..."):
                frames = helper.extract_frames(
                    video_path,
                    sample_rate=sample_rate,
                    max_frames=max_frames
                )

                if frames:
                    st.success(f"âœ… {len(frames)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")

                    # ì¼ë¶€ í”„ë ˆì„ í‘œì‹œ
                    num_display = min(6, len(frames))
                    cols = st.columns(3)

                    for i in range(num_display):
                        with cols[i % 3]:
                            st.image(frames[i], caption=f"Frame {i}", use_container_width=True)

    def _demo_optical_flow(self, video_path: str, helper):
        """Optical Flow ë°ëª¨"""
        st.subheader("ğŸŒŠ Optical Flow")

        st.markdown("""
        **ì„¤ëª…**: Farneback ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì—°ì† í”„ë ˆì„ ê°„ ì›€ì§ì„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        **ìƒ‰ìƒ ì˜ë¯¸**:
        - ğŸ”´ ë¹¨ê°•: ì˜¤ë¥¸ìª½ ì´ë™
        - ğŸ”µ íŒŒë‘: ì™¼ìª½ ì´ë™
        - ğŸŸ¢ ì´ˆë¡: ì•„ë˜ ì´ë™
        - ğŸŸ¡ ë…¸ë‘: ìœ„ ì´ë™
        - ë°ê¸°: ì›€ì§ì„ ì†ë„
        """)

        if st.button("ğŸŒŠ Optical Flow ê³„ì‚°", type="primary"):
            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                # í”„ë ˆì„ ì¶”ì¶œ
                frames = helper.extract_frames(video_path, sample_rate=5, max_frames=20)

                if len(frames) >= 2:
                    # ì²« 2ê°œ í”„ë ˆì„ìœ¼ë¡œ Flow ê³„ì‚°
                    flow = helper.compute_optical_flow(frames[0], frames[1])
                    flow_vis = helper.visualize_flow(flow)

                    # ì‹œê°í™”
                    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

                    axes[0].imshow(frames[0])
                    axes[0].set_title("Frame t")
                    axes[0].axis('off')

                    axes[1].imshow(frames[1])
                    axes[1].set_title("Frame t+1")
                    axes[1].axis('off')

                    axes[2].imshow(flow_vis)
                    axes[2].set_title("Optical Flow")
                    axes[2].axis('off')

                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.close()

                    # í†µê³„
                    magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
                    st.info(f"""
                    **ì›€ì§ì„ í†µê³„**:
                    - í‰ê·  ì´ë™ ê±°ë¦¬: {np.mean(magnitude):.2f} í”½ì…€
                    - ìµœëŒ€ ì´ë™ ê±°ë¦¬: {np.max(magnitude):.2f} í”½ì…€
                    """)

    # ==================== Tab 3: ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ====================

    def render_pretrained_models(self):
        """ì‚¬ì „í›ˆë ¨ ëª¨ë¸ í™œìš©"""
        st.header("ğŸ¤– ì‚¬ì „í›ˆë ¨ ëª¨ë¸ í™œìš©")

        st.markdown("""
        HuggingFaceì˜ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ë¡œ í–‰ë™ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.

        ### ì§€ì› ëª¨ë¸
        - **VideoMAE**: Masked Autoencoding, Kinetics-400 ë°ì´í„°ì…‹
        - **TimeSformer**: Divided Space-Time Attention
        - **X-CLIP**: CLIP ê¸°ë°˜ ë¹„ë””ì˜¤ ëª¨ë¸

        âš ï¸ **ì£¼ì˜**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (~1GB).
        """)

        helper = get_video_helper()

        # ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ëŠ¥ ê²½ê³ 
        if not helper.is_available('action_classification'):
            st.warning("""
            âš ï¸ **Transformers ëª¨ë“œ í•„ìš”**

            í–‰ë™ ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ transformersì™€ torchë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:
            ```bash
            pip install transformers torch
            ```

            í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
            """)

        # ëª¨ë¸ ì„ íƒ
        model_name = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            ["videomae", "timesformer", "xclip"],
            help="VideoMAE ê¶Œì¥ (ë¹ ë¥´ê³  ì •í™•)"
        )

        # ë¹„ë””ì˜¤ ì—…ë¡œë“œ
        uploaded = st.file_uploader(
            "ë¹„ë””ì˜¤ íŒŒì¼",
            type=['mp4', 'avi', 'mov'],
            key="model_upload"
        )

        if uploaded:
            video_bytes = uploaded.read()
            temp_path = helper.save_temp_video(video_bytes)

            if temp_path:
                # ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸°
                st.video(uploaded)

                if st.button("ğŸ¬ í–‰ë™ ë¶„ë¥˜", type="primary"):
                    with st.spinner("ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  ì¤‘... (ìµœëŒ€ 1ë¶„)"):
                        results = helper.classify_action(temp_path, model_name, top_k=5)

                        # ê²°ê³¼ í‘œì‹œ
                        st.subheader("ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼")

                        if results and results[0][0] != 'error':
                            for i, (label, score) in enumerate(results):
                                st.metric(
                                    f"#{i+1} {label}",
                                    f"{score*100:.1f}%"
                                )

                                # í”„ë¡œê·¸ë ˆìŠ¤ ë°”
                                st.progress(score)

                            # ì„¤ëª…
                            top_label = results[0][0]
                            st.success(f"âœ… ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í–‰ë™: **{top_label}**")
                        else:
                            st.error("âŒ ë¶„ë¥˜ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # ==================== Tab 4: ì‹¤ì‹œê°„ ì¸ì‹ ====================

    def render_realtime(self):
        """ì‹¤ì‹œê°„ í–‰ë™ ì¸ì‹ ì•ˆë‚´"""
        st.header("ğŸ“¹ ì‹¤ì‹œê°„ í–‰ë™ ì¸ì‹")

        st.markdown("""
        ## ì‹¤ì‹œê°„ ì›¹ìº  í–‰ë™ ì¸ì‹

        ì‹¤ì‹œê°„ ì›¹ìº  ì²˜ë¦¬ëŠ” **lab íŒŒì¼**ì—ì„œ ì œê³µë©ë‹ˆë‹¤.

        ### ì‹¤í–‰ ë°©ë²•

        1. í„°ë¯¸ë„ì—ì„œ lab íŒŒì¼ ì‹¤í–‰:
        ```bash
        cd modules/week07/labs
        python lab04_realtime_recognition.py
        ```

        2. ì›¹ìº ì´ ìë™ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤

        3. í‚¤ë³´ë“œ ì¡°ì‘:
        - **SPACE**: í”„ë ˆì„ ì €ì¥
        - **ESC**: ì¢…ë£Œ

        ---

        ### ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™” íŒ

        #### 1. í”„ë ˆì„ ìƒ˜í”Œë§
        - **ë¬¸ì œ**: 60fpsë¡œ ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ ì‹œ ëŠë¦¼
        - **í•´ê²°**: ë§¤ 5í”„ë ˆì„ë‹¹ 1ê°œë§Œ ì²˜ë¦¬ (12fps)

        #### 2. í•´ìƒë„ ë‹¤ìš´ìƒ˜í”Œë§
        - **ë¬¸ì œ**: 1080p ì²˜ë¦¬ ì‹œ ëŠë¦¼
        - **í•´ê²°**: 480p ë˜ëŠ” 360pë¡œ ë¦¬ì‚¬ì´ì¦ˆ

        #### 3. í”„ë ˆì„ ë²„í¼ë§
        - **ë¬¸ì œ**: ë‹¨ì¼ í”„ë ˆì„ìœ¼ë¡œëŠ” í–‰ë™ íŒë‹¨ ì–´ë ¤ì›€
        - **í•´ê²°**: ìµœê·¼ 16í”„ë ˆì„ì„ ë²„í¼ì— ì €ì¥

        #### 4. ë¹„ë™ê¸° ì²˜ë¦¬
        - **ë¬¸ì œ**: ëª¨ë¸ ì¶”ë¡  ì¤‘ í”„ë ˆì„ ë“œë¡­
        - **í•´ê²°**: ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì¶”ë¡ 

        ---

        ### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

        | ì„¤ì • | FPS | ì •í™•ë„ | ë©”ëª¨ë¦¬ |
        |------|-----|--------|--------|
        | CPU, 1080p, ëª¨ë“  í”„ë ˆì„ | ~5 | ë†’ìŒ | ~2GB |
        | CPU, 480p, 5í”„ë ˆì„ ìƒ˜í”Œ | ~20 | ì¤‘ê°„ | ~1GB |
        | GPU, 1080p, ëª¨ë“  í”„ë ˆì„ | ~30 | ë†’ìŒ | ~3GB |
        | GPU, 480p, 5í”„ë ˆì„ ìƒ˜í”Œ | ~60 | ì¤‘ê°„ | ~1.5GB |

        ğŸ’¡ **ê¶Œì¥**: GPU + 480p + 5í”„ë ˆì„ ìƒ˜í”Œë§

        ---

        ## Lab íŒŒì¼ ëª©ë¡

        ### lab04_realtime_recognition.py
        OpenCV ê¸°ë°˜ ì‹¤ì‹œê°„ ì›¹ìº  í–‰ë™ ì¸ì‹

        **ê¸°ëŠ¥**:
        - ì›¹ìº  ì…ë ¥ ì²˜ë¦¬
        - í”„ë ˆì„ ë²„í¼ë§
        - í–‰ë™ ë¶„ë¥˜ (ê°„ë‹¨í•œ ì›€ì§ì„ ê¸°ë°˜)
        - FPS í‘œì‹œ

        ### ì‹¤í–‰ ì˜ˆì‹œ
        ```python
        # lab04_realtime_recognition.py
        import cv2
        from action_helpers import get_video_helper

        helper = get_video_helper()
        cap = cv2.VideoCapture(0)

        frame_buffer = []

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # í”„ë ˆì„ ë²„í¼ì— ì¶”ê°€
            frame_buffer.append(frame)
            if len(frame_buffer) > 16:
                frame_buffer.pop(0)

            # 16í”„ë ˆì„ ëª¨ì´ë©´ ë¶„ë¥˜
            if len(frame_buffer) == 16:
                # ê°„ë‹¨í•œ ì›€ì§ì„ ê¸°ë°˜ ë¶„ë¥˜
                pass

            cv2.imshow('Realtime Recognition', frame)

            if cv2.waitKey(1) & 0xFF == 27:  # ESC
                break

        cap.release()
        cv2.destroyAllWindows()
        ```
        """)

        st.info("""
        ğŸ’¡ **Streamlit ì œí•œ ì‚¬í•­**

        Streamlitì€ ì›¹ ê¸°ë°˜ì´ë¼ ì§ì ‘ ì›¹ìº  ì ‘ê·¼ì´ ì œí•œì ì…ë‹ˆë‹¤.
        ì‹¤ì‹œê°„ ì²˜ë¦¬ëŠ” **ë³„ë„ Python ìŠ¤í¬ë¦½íŠ¸**ë¡œ ì‹¤í–‰í•˜ì„¸ìš”.
        """)

    # ==================== Tab 5: ì‹¤ì „ ì‘ìš© ====================

    def render_applications(self):
        """ì‹¤ì „ ì‘ìš© ì˜ˆì œ"""
        st.header("ğŸ’¼ ì‹¤ì „ ì‘ìš©")

        st.markdown("""
        ## í–‰ë™ì¸ì‹ í™œìš© ì‚¬ë¡€

        ì‹¤ì „ì—ì„œ í–‰ë™ì¸ì‹ì„ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ ì‹¤ìŠµí•©ë‹ˆë‹¤.
        """)

        app_type = st.selectbox(
            "ì‘ìš© ì˜ˆì œ ì„ íƒ",
            [
                "ìš´ë™ ì¹´ìš´í„° (í‘¸ì‹œì—…/ìŠ¤ì¿¼íŠ¸)",
                "ì œìŠ¤ì²˜ ì¸ì‹",
                "ì´ìƒ í–‰ë™ ê°ì§€"
            ]
        )

        if app_type == "ìš´ë™ ì¹´ìš´í„° (í‘¸ì‹œì—…/ìŠ¤ì¿¼íŠ¸)":
            self._app_exercise_counter()
        elif app_type == "ì œìŠ¤ì²˜ ì¸ì‹":
            self._app_gesture()
        else:
            self._app_anomaly()

    def _app_exercise_counter(self):
        """ìš´ë™ ì¹´ìš´í„° ì•±"""
        st.subheader("ğŸ‹ï¸ ìš´ë™ ì¹´ìš´í„°")

        st.markdown("""
        **ê¸°ëŠ¥**: MediaPipe Poseë¡œ ê´€ì ˆ ê°ë„ë¥¼ ì¶”ì í•˜ì—¬ ìš´ë™ ë°˜ë³µ íšŸìˆ˜ ìë™ ì¹´ìš´íŠ¸

        **ì§€ì› ìš´ë™**:
        - í‘¸ì‹œì—… (Pushup): íŒ”ê¿ˆì¹˜ ê°ë„
        - ìŠ¤ì¿¼íŠ¸ (Squat): ë¬´ë¦ ê°ë„
        - ì í”„ì­ (Jumping Jack): íŒ”/ë‹¤ë¦¬ ê°ë„
        """)

        helper = get_video_helper()

        # MediaPipe ë¯¸ì„¤ì¹˜ ê²½ê³ 
        try:
            import mediapipe
        except ImportError:
            st.warning("""
            âš ï¸ **MediaPipe ë¯¸ì„¤ì¹˜**

            ìš´ë™ ì¹´ìš´í„°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:
            ```bash
            pip install mediapipe
            ```

            í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
            """)

        # ìš´ë™ ì„ íƒ
        exercise_type = st.selectbox(
            "ìš´ë™ ì¢…ë¥˜",
            ["pushup", "squat", "jumping_jack"]
        )

        # ë¹„ë””ì˜¤ ì—…ë¡œë“œ
        uploaded = st.file_uploader(
            "ìš´ë™ ë¹„ë””ì˜¤ ì—…ë¡œë“œ",
            type=['mp4', 'avi', 'mov'],
            key="exercise_upload"
        )

        if uploaded:
            video_bytes = uploaded.read()
            temp_path = helper.save_temp_video(video_bytes)

            if temp_path:
                st.video(uploaded)

                if st.button("ğŸ‹ï¸ ìš´ë™ ì¹´ìš´íŠ¸", type="primary"):
                    with st.spinner("ê´€ì ˆ ì¶”ì  ë° ì¹´ìš´íŠ¸ ì¤‘..."):
                        result = helper.count_exercise_reps(temp_path, exercise_type)

                        # ê²°ê³¼ í‘œì‹œ
                        col1, col2 = st.columns(2)

                        with col1:
                            st.metric("ë°˜ë³µ íšŸìˆ˜", f"{result['count']}íšŒ")

                        with col2:
                            st.metric("ì‹ ë¢°ë„", f"{result['confidence']*100:.1f}%")

                        # ê°ë„ ê·¸ë˜í”„
                        if result['angle_history']:
                            st.subheader("ğŸ“Š ê´€ì ˆ ê°ë„ ë³€í™”")

                            fig, ax = plt.subplots(figsize=(10, 4))
                            ax.plot(result['angle_history'], linewidth=2)
                            ax.axhline(y=100, color='r', linestyle='--', label='Down position')
                            ax.axhline(y=140, color='g', linestyle='--', label='Up position')
                            ax.set_xlabel("í”„ë ˆì„")
                            ax.set_ylabel("ê°ë„ (ë„)")
                            ax.set_title(f"{exercise_type} ê´€ì ˆ ê°ë„ ë³€í™”")
                            ax.legend()
                            ax.grid(True, alpha=0.3)
                            st.pyplot(fig)
                            plt.close()

                        st.success(f"âœ… {exercise_type} {result['count']}íšŒ ì™„ë£Œ!")

    def _app_gesture(self):
        """ì œìŠ¤ì²˜ ì¸ì‹"""
        st.subheader("ğŸ‘‹ ì œìŠ¤ì²˜ ì¸ì‹")

        st.markdown("""
        **ê°œë…**: ì†ë™ì‘ì„ ì¸ì‹í•˜ì—¬ ì»´í“¨í„°ë¥¼ ì œì–´

        **ì‘ìš© ë¶„ì•¼**:
        - ìŠ¤ë§ˆíŠ¸ í™ˆ ì œì–´ (ì†ì§“ìœ¼ë¡œ ì¡°ëª… ì¡°ì ˆ)
        - í”„ë ˆì  í…Œì´ì…˜ ì œì–´ (ìŠ¬ë¼ì´ë“œ ë„˜ê¸°ê¸°)
        - VR/AR ì¸í„°í˜ì´ìŠ¤

        **ì£¼ìš” ì œìŠ¤ì²˜**:
        - ğŸ‘ ì¢‹ì•„ìš” (Thumbs Up)
        - ğŸ‘‹ ì† í”ë“¤ê¸° (Wave)
        - âœŒï¸ V ì‚¬ì¸ (Peace)
        - ğŸ‘Š ì£¼ë¨¹ (Fist)
        - âœ‹ ì •ì§€ (Stop)

        ---

        ğŸ’¡ **êµ¬í˜„ íŒ**:

        1. **MediaPipe Hands** ì‚¬ìš©
        2. 21ê°œ ì† ëœë“œë§ˆí¬ ì¶”ì 
        3. ì†ê°€ë½ ê´€ì ˆ ê°ë„ë¡œ ì œìŠ¤ì²˜ ë¶„ë¥˜
        4. ì‹œê°„ì  ì¼ê´€ì„± ì²´í¬ (ë–¨ë¦¼ ë°©ì§€)

        ---

        ì´ ê¸°ëŠ¥ì€ **lab05_practical_apps.py**ì—ì„œ êµ¬í˜„ë©ë‹ˆë‹¤.
        """)

        st.info("""
        ğŸ“ **ì‹¤ìŠµ ê³¼ì œ**

        lab05_practical_apps.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì œìŠ¤ì²˜ ì¸ì‹ì„ ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš”!
        """)

    def _app_anomaly(self):
        """ì´ìƒ í–‰ë™ ê°ì§€"""
        st.subheader("âš ï¸ ì´ìƒ í–‰ë™ ê°ì§€")

        st.markdown("""
        **ëª©ì **: CCTV ì˜ìƒì—ì„œ ìœ„í—˜ ìƒí™© ìë™ ê°ì§€

        **ê°ì§€ ëŒ€ìƒ**:
        - ğŸš¨ í­ë ¥ (ì‹¸ì›€, í­í–‰)
        - ğŸƒ ê¸‰ê²©í•œ ì›€ì§ì„ (ë„ë§, ì¶”ê²©)
        - ğŸ˜µ ì“°ëŸ¬ì§ (ë‚™ìƒ, ì‘ê¸‰ìƒí™©)
        - ğŸ”¥ í™”ì¬/ì—°ê¸°
        """)

        # ë¶„ì„ ë°©ë²• ì„ íƒ
        detection_method = st.radio(
            "ê°ì§€ ë°©ë²• ì„ íƒ",
            ["Optical Flow (ì„ê³„ê°’ ê¸°ë°˜)", "MediaPipe (í¬ì¦ˆ ê¸°ë°˜)", "Google Video Intelligence (AI ê¸°ë°˜)"],
            horizontal=True
        )

        # ë¹„ë””ì˜¤ ì—…ë¡œë“œ
        uploaded_video = st.file_uploader(
            "ğŸ¥ CCTV ë˜ëŠ” ë³´ì•ˆ ì˜ìƒ ì—…ë¡œë“œ",
            type=['mp4', 'avi', 'mov'],
            key="anomaly_upload"
        )

        if uploaded_video:
            # ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸°
            st.video(uploaded_video)

            # VideoHelper ë¡œë“œ
            helper = get_video_helper()

            # ì„ì‹œ íŒŒì¼ ì €ì¥
            video_bytes = uploaded_video.read()
            temp_path = helper.save_temp_video(video_bytes)

            if temp_path:
                # íŒŒë¼ë¯¸í„° ì„¤ì •
                col1, col2, col3 = st.columns(3)

                with col1:
                    sample_rate = st.slider(
                        "í”„ë ˆì„ ìƒ˜í”Œë§ Rate",
                        min_value=1, max_value=30, value=5,
                        help="ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ í”„ë ˆì„ ë¶„ì„ (ì •í™•ë„â†‘, ì†ë„â†“)"
                    )

                with col2:
                    if detection_method == "Optical Flow (ì„ê³„ê°’ ê¸°ë°˜)":
                        motion_threshold = st.slider(
                            "ì›€ì§ì„ ì„ê³„ê°’ (í”½ì…€)",
                            min_value=5, max_value=30, value=15,
                            help="ì´ ê°’ ì´ìƒì˜ ì›€ì§ì„ì„ ì´ìƒ í–‰ë™ìœ¼ë¡œ íŒë‹¨"
                        )
                    elif detection_method == "MediaPipe (í¬ì¦ˆ ê¸°ë°˜)":
                        fall_threshold = st.slider(
                            "ë‚™ìƒ ê°ì§€ ì„ê³„ê°’",
                            min_value=0.1, max_value=1.0, value=0.5,
                            help="ì‹ ì²´ ì¤‘ì‹¬ì  ë³€í™”ëŸ‰ ì„ê³„ê°’"
                        )
                    else:  # Google Video Intelligence
                        confidence_threshold = st.slider(
                            "ì‹ ë¢°ë„ ì„ê³„ê°’",
                            min_value=0.5, max_value=1.0, value=0.7,
                            help="ì´ ê°’ ì´ìƒì˜ ì‹ ë¢°ë„ë§Œ í‘œì‹œ"
                        )

                with col3:
                    max_frames = st.number_input(
                        "ìµœëŒ€ ë¶„ì„ í”„ë ˆì„",
                        min_value=10, max_value=500, value=100,
                        help="ë©”ëª¨ë¦¬ ì œí•œì„ ìœ„í•œ ìµœëŒ€ í”„ë ˆì„ ìˆ˜"
                    )

                # ë¶„ì„ ì‹œì‘
                if st.button("ğŸ” ì´ìƒ í–‰ë™ ë¶„ì„ ì‹œì‘", type="primary", key="analyze_anomaly"):
                    with st.spinner(f"{detection_method} ë°©ë²•ìœ¼ë¡œ ë¶„ì„ ì¤‘..."):
                        if detection_method == "Optical Flow (ì„ê³„ê°’ ê¸°ë°˜)":
                            results = self._analyze_with_optical_flow(
                                helper, temp_path, sample_rate, motion_threshold, max_frames
                            )
                        elif detection_method == "MediaPipe (í¬ì¦ˆ ê¸°ë°˜)":
                            results = self._analyze_with_mediapipe(
                                helper, temp_path, sample_rate, fall_threshold, max_frames
                            )
                        else:  # Google Video Intelligence
                            results = self._analyze_with_google_api(
                                helper, temp_path, confidence_threshold
                            )

                        # ê²°ê³¼ í‘œì‹œ
                        self._display_anomaly_results(results, detection_method)

        # ë°©ë²•ë¡  ì„¤ëª…
        with st.expander("ğŸ“š ê°ì§€ ë°©ë²• ìƒì„¸ ì„¤ëª…", expanded=False):
            st.markdown("""
            ### 1. Optical Flow (ì„ê³„ê°’ ê¸°ë°˜)
            **ì›ë¦¬**: ì—°ì† í”„ë ˆì„ ê°„ í”½ì…€ ì´ë™ëŸ‰ ê³„ì‚°
            - ì •ìƒ: í‰ê·  ì›€ì§ì„ < ì„ê³„ê°’
            - ì´ìƒ: í‰ê·  ì›€ì§ì„ > ì„ê³„ê°’
            - **ì¥ì **: ë¹ ë¥´ê³  ê°„ë‹¨
            - **ë‹¨ì **: ì •í™•ë„ ì œí•œì 

            ### 2. MediaPipe (í¬ì¦ˆ ê¸°ë°˜)
            **ì›ë¦¬**: ì‹ ì²´ í¬ì¦ˆ ì¶”ì ì„ í†µí•œ ì´ìƒ ê°ì§€
            - ë‚™ìƒ: ì‹ ì²´ ì¤‘ì‹¬ì  ê¸‰ê²©í•œ í•˜ê°•
            - í­ë ¥: ë¹ ë¥¸ íŒ” ì›€ì§ì„ íŒ¨í„´
            - **ì¥ì **: êµ¬ì²´ì ì¸ í–‰ë™ êµ¬ë¶„ ê°€ëŠ¥
            - **ë‹¨ì **: ì‚¬ëŒì´ ë³´ì—¬ì•¼ í•¨

            ### 3. Google Video Intelligence (AI ê¸°ë°˜)
            **ì›ë¦¬**: ì‚¬ì „ í•™ìŠµëœ AI ëª¨ë¸ í™œìš©
            - 400+ í–‰ë™ ë ˆì´ë¸” ì¸ì‹
            - ë†’ì€ ì •í™•ë„
            - **ì¥ì **: ë‹¤ì–‘í•œ ìƒí™© ì¸ì‹
            - **ë‹¨ì **: API ë¹„ìš© ë°œìƒ
            """)

        st.warning("""
        âš ï¸ **ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­**

        CCTV ê¸°ë°˜ ì´ìƒ í–‰ë™ ê°ì§€ëŠ” **í”„ë¼ì´ë²„ì‹œ ì¹¨í•´** ìš°ë ¤ê°€ ìˆìŠµë‹ˆë‹¤.
        - ëª…í™•í•œ ë™ì˜ í•„ìš”
        - ì–¼êµ´ ìµëª…í™”
        - ë²•ì  ê·œì œ ì¤€ìˆ˜
        """)

    def _analyze_with_optical_flow(self, helper, video_path, sample_rate, threshold, max_frames):
        """Optical Flow ê¸°ë°˜ ì´ìƒ í–‰ë™ ë¶„ì„"""
        try:
            import cv2
            import numpy as np

            # í”„ë ˆì„ ì¶”ì¶œ
            frames = helper.extract_frames(video_path, sample_rate=sample_rate, max_frames=max_frames)

            motion_scores = []
            anomaly_frames = []

            # í”„ë ˆì„ ê°„ ì›€ì§ì„ ê³„ì‚°
            for i in range(len(frames) - 1):
                # Optical Flow ê³„ì‚°
                flow = helper.compute_optical_flow(frames[i], frames[i+1])
                magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
                avg_motion = np.mean(magnitude)

                motion_scores.append(avg_motion)

                # ì´ìƒ ê°ì§€
                if avg_motion > threshold:
                    anomaly_frames.append({
                        'frame': i,
                        'motion': avg_motion,
                        'type': 'High Motion Detected'
                    })

            # ì „ì²´ í†µê³„
            avg_motion = np.mean(motion_scores) if motion_scores else 0
            max_motion = np.max(motion_scores) if motion_scores else 0

            return {
                'status': 'completed',
                'avg_motion': avg_motion,
                'max_motion': max_motion,
                'motion_scores': motion_scores,
                'anomalies': anomaly_frames,
                'total_frames': len(frames),
                'anomaly_count': len(anomaly_frames)
            }

        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    def _analyze_with_mediapipe(self, helper, video_path, sample_rate, fall_threshold, max_frames):
        """MediaPipe ê¸°ë°˜ í¬ì¦ˆ ë¶„ì„"""
        try:
            import mediapipe as mp
            import numpy as np

            mp_pose = mp.solutions.pose
            pose = mp_pose.Pose(
                static_image_mode=False,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )

            # í”„ë ˆì„ ì¶”ì¶œ
            frames = helper.extract_frames(video_path, sample_rate=sample_rate, max_frames=max_frames)

            pose_history = []
            anomaly_frames = []

            for i, frame in enumerate(frames):
                # RGB ë³€í™˜ ë° í¬ì¦ˆ ê°ì§€
                rgb_frame = np.array(frame)
                results = pose.process(rgb_frame)

                if results.pose_landmarks:
                    # ì‹ ì²´ ì¤‘ì‹¬ì  ê³„ì‚° (ì—‰ë©ì´ ì¤‘ì‹¬)
                    left_hip = results.pose_landmarks.landmark[23]
                    right_hip = results.pose_landmarks.landmark[24]
                    center_y = (left_hip.y + right_hip.y) / 2

                    pose_history.append(center_y)

                    # ë‚™ìƒ ê°ì§€ (ê¸‰ê²©í•œ Y ë³€í™”)
                    if len(pose_history) > 1:
                        y_change = abs(pose_history[-1] - pose_history[-2])
                        if y_change > fall_threshold:
                            anomaly_frames.append({
                                'frame': i,
                                'change': y_change,
                                'type': 'Potential Fall Detected'
                            })

            pose.close()

            return {
                'status': 'completed',
                'pose_detected': len(pose_history),
                'anomalies': anomaly_frames,
                'total_frames': len(frames),
                'anomaly_count': len(anomaly_frames)
            }

        except ImportError:
            return {'status': 'error', 'message': 'MediaPipe not installed'}
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    def _analyze_with_google_api(self, helper, video_path, confidence_threshold):
        """Google Video Intelligence API ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œ êµ¬í˜„ì‹œ Google API í˜¸ì¶œ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        import random

        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
        potential_anomalies = [
            {'label': 'fighting', 'confidence': 0.85, 'start': 2.5, 'end': 4.2},
            {'label': 'running', 'confidence': 0.72, 'start': 5.0, 'end': 6.5},
            {'label': 'falling', 'confidence': 0.68, 'start': 8.0, 'end': 8.5},
            {'label': 'person walking', 'confidence': 0.95, 'start': 0.0, 'end': 2.0}
        ]

        # ì‹ ë¢°ë„ ì„ê³„ê°’ í•„í„°
        anomalies = [a for a in potential_anomalies
                    if a['confidence'] >= confidence_threshold
                    and a['label'] in ['fighting', 'running', 'falling']]

        return {
            'status': 'completed',
            'anomalies': anomalies,
            'total_labels': len(potential_anomalies),
            'anomaly_count': len(anomalies)
        }

    def _display_anomaly_results(self, results, method):
        """ì´ìƒ í–‰ë™ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        if results['status'] == 'error':
            st.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {results['message']}")
            return

        # ìš”ì•½ ë©”íŠ¸ë¦­
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("ë¶„ì„ ë°©ë²•", method.split(" ")[0])

        with col2:
            st.metric("ì´ìƒ ê°ì§€", f"{results['anomaly_count']}ê±´")

        with col3:
            if 'total_frames' in results:
                st.metric("ë¶„ì„ í”„ë ˆì„", results['total_frames'])
            else:
                st.metric("ë¶„ì„ ì™„ë£Œ", "âœ…")

        # ì´ìƒ í–‰ë™ ê°ì§€ ê²°ê³¼
        if results['anomaly_count'] > 0:
            st.error(f"ğŸš¨ **ì´ìƒ í–‰ë™ ê°ì§€ë¨!** ({results['anomaly_count']}ê±´)")

            # ìƒì„¸ ê²°ê³¼
            with st.expander("ğŸ” ìƒì„¸ ë¶„ì„ ê²°ê³¼", expanded=True):
                if method.startswith("Optical Flow"):
                    st.subheader("ì›€ì§ì„ ë¶„ì„")
                    for anomaly in results['anomalies']:
                        st.warning(f"Frame {anomaly['frame']}: {anomaly['type']} (ì›€ì§ì„: {anomaly['motion']:.2f} í”½ì…€)")

                    # ì›€ì§ì„ ê·¸ë˜í”„
                    if 'motion_scores' in results and results['motion_scores']:
                        import matplotlib.pyplot as plt
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.plot(results['motion_scores'], linewidth=2)
                        ax.axhline(y=15, color='r', linestyle='--', label='ì´ìƒ ì„ê³„ê°’')
                        ax.set_xlabel("í”„ë ˆì„")
                        ax.set_ylabel("í‰ê·  ì›€ì§ì„ (í”½ì…€)")
                        ax.set_title("í”„ë ˆì„ë³„ ì›€ì§ì„ ë¶„ì„")
                        ax.legend()
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)
                        plt.close()

                elif method.startswith("MediaPipe"):
                    st.subheader("í¬ì¦ˆ ê¸°ë°˜ ê°ì§€")
                    for anomaly in results['anomalies']:
                        st.warning(f"Frame {anomaly['frame']}: {anomaly['type']} (ë³€í™”ëŸ‰: {anomaly['change']:.3f})")

                else:  # Google API
                    st.subheader("AI ê¸°ë°˜ ê°ì§€")
                    for anomaly in results['anomalies']:
                        st.warning(
                            f"âš ï¸ {anomaly['label'].upper()} "
                            f"(ì‹ ë¢°ë„: {anomaly['confidence']:.1%}) "
                            f"ì‹œê°„: {anomaly['start']:.1f}ì´ˆ - {anomaly['end']:.1f}ì´ˆ"
                        )
        else:
            st.success("âœ… ì •ìƒ - ì´ìƒ í–‰ë™ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            if method.startswith("Optical Flow") and 'avg_motion' in results:
                st.info(f"í‰ê·  ì›€ì§ì„: {results['avg_motion']:.2f} í”½ì…€ (ì •ìƒ ë²”ìœ„)")

    # ==================== Tab 6: MediaPipe ì‹¤ì‹œê°„ ====================

    def render_mediapipe_realtime(self):
        """MediaPipeë¥¼ ì´ìš©í•œ ì‹¤ì‹œê°„ í–‰ë™ ì¸ì‹"""
        from .action_recognition_realtime import RealtimeActionRecognitionModule

        # ì‹¤ì‹œê°„ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        realtime_module = RealtimeActionRecognitionModule()

        # MediaPipe íƒ­ ë Œë”ë§
        realtime_module.render_mediapipe_tab()

    # ==================== Tab 7: Google Video Intelligence ====================

    def render_google_video_intelligence(self):
        """Google Video Intelligence APIë¥¼ ì´ìš©í•œ ë¹„ë””ì˜¤ ë¶„ì„"""
        from .action_recognition_realtime import RealtimeActionRecognitionModule

        # ì‹¤ì‹œê°„ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        realtime_module = RealtimeActionRecognitionModule()

        # Google Video Intelligence íƒ­ ë Œë”ë§
        realtime_module.render_google_cloud_tab()
