"""
Lab 05: API ì„±ëŠ¥ ë¹„êµ (API Performance Comparison)

ì´ ì‹¤ìŠµì—ì„œëŠ” ì—¬ëŸ¬ ê°ì • ì¸ì‹ APIì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤:
- Google Gemini vs OpenAI GPT-4o vs Simulation
- ì •í™•ë„ (ì¼ê´€ì„±) ë¹„êµ
- ì†ë„ (ì‘ë‹µ ì‹œê°„) ë¹„êµ
- ë¹„ìš© ì¶”ì • ë¹„êµ
- ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥

ì‚¬ìš©ë²•:
    python lab05_comparison.py --input image.jpg
    python lab05_comparison.py --input image.jpg --runs 5
    python lab05_comparison.py --batch images/ --output benchmark.txt
"""

import os
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
from PIL import Image
import statistics

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from modules.week08.emotion_helpers import EmotionHelper


# API ë¹„ìš© ì¶”ì • (USD)
API_COSTS = {
    'gemini': {
        'per_1k_images': 0.0025,  # Gemini Pro Vision (ê°€ìƒ ì˜ˆì‹œ)
        'name': 'Google Gemini 2.5 Pro'
    },
    'openai': {
        'per_1k_images': 0.01,    # GPT-4o Vision
        'name': 'OpenAI GPT-4o'
    },
    'simulation': {
        'per_1k_images': 0.0,     # ë¬´ë£Œ
        'name': 'Simulation Mode'
    }
}


def benchmark_api(
    api_mode: str,
    image: Image.Image,
    runs: int = 3
) -> Dict:
    """
    íŠ¹ì • API ëª¨ë“œì˜ ì„±ëŠ¥ì„ ë²¤ì¹˜ë§ˆí¬í•©ë‹ˆë‹¤.

    Args:
        api_mode: API ëª¨ë“œ ('gemini', 'openai', 'simulation')
        image: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        runs: ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜

    Returns:
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"  ğŸ”„ {API_COSTS[api_mode]['name']} í…ŒìŠ¤íŠ¸ ì¤‘... ({runs}íšŒ ì‹¤í–‰)")

    # EmotionHelper ì´ˆê¸°í™”
    helper = EmotionHelper()

    # ê°•ì œë¡œ íŠ¹ì • ëª¨ë“œ ì„¤ì •
    if api_mode == 'gemini' and helper.gemini_model is None:
        print(f"    âš ï¸ Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    if api_mode == 'openai' and helper.openai_client is None:
        print(f"    âš ï¸ OpenAI APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    # ëª¨ë“œ ê°•ì œ ì„¤ì •
    helper.mode = api_mode

    times = []
    results = []

    for i in range(runs):
        start = time.time()

        try:
            result = helper.analyze_basic_emotion(image)
            elapsed = time.time() - start

            times.append(elapsed)
            results.append(result)

        except Exception as e:
            print(f"    âŒ ì‹¤í–‰ {i+1} ì‹¤íŒ¨: {e}")
            continue

    if not times:
        return None

    # í†µê³„ ê³„ì‚°
    avg_time = statistics.mean(times)
    min_time = min(times)
    max_time = max(times)
    std_time = statistics.stdev(times) if len(times) > 1 else 0

    # ì¼ê´€ì„± ê³„ì‚° (ê²°ê³¼ ê°„ í‘œì¤€í¸ì°¨)
    consistency = calculate_consistency(results)

    # ë¹„ìš© ê³„ì‚° (1000íšŒ ê¸°ì¤€)
    cost_per_1k = API_COSTS[api_mode]['per_1k_images']

    return {
        'mode': api_mode,
        'name': API_COSTS[api_mode]['name'],
        'runs': len(times),
        'avg_time': avg_time,
        'min_time': min_time,
        'max_time': max_time,
        'std_time': std_time,
        'consistency': consistency,
        'cost_per_1k': cost_per_1k,
        'results': results
    }


def calculate_consistency(results: List[Dict[str, float]]) -> float:
    """
    ì—¬ëŸ¬ ë¶„ì„ ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        results: ê°ì • ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì¼ê´€ì„± ì ìˆ˜ (0.0 ~ 1.0, ë†’ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)
    """
    if len(results) < 2:
        return 1.0

    # ê° ê°ì •ë³„ í‘œì¤€í¸ì°¨ ê³„ì‚°
    emotions = list(results[0].keys())
    variances = []

    for emotion in emotions:
        values = [r[emotion] for r in results]
        if len(set(values)) > 1:
            variance = statistics.variance(values)
            variances.append(variance)

    if not variances:
        return 1.0

    # í‰ê·  ë¶„ì‚° â†’ ì¼ê´€ì„± ì ìˆ˜ë¡œ ë³€í™˜
    avg_variance = statistics.mean(variances)
    consistency = max(0.0, 1.0 - (avg_variance * 10))  # ìŠ¤ì¼€ì¼ ì¡°ì •

    return consistency


def print_comparison_table(benchmarks: List[Dict]):
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        benchmarks: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š API ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("=" * 80)

    # í—¤ë”
    print(f"\n{'API':<25} {'í‰ê·  ì‹œê°„':<12} {'ì¼ê´€ì„±':<10} {'ë¹„ìš©(1K)':<12} {'ì‹¤í–‰ íšŸìˆ˜':<8}")
    print("-" * 80)

    # ê° API ê²°ê³¼
    for bench in benchmarks:
        if bench is None:
            continue

        print(f"{bench['name']:<25} "
              f"{bench['avg_time']:<12.3f}ì´ˆ "
              f"{bench['consistency']:<10.2%} "
              f"${bench['cost_per_1k']:<11.4f} "
              f"{bench['runs']:<8}íšŒ")

    # ìƒì„¸ í†µê³„
    print("\n" + "=" * 80)
    print("ğŸ“ˆ ìƒì„¸ í†µê³„")
    print("=" * 80)

    for bench in benchmarks:
        if bench is None:
            continue

        print(f"\n{bench['name']}:")
        print(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {bench['avg_time']:.3f}ì´ˆ")
        print(f"  - ìµœì†Œ ì‘ë‹µ ì‹œê°„: {bench['min_time']:.3f}ì´ˆ")
        print(f"  - ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {bench['max_time']:.3f}ì´ˆ")
        print(f"  - í‘œì¤€ í¸ì°¨: {bench['std_time']:.3f}ì´ˆ")
        print(f"  - ì¼ê´€ì„± ì ìˆ˜: {bench['consistency']:.2%}")
        print(f"  - 1,000íšŒ ë¹„ìš©: ${bench['cost_per_1k']:.4f}")

        # ì§€ë°°ì  ê°ì • í‘œì‹œ
        if bench['results']:
            dominant_emotions = []
            for result in bench['results']:
                dominant = max(result.items(), key=lambda x: x[1])[0]
                dominant_emotions.append(dominant)

            # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚œ ê°ì •
            from collections import Counter
            emotion_counts = Counter(dominant_emotions)
            most_common = emotion_counts.most_common(1)[0]

            print(f"  - ì£¼ìš” ê°ì •: {most_common[0].upper()} ({most_common[1]}/{bench['runs']}íšŒ)")


def print_winner_summary(benchmarks: List[Dict]):
    """
    ê° ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ì„±ëŠ¥ APIë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

    Args:
        benchmarks: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    valid_benchmarks = [b for b in benchmarks if b is not None]

    if not valid_benchmarks:
        return

    print("\n" + "=" * 80)
    print("ğŸ† ì¹´í…Œê³ ë¦¬ë³„ ìš°ìŠ¹ì")
    print("=" * 80)

    # ì†ë„ ìš°ìŠ¹ì
    fastest = min(valid_benchmarks, key=lambda x: x['avg_time'])
    print(f"\nâš¡ ìµœê³  ì†ë„: {fastest['name']}")
    print(f"   - í‰ê·  ì‘ë‹µ ì‹œê°„: {fastest['avg_time']:.3f}ì´ˆ")

    # ì¼ê´€ì„± ìš°ìŠ¹ì
    most_consistent = max(valid_benchmarks, key=lambda x: x['consistency'])
    print(f"\nâœ… ìµœê³  ì¼ê´€ì„±: {most_consistent['name']}")
    print(f"   - ì¼ê´€ì„± ì ìˆ˜: {most_consistent['consistency']:.2%}")

    # ë¹„ìš© íš¨ìœ¨ì„± ìš°ìŠ¹ì
    cheapest = min(valid_benchmarks, key=lambda x: x['cost_per_1k'])
    print(f"\nğŸ’° ìµœê³  ë¹„ìš© íš¨ìœ¨: {cheapest['name']}")
    print(f"   - 1,000íšŒ ë¹„ìš©: ${cheapest['cost_per_1k']:.4f}")

    # ì¢…í•© ì ìˆ˜ (ì •ê·œí™”ëœ ì†ë„ + ì¼ê´€ì„± - ë¹„ìš©)
    for bench in valid_benchmarks:
        speed_score = 1.0 - (bench['avg_time'] / max(b['avg_time'] for b in valid_benchmarks))
        consistency_score = bench['consistency']
        cost_score = 1.0 - (bench['cost_per_1k'] / max(b['cost_per_1k'] for b in valid_benchmarks if b['cost_per_1k'] > 0))

        bench['overall_score'] = (speed_score + consistency_score + cost_score) / 3

    best_overall = max(valid_benchmarks, key=lambda x: x['overall_score'])
    print(f"\nğŸ¯ ì¢…í•© ìµœê³ : {best_overall['name']}")
    print(f"   - ì¢…í•© ì ìˆ˜: {best_overall['overall_score']:.2%}")


def save_benchmark_report(benchmarks: List[Dict], output_path: str):
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        benchmarks: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("API ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ\n")
        f.write("=" * 80 + "\n\n")

        for bench in benchmarks:
            if bench is None:
                continue

            f.write(f"{bench['name']}\n")
            f.write("-" * 40 + "\n")
            f.write(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {bench['avg_time']:.3f}ì´ˆ\n")
            f.write(f"ì¼ê´€ì„± ì ìˆ˜: {bench['consistency']:.2%}\n")
            f.write(f"1,000íšŒ ë¹„ìš©: ${bench['cost_per_1k']:.4f}\n")
            f.write(f"ì‹¤í–‰ íšŸìˆ˜: {bench['runs']}\n")
            f.write("\n")

    print(f"\nğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Lab 05: API ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬"
    )

    parser.add_argument(
        "--input",
        type=str,
        help="í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ"
    )

    parser.add_argument(
        "--batch",
        type=str,
        help="ë°°ì¹˜ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬"
    )

    parser.add_argument(
        "--runs",
        type=int,
        default=3,
        help="ê° APIë‹¹ ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)"
    )

    parser.add_argument(
        "--modes",
        type=str,
        nargs='+',
        default=['gemini', 'openai', 'simulation'],
        choices=['gemini', 'openai', 'simulation'],
        help="í…ŒìŠ¤íŠ¸í•  API ëª¨ë“œ (ê¸°ë³¸ê°’: ëª¨ë‘)"
    )

    parser.add_argument(
        "--output",
        type=str,
        help="ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ì €ì¥ ê²½ë¡œ (ì˜ˆ: benchmark.txt)"
    )

    args = parser.parse_args()

    if not args.input and not args.batch:
        parser.print_help()
        print("\nì˜ˆì œ:")
        print("  python lab05_comparison.py --input image.jpg")
        print("  python lab05_comparison.py --input image.jpg --runs 5")
        return

    # ì´ë¯¸ì§€ ë¡œë“œ
    if args.input:
        input_path = Path(args.input)

        if not input_path.exists():
            print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
            return

        image = Image.open(input_path)
        images = [image]

    elif args.batch:
        batch_dir = Path(args.batch)

        if not batch_dir.exists():
            print(f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.batch}")
            return

        # ì´ë¯¸ì§€ ì°¾ê¸°
        extensions = ['.jpg', '.jpeg', '.png', '.webp']
        image_files = []

        for ext in extensions:
            image_files.extend(batch_dir.glob(f"*{ext}"))

        if not image_files:
            print(f"âŒ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.batch}")
            return

        images = [Image.open(f) for f in sorted(image_files)]

    print(f"ğŸš€ API ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print(f"   - í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(images)}ê°œ")
    print(f"   - ë°˜ë³µ íšŸìˆ˜: {args.runs}íšŒ")
    print(f"   - í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {', '.join(args.modes)}")
    print()

    # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ë²¤ì¹˜ë§ˆí¬
    all_benchmarks = []

    for img_idx, image in enumerate(images, 1):
        if len(images) > 1:
            print(f"\nì´ë¯¸ì§€ {img_idx}/{len(images)} í…ŒìŠ¤íŠ¸ ì¤‘...")

        benchmarks = []

        for mode in args.modes:
            bench = benchmark_api(mode, image, runs=args.runs)
            benchmarks.append(bench)

        all_benchmarks.append(benchmarks)

    # í‰ê·  ë²¤ì¹˜ë§ˆí¬ ê³„ì‚° (ë°°ì¹˜ ëª¨ë“œ)
    if len(images) > 1:
        avg_benchmarks = []

        for mode_idx, mode in enumerate(args.modes):
            mode_results = [b[mode_idx] for b in all_benchmarks if b[mode_idx] is not None]

            if mode_results:
                avg_bench = {
                    'mode': mode,
                    'name': mode_results[0]['name'],
                    'runs': sum(b['runs'] for b in mode_results),
                    'avg_time': statistics.mean([b['avg_time'] for b in mode_results]),
                    'min_time': min(b['min_time'] for b in mode_results),
                    'max_time': max(b['max_time'] for b in mode_results),
                    'std_time': statistics.mean([b['std_time'] for b in mode_results]),
                    'consistency': statistics.mean([b['consistency'] for b in mode_results]),
                    'cost_per_1k': mode_results[0]['cost_per_1k'],
                    'results': []
                }
                avg_benchmarks.append(avg_bench)
            else:
                avg_benchmarks.append(None)

        benchmarks = avg_benchmarks
    else:
        benchmarks = all_benchmarks[0]

    # ê²°ê³¼ ì¶œë ¥
    print_comparison_table(benchmarks)
    print_winner_summary(benchmarks)

    # ë³´ê³ ì„œ ì €ì¥
    if args.output:
        save_benchmark_report(benchmarks, args.output)

    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()
