"""
05. HuggingFace ìƒíƒœê³„ í™œìš©
Week 2: ë””ì§€í„¸ ì´ë¯¸ì§€ ê¸°ì´ˆì™€ CNN

HuggingFaceì˜ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì„ í™œìš©í•œ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤ìŠµ
"""

from transformers import pipeline
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import requests
from io import BytesIO
import os
import torch

class HuggingFaceModels:
    """HuggingFace ëª¨ë¸ í™œìš© ì‹¤ìŠµ í´ë˜ìŠ¤"""

    def __init__(self):
        self.setup_korean_font()
        print("ğŸ¤— HuggingFace ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.device = 0 if torch.cuda.is_available() else -1
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {'GPU' if self.device == 0 else 'CPU'}")

    def setup_korean_font(self):
        """í•œê¸€ í°íŠ¸ ì„¤ì •"""
        if os.name == 'nt':
            plt.rcParams['font.family'] = 'Malgun Gothic'
        plt.rcParams['axes.unicode_minus'] = False

    def create_sample_image(self):
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±"""
        # ê°„ë‹¨í•œ ë„í˜•ì´ ìˆëŠ” ì´ë¯¸ì§€ ìƒì„±
        img = np.ones((224, 224, 3), dtype=np.uint8) * 255

        # ë¹¨ê°„ ì‚¬ê°í˜•
        img[50:100, 50:100] = [255, 0, 0]

        # íŒŒë€ ì› (ê·¼ì‚¬)
        center = (150, 150)
        radius = 30
        y, x = np.ogrid[:224, :224]
        mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2
        img[mask] = [0, 0, 255]

        # ì´ˆë¡ ì‚¼ê°í˜• (ê·¼ì‚¬)
        for i in range(30):
            for j in range(i):
                if 180+i < 224 and 50+j < 224:
                    img[180+i, 50+j] = [0, 255, 0]
                if 180+i < 224 and 50-j >= 0:
                    img[180+i, 50-j] = [0, 255, 0]

        return Image.fromarray(img)

    def download_sample_image(self, url=None):
        """ì¸í„°ë„·ì—ì„œ ìƒ˜í”Œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        if url is None:
            # ê¸°ë³¸ ìƒ˜í”Œ ì´ë¯¸ì§€ URL (ê³ ì–‘ì´ ì´ë¯¸ì§€)
            url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png"

        try:
            response = requests.get(url)
            img = Image.open(BytesIO(response.content))
            return img
        except:
            print("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±")
            return self.create_sample_image()

    def demonstrate_image_classification(self):
        """5.3 ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤ìŠµ"""
        print("\n=== 5.3 ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤ìŠµ ===")

        # ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ ìƒì„±
        classifier = pipeline(
            "image-classification",
            model="google/vit-base-patch16-224",
            device=self.device
        )

        # ìƒ˜í”Œ ì´ë¯¸ì§€ ì¤€ë¹„
        images = {
            "ìƒ˜í”Œ ì´ë¯¸ì§€": self.create_sample_image(),
            "ë‹¤ìš´ë¡œë“œ ì´ë¯¸ì§€": self.download_sample_image()
        }

        # ì‹œê°í™”
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        for idx, (name, img) in enumerate(images.items()):
            # ë¶„ë¥˜ ìˆ˜í–‰
            results = classifier(img)

            # ì´ë¯¸ì§€ í‘œì‹œ
            axes[idx].imshow(img)
            axes[idx].set_title(name, fontsize=12, fontweight='bold')
            axes[idx].axis('off')

            # ìƒìœ„ 3ê°œ ê²°ê³¼ í‘œì‹œ
            result_text = "ì˜ˆì¸¡ ê²°ê³¼:\n"
            for i, result in enumerate(results[:3]):
                result_text += f"{i+1}. {result['label']}: {result['score']:.2%}\n"

            axes[idx].text(0.5, -0.15, result_text,
                          transform=axes[idx].transAxes,
                          ha='center', va='top', fontsize=10,
                          bbox=dict(boxstyle="round", facecolor='lightyellow'))

        plt.tight_layout()
        plt.savefig('05_image_classification.png', dpi=150, bbox_inches='tight')
        plt.show()

        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ì´ë¯¸ì§€ ë¶„ë¥˜ ê²°ê³¼:")
        for name, img in images.items():
            print(f"\n{name}:")
            results = classifier(img)
            for result in results[:3]:
                print(f"  - {result['label']}: {result['score']:.2%}")

    def demonstrate_object_detection(self):
        """5.4 ê°ì²´ ê²€ì¶œ ì‹¤ìŠµ"""
        print("\n=== 5.4 ê°ì²´ ê²€ì¶œ ì‹¤ìŠµ ===")

        # ê°ì²´ ê²€ì¶œ íŒŒì´í”„ë¼ì¸
        detector = pipeline(
            "object-detection",
            model="facebook/detr-resnet-50",
            device=self.device
        )

        # ìƒ˜í”Œ ì´ë¯¸ì§€
        img = self.download_sample_image()

        # ê°ì²´ ê²€ì¶œ ìˆ˜í–‰
        results = detector(img)

        # ì‹œê°í™”
        fig, ax = plt.subplots(figsize=(10, 8))
        ax.imshow(img)

        # ê²€ì¶œëœ ê°ì²´ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        img_width, img_height = img.size

        for obj in results:
            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
            box = obj['box']
            xmin = box['xmin']
            ymin = box['ymin']
            xmax = box['xmax']
            ymax = box['ymax']

            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            rect = plt.Rectangle(
                (xmin, ymin), xmax - xmin, ymax - ymin,
                linewidth=2, edgecolor='red', facecolor='none'
            )
            ax.add_patch(rect)

            # ë ˆì´ë¸” í‘œì‹œ
            label = f"{obj['label']}: {obj['score']:.2f}"
            ax.text(xmin, ymin - 5, label,
                   bbox=dict(boxstyle="round", facecolor='yellow', alpha=0.7),
                   fontsize=10, fontweight='bold')

        ax.set_title('ê°ì²´ ê²€ì¶œ ê²°ê³¼', fontsize=14, fontweight='bold')
        ax.axis('off')

        plt.tight_layout()
        plt.savefig('05_object_detection.png', dpi=150, bbox_inches='tight')
        plt.show()

        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ¯ ê²€ì¶œëœ ê°ì²´:")
        for obj in results:
            print(f"  - {obj['label']}: {obj['score']:.2%}")
            print(f"    ìœ„ì¹˜: ({obj['box']['xmin']:.0f}, {obj['box']['ymin']:.0f}) "
                  f"- ({obj['box']['xmax']:.0f}, {obj['box']['ymax']:.0f})")

    def demonstrate_image_segmentation(self):
        """5.4 ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤ìŠµ"""
        print("\n=== 5.4 ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤ìŠµ ===")

        # ì„¸ê·¸ë©˜í…Œì´ì…˜ íŒŒì´í”„ë¼ì¸
        segmenter = pipeline(
            "image-segmentation",
            model="facebook/detr-resnet-50-panoptic",
            device=self.device
        )

        # ìƒ˜í”Œ ì´ë¯¸ì§€
        img = self.download_sample_image()

        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
        results = segmenter(img)

        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0, 0].imshow(img)
        axes[0, 0].set_title('ì›ë³¸ ì´ë¯¸ì§€', fontsize=12, fontweight='bold')
        axes[0, 0].axis('off')

        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼
        if results:
            # ì²« ë²ˆì§¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬
            if len(results) > 0:
                mask1 = results[0]['mask']
                axes[0, 1].imshow(mask1, cmap='viridis')
                axes[0, 1].set_title(f"ì„¸ê·¸ë©˜íŠ¸ 1: {results[0].get('label', 'Unknown')}",
                                   fontsize=12, fontweight='bold')
                axes[0, 1].axis('off')

            # ë‘ ë²ˆì§¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ (ìˆëŠ” ê²½ìš°)
            if len(results) > 1:
                mask2 = results[1]['mask']
                axes[1, 0].imshow(mask2, cmap='plasma')
                axes[1, 0].set_title(f"ì„¸ê·¸ë©˜íŠ¸ 2: {results[1].get('label', 'Unknown')}",
                                   fontsize=12, fontweight='bold')
                axes[1, 0].axis('off')

            # ëª¨ë“  ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
            combined_mask = np.zeros_like(np.array(img))
            for i, result in enumerate(results[:3]):  # ìµœëŒ€ 3ê°œ ì„¸ê·¸ë©˜íŠ¸
                mask = np.array(result['mask'])
                if len(mask.shape) == 2:
                    mask = np.stack([mask] * 3, axis=2)
                combined_mask += mask * (i + 1) * 50

            axes[1, 1].imshow(np.array(img))
            axes[1, 1].imshow(combined_mask, alpha=0.5)
            axes[1, 1].set_title('í†µí•© ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼', fontsize=12, fontweight='bold')
            axes[1, 1].axis('off')

        plt.tight_layout()
        plt.savefig('05_image_segmentation.png', dpi=150, bbox_inches='tight')
        plt.show()

        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ¯ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼:")
        for i, result in enumerate(results):
            label = result.get('label', f'Segment {i+1}')
            print(f"  - {label}: ë§ˆìŠ¤í¬ í¬ê¸° {np.array(result['mask']).shape}")

    def demonstrate_feature_extraction(self):
        """ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì‹¤ìŠµ"""
        print("\n=== ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì‹¤ìŠµ ===")

        from transformers import AutoFeatureExtractor, AutoModel

        # ëª¨ë¸ê³¼ íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë“œ
        model_name = "google/vit-base-patch16-224"
        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)

        # ìƒ˜í”Œ ì´ë¯¸ì§€
        img = self.create_sample_image()

        # íŠ¹ì§• ì¶”ì¶œ
        inputs = feature_extractor(images=img, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)

        # íŠ¹ì§• ë²¡í„°
        features = outputs.last_hidden_state
        pooled_features = outputs.pooler_output

        # ì‹œê°í™”
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0].imshow(img)
        axes[0].set_title('ì›ë³¸ ì´ë¯¸ì§€', fontsize=12, fontweight='bold')
        axes[0].axis('off')

        # íŠ¹ì§• ë§µ (ì²« ë²ˆì§¸ íŒ¨ì¹˜ë“¤)
        feature_map = features[0, 1:, :].reshape(14, 14, -1)
        axes[1].imshow(feature_map[:, :, 0], cmap='viridis')
        axes[1].set_title('íŠ¹ì§• ë§µ (14x14 íŒ¨ì¹˜)', fontsize=12, fontweight='bold')
        axes[1].axis('off')

        # í’€ë§ëœ íŠ¹ì§• ë²¡í„°
        axes[2].bar(range(min(50, pooled_features.shape[1])),
                   pooled_features[0, :50].numpy())
        axes[2].set_title('íŠ¹ì§• ë²¡í„° (ì²˜ìŒ 50ì°¨ì›)', fontsize=12, fontweight='bold')
        axes[2].set_xlabel('ì°¨ì›')
        axes[2].set_ylabel('ê°’')

        plt.tight_layout()
        plt.savefig('05_feature_extraction.png', dpi=150, bbox_inches='tight')
        plt.show()

        print(f"\nğŸ“ íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼:")
        print(f"  - íŠ¹ì§• ë§µ í¬ê¸°: {features.shape}")
        print(f"  - í’€ë§ëœ íŠ¹ì§• í¬ê¸°: {pooled_features.shape}")
        print(f"  - íŠ¹ì§• ë²¡í„° ì°¨ì›: {pooled_features.shape[1]}")

    def compare_models(self):
        """ë‹¤ì–‘í•œ ëª¨ë¸ ë¹„êµ"""
        print("\n=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===")

        # ë¹„êµí•  ëª¨ë¸ë“¤
        models = {
            "ViT": "google/vit-base-patch16-224",
            "ResNet": "microsoft/resnet-50",
            "EfficientNet": "timm/efficientnet_b0"
        }

        # ìƒ˜í”Œ ì´ë¯¸ì§€
        img = self.download_sample_image()

        results_dict = {}

        print("\nğŸ”„ ëª¨ë¸ë³„ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        for model_name, model_id in models.items():
            try:
                classifier = pipeline(
                    "image-classification",
                    model=model_id,
                    device=self.device
                )
                results = classifier(img)
                results_dict[model_name] = results[:3]
                print(f"  âœ… {model_name} ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {model_name} ì‹¤íŒ¨: {e}")
                results_dict[model_name] = []

        # ê²°ê³¼ ë¹„êµ ì¶œë ¥
        print("\nğŸ“Š ëª¨ë¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ:")
        for model_name, results in results_dict.items():
            print(f"\n{model_name}:")
            for result in results:
                print(f"  - {result['label']}: {result['score']:.2%}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ¤— 05. HuggingFace ìƒíƒœê³„ í™œìš©")
    print("=" * 60)

    hf_models = HuggingFaceModels()

    # 5.3 ì´ë¯¸ì§€ ë¶„ë¥˜
    hf_models.demonstrate_image_classification()

    # 5.4 ê°ì²´ ê²€ì¶œ
    hf_models.demonstrate_object_detection()

    # 5.5 ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜
    hf_models.demonstrate_image_segmentation()

    # íŠ¹ì§• ì¶”ì¶œ
    hf_models.demonstrate_feature_extraction()

    # ëª¨ë¸ ë¹„êµ
    hf_models.compare_models()

    print("\n" + "=" * 60)
    print("âœ… 05. HuggingFace ëª¨ë¸ ì‹¤ìŠµ ì™„ë£Œ!")
    print("ìƒì„±ëœ íŒŒì¼:")
    print("  - 05_image_classification.png")
    print("  - 05_object_detection.png")
    print("  - 05_image_segmentation.png")
    print("  - 05_feature_extraction.png")
    print("=" * 60)

if __name__ == "__main__":
    main()