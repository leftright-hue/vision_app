"""
Week 2 CNN ëª¨ë“ˆ
CNNê³¼ ì´ë¯¸ì§€ ì²˜ë¦¬ ê´€ë ¨ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import streamlit as st
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
import cv2
import matplotlib.pyplot as plt

from core.base_processor import BaseImageProcessor
from core.ai_models import AIModelManager
from .filters import ImageFilters


class InstagramFilterMaker:
    """ğŸ­ ë‚˜ë§Œì˜ Instagram í•„í„° ì œì‘ì†Œ - Streamlit ë²„ì „"""
    
    def __init__(self):
        self.current_image = None
        self.original_image = None
    
    def load_image_from_upload(self, uploaded_file):
        """ğŸ“¸ ì—…ë¡œë“œëœ ì‚¬ì§„ ë¶ˆëŸ¬ì˜¤ê¸°"""
        try:
            # PIL Imageë¡œ ë³€í™˜
            pil_image = Image.open(uploaded_file)
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (BGR)
            opencv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            
            self.current_image = opencv_image
            self.original_image = opencv_image.copy()
            return True
        except Exception as e:
            st.error(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_image_info(self):
        """ğŸ” ì‚¬ì§„ ì •ë³´ ë°˜í™˜"""
        if self.current_image is None:
            return None
        
        height, width = self.current_image.shape[:2]
        channels = self.current_image.shape[2] if len(self.current_image.shape) == 3 else 1
        
        info = {
            'width': width,
            'height': height,
            'channels': channels,
            'total_pixels': width * height,
            'min_value': self.current_image.min(),
            'max_value': self.current_image.max(),
            'mean_brightness': self.current_image.mean()
        }
        return info
    
    def display_color_spaces(self):
        """ğŸŒˆ ìƒ‰ê¹”ì˜ ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ ë³´ê¸°"""
        if self.current_image is None:
            return None
        
        # ìƒ‰ìƒ ê³µê°„ ë³€í™˜
        img_rgb = cv2.cvtColor(self.current_image, cv2.COLOR_BGR2RGB)
        img_hsv = cv2.cvtColor(self.current_image, cv2.COLOR_BGR2HSV)
        img_lab = cv2.cvtColor(self.current_image, cv2.COLOR_BGR2LAB)
        img_gray = cv2.cvtColor(self.current_image, cv2.COLOR_BGR2GRAY)
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('ğŸŒˆ ìƒ‰ê¹”ì˜ 6ê°€ì§€ ë³€ì‹  - ê°™ì€ ì‚¬ì§„, ë‹¤ë¥¸ í‘œí˜„!', fontsize=16)
        
        # ì›ë³¸ (RGB)
        axes[0, 0].imshow(img_rgb)
        axes[0, 0].set_title('ğŸŒŸ RGB ì›ë³¸\n(ìš°ë¦¬ê°€ ë³´ëŠ” ê·¸ëŒ€ë¡œ)')
        axes[0, 0].axis('off')
        
        # HSV
        axes[0, 1].imshow(img_hsv)
        axes[0, 1].set_title('ğŸ¨ HSV\n(ìƒ‰ê¹”+ì§„í•˜ê¸°+ë°ê¸°)')
        axes[0, 1].axis('off')
        
        # LAB
        axes[0, 2].imshow(img_lab)
        axes[0, 2].set_title('ğŸ”¬ LAB\n(ê³¼í•™ì ë°©ì‹)')
        axes[0, 2].axis('off')
        
        # Grayscale
        axes[1, 0].imshow(img_gray, cmap='gray')
        axes[1, 0].set_title('ğŸ¬ í‘ë°±\n(ì˜›ë‚  ì˜í™”ì²˜ëŸ¼)')
        axes[1, 0].axis('off')
        
        # RGB ì±„ë„ ë¶„ë¦¬
        r, g, b = img_rgb[:, :, 0], img_rgb[:, :, 1], img_rgb[:, :, 2]
        rgb_combined = np.hstack([r, g, b])
        axes[1, 1].imshow(rgb_combined, cmap='gray')
        axes[1, 1].set_title('ğŸ”´ğŸŸ¢ğŸ”µ RGB ì±„ë„\n(ë¹¨ê°•|ì´ˆë¡|íŒŒë‘)')
        axes[1, 1].axis('off')
        
        # HSV ì±„ë„ ë¶„ë¦¬
        h, s, v = img_hsv[:, :, 0], img_hsv[:, :, 1], img_hsv[:, :, 2]
        hsv_combined = np.hstack([h, s, v])
        axes[1, 2].imshow(hsv_combined, cmap='gray')
        axes[1, 2].set_title('ğŸŒˆğŸ’ªâ˜€ï¸ HSV ì±„ë„\n(ìƒ‰ê¹”|ì§„í•˜ê¸°|ë°ê¸°)')
        axes[1, 2].axis('off')
        
        plt.tight_layout()
        return fig
    
    def apply_convolution_filters(self):
        """ğŸ­ Instagram í•„í„° 6ì¢… ì„¸íŠ¸"""
        if self.current_image is None:
            return None, None
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(self.current_image, cv2.COLOR_BGR2GRAY)
        
        # ë‹¤ì–‘í•œ í•„í„° ì •ì˜
        filters = {
            'ğŸŒŸ ì›ë³¸': np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),
            'âœ¨ ì„ ëª…': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),
            'ğŸ’« ëª½í™˜': np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) / 9,
            'ğŸ” ê²½ê³„': np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),
            'ğŸ“ ì„¸ë¡œì„ ': np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),
            'ğŸ“ ê°€ë¡œì„ ': np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])
        }
        
        # ê²°ê³¼ ì‹œê°í™”
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('ğŸ­ Instagram í•„í„° 6ì¢… ì„¸íŠ¸ - ì–´ë–¤ ê²Œ ê°€ì¥ ë©‹ìˆë‚˜ìš”?', fontsize=16)
        
        axes = axes.flatten()
        results = {}
        
        for i, (name, kernel) in enumerate(filters.items()):
            if 'ì›ë³¸' in name:
                result = gray
            else:
                result = cv2.filter2D(gray, -1, kernel)
            
            results[name] = result
            axes[i].imshow(result, cmap='gray')
            axes[i].set_title(f'{name}')
            axes[i].axis('off')
            
            # ê° í•„í„°ì˜ íŠ¹ì§• ì„¤ëª…
            descriptions = {
                'ğŸŒŸ ì›ë³¸': 'ì›ë˜ ëª¨ìŠµ',
                'âœ¨ ì„ ëª…': 'ë˜ë ·í•˜ê²Œ!',
                'ğŸ’« ëª½í™˜': 'ë¶€ë“œëŸ½ê²Œ~',
                'ğŸ” ê²½ê³„': 'ìœ¤ê³½ì„ ë§Œ!',
                'ğŸ“ ì„¸ë¡œì„ ': 'ì„¸ë¡œ ê°•ì¡°',
                'ğŸ“ ê°€ë¡œì„ ': 'ê°€ë¡œ ê°•ì¡°'
            }
            
            if name in descriptions:
                axes[i].text(0.5, -0.1, descriptions[name], 
                           transform=axes[i].transAxes, ha='center', fontsize=10)
        
        plt.tight_layout()
        return fig, results
    
    def edge_detection_comparison(self):
        """ğŸ•µï¸ íƒì •ê²Œì„: ì‚¬ì§„ ì† ê²½ê³„ì„  ì°¾ê¸° ëŒ€ê²°!"""
        if self.current_image is None:
            return None
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(self.current_image, cv2.COLOR_BGR2GRAY)
        
        # ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•œ ë¸”ëŸ¬ë§
        blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)
        
        # ë‹¤ì–‘í•œ ì—£ì§€ ê²€ì¶œ ë°©ë²•
        sobel_x = cv2.Sobel(blurred, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(blurred, cv2.CV_64F, 0, 1, ksize=3)
        sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)
        
        laplacian = cv2.Laplacian(blurred, cv2.CV_64F)
        canny = cv2.Canny(blurred, 50, 150)
        
        # ê²°ê³¼ ì‹œê°í™”
        images = [gray, sobel_x, sobel_y, sobel_combined, laplacian, canny]
        titles = ['ğŸ“· ì›ë³¸', 'ğŸ“ ì„¸ë¡œíƒì •', 'ğŸ“ ê°€ë¡œíƒì •', 'ğŸ¯ í•©ì²´íƒì •', 'ğŸ’« ì „ë°©í–¥íƒì •', 'ğŸ† ì²œì¬íƒì •']
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('ğŸ•µï¸ê²½ê³„ì„  ì°¾ê¸° íƒì • ëŒ€íšŒ - ëˆ„ê°€ ê°€ì¥ ì˜ ì°¾ì„ê¹Œìš”?', fontsize=16)
        
        axes = axes.flatten()
        
        for i, (img, title) in enumerate(zip(images, titles)):
            axes[i].imshow(img, cmap='gray')
            axes[i].set_title(title)
            axes[i].axis('off')
        
        plt.tight_layout()
        return fig

class CNNModule(BaseImageProcessor):
    """CNN ëª¨ë“ˆ í´ë˜ìŠ¤"""

    def __init__(self):
        super().__init__()
        self.ai_manager = AIModelManager()
        self.filters = ImageFilters()

    def render(self):
        """Week 2 ëª¨ë“ˆ ë Œë”ë§ - ë©”ì¸ ë©”ì„œë“œ"""
        self.render_ui()

    def render_ui(self)
        """Week 2 ëª¨ë“ˆ UI ë Œë”ë§"""
        st.title("ğŸ§  Week 2: CNNê³¼ ë””ì§€í„¸ ì´ë¯¸ì§€")
        st.markdown("---")

        # íƒ­ ìƒì„±
        tabs = st.tabs([
            "ğŸ“š ì´ë¡ ",
            "ğŸ”¬ ì´ë¯¸ì§€ ê¸°ì´ˆ",
            "ğŸ¨ í•„í„°ë§",
            "ğŸ¤– CNN ì‹œê°í™”",
            "ğŸš€ HuggingFace",
            "ğŸ“Š í†µí•© ë¶„ì„",
            "ğŸ­ Instagram í•„í„°"
        ])

        with tabs[0]:
            self._render_theory_tab()

        with tabs[1]:
            self._render_image_basics_tab()

        with tabs[2]:
            self._render_filtering_tab()

        with tabs[3]:
            self._render_cnn_visualization_tab()

        with tabs[4]:
            self._render_huggingface_tab()

        with tabs[5]:
            self._render_integrated_analysis_tab()

        with tabs[6]:
            self._render_instagram_filter_tab()

    def _render_theory_tab(self):
        """ì´ë¡  íƒ­"""
        st.header("ğŸ“– CNN ì´ë¡ ")

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("1. ë””ì§€í„¸ ì´ë¯¸ì§€ì˜ êµ¬ì¡°")
            st.markdown("""
            - **í”½ì…€**: ì´ë¯¸ì§€ì˜ ê¸°ë³¸ ë‹¨ìœ„
            - **ì±„ë„**: RGB, ê·¸ë ˆì´ìŠ¤ì¼€ì¼
            - **í•´ìƒë„**: ê°€ë¡œ Ã— ì„¸ë¡œ í”½ì…€ ìˆ˜
            - **ë¹„íŠ¸ ê¹Šì´**: ìƒ‰ìƒ í‘œí˜„ ëŠ¥ë ¥
            """)

            st.subheader("2. Convolution ì—°ì‚°")
            st.markdown("""
            - **ì»¤ë„/í•„í„°**: íŠ¹ì§• ì¶”ì¶œ ë„êµ¬
            - **ìŠ¤íŠ¸ë¼ì´ë“œ**: ì»¤ë„ ì´ë™ ê°„ê²©
            - **íŒ¨ë”©**: ê²½ê³„ ì²˜ë¦¬ ë°©ë²•
            - **íŠ¹ì§• ë§µ**: Convolution ê²°ê³¼
            """)

        with col2:
            st.subheader("3. CNN êµ¬ì¡°")
            st.markdown("""
            - **Convolutional Layer**: íŠ¹ì§• ì¶”ì¶œ
            - **Activation (ReLU)**: ë¹„ì„ í˜•ì„±
            - **Pooling Layer**: í¬ê¸° ì¶•ì†Œ
            - **Fully Connected**: ìµœì¢… ë¶„ë¥˜
            """)

            st.subheader("4. ì£¼ìš” ì•„í‚¤í…ì²˜")
            st.markdown("""
            - **LeNet-5** (1998): ìµœì´ˆì˜ CNN
            - **AlexNet** (2012): GPU í™œìš©
            - **VGGNet** (2014): ê¹Šì€ ë„¤íŠ¸ì›Œí¬
            - **ResNet** (2015): Skip Connection
            """)

    def _render_image_basics_tab(self):
        """ì´ë¯¸ì§€ ê¸°ì´ˆ íƒ­"""
        st.header("ğŸ”¬ ë””ì§€í„¸ ì´ë¯¸ì§€ ê¸°ì´ˆ")

        # íƒ­ ìƒì„±: ì˜ˆì œ í•™ìŠµê³¼ ì‚¬ìš©ì ì´ë¯¸ì§€ ë¶„ì„
        sub_tabs = st.tabs(["ğŸ“š ì˜ˆì œë¡œ í•™ìŠµí•˜ê¸°", "ğŸ” ë‚´ ì´ë¯¸ì§€ ë¶„ì„í•˜ê¸°"])

        with sub_tabs[0]:
            st.markdown("### 1. í”½ì…€ê³¼ ì´ë¯¸ì§€ ë°°ì—´")

            # í”½ì…€ ë°°ì—´ ì‹œê°í™”
            col1, col2 = st.columns(2)

            with col1:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ ì˜ˆì œ
                grayscale_example = np.array([
                    [0,   50,  100, 150, 200],
                    [10,  60,  110, 160, 210],
                    [20,  70,  120, 170, 220],
                    [30,  80,  130, 180, 230],
                    [40,  90,  140, 190, 255]
                ], dtype=np.uint8)

                st.image(grayscale_example, caption="5x5 ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€", use_container_width=True)
                st.caption("ğŸ’¡ **ì„¤ëª…**: ê° í”½ì…€ì€ 0(ê²€ì •)~255(í°ìƒ‰) ì‚¬ì´ì˜ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.")
                st.code(f"Shape: {grayscale_example.shape}\nDtype: {grayscale_example.dtype}")

            with col2:
                # RGB ì»¬ëŸ¬ ì´ë¯¸ì§€ ì˜ˆì œ - í¬ê¸°ë¥¼ í‚¤ì›Œì„œ ì˜ ë³´ì´ë„ë¡
                color_example = np.array([
                    [[255, 0, 0], [0, 255, 0]],    # ë¹¨ê°•, ì´ˆë¡
                    [[0, 0, 255], [255, 255, 255]]  # íŒŒë‘, í°ìƒ‰
                ], dtype=np.uint8)

                # ì´ë¯¸ì§€ë¥¼ í¬ê²Œ í™•ëŒ€ (2x2 -> 100x100 í”½ì…€ë¡œ)
                from PIL import Image as PILImage
                color_pil = PILImage.fromarray(color_example, mode='RGB')
                color_pil_resized = color_pil.resize((100, 100), PILImage.Resampling.NEAREST)

                st.image(color_pil_resized, caption="2x2 RGB ì»¬ëŸ¬ ì´ë¯¸ì§€ (í™•ëŒ€)", use_container_width=True)
                st.caption("ğŸ’¡ **ì„¤ëª…**: RGB ì´ë¯¸ì§€ëŠ” 3ê°œ ì±„ë„(R, G, B)ì˜ ì¡°í•©ì…ë‹ˆë‹¤.")
                st.code(f"ì›ë³¸ Shape: {color_example.shape}\nDtype: {color_example.dtype}")

                # ìƒ‰ìƒ ì„¤ëª… ì¶”ê°€
                st.markdown("""
                **í”½ì…€ ìƒ‰ìƒ**:
                - ì¢Œìƒë‹¨: ğŸ”´ ë¹¨ê°• (255, 0, 0)
                - ìš°ìƒë‹¨: ğŸŸ¢ ì´ˆë¡ (0, 255, 0)
                - ì¢Œí•˜ë‹¨: ğŸ”µ íŒŒë‘ (0, 0, 255)
                - ìš°í•˜ë‹¨: âšª í°ìƒ‰ (255, 255, 255)
                """)

            st.markdown("---")
            st.markdown("### 2. ìƒ‰ìƒ ê³µê°„ ë³€í™˜")

            # ìƒ‰ìƒ ê·¸ë¼ë°ì´ì…˜ ìƒ˜í”Œ ìƒì„±
            size = 100
            gradient_img = np.zeros((size, size, 3), dtype=np.uint8)
            for i in range(size):
                for j in range(size):
                    gradient_img[i, j, 0] = int(255 * i / size)  # R
                    gradient_img[i, j, 1] = int(255 * j / size)  # G
                    gradient_img[i, j, 2] = int(255 * (1 - i/size))  # B

            # ìƒ‰ìƒ ê³µê°„ ë³€í™˜
            hsv_img = cv2.cvtColor(gradient_img, cv2.COLOR_RGB2HSV)
            gray_img = cv2.cvtColor(gradient_img, cv2.COLOR_RGB2GRAY)

            col1, col2, col3 = st.columns(3)

            with col1:
                st.image(gradient_img, caption="RGB ì›ë³¸", use_container_width=True)
                st.caption("ğŸ’¡ 3ê°œ ì±„ë„ (R, G, B)")

            with col2:
                st.image(hsv_img, caption="HSV ìƒ‰ìƒê³µê°„", use_container_width=True)
                st.caption("ğŸ’¡ ìƒ‰ìƒ, ì±„ë„, ëª…ë„")

            with col3:
                st.image(gray_img, caption="ê·¸ë ˆì´ìŠ¤ì¼€ì¼", use_container_width=True)
                st.caption("ğŸ’¡ ë‹¨ì¼ ì±„ë„ (ë°ê¸°)")

            st.markdown("---")
            st.markdown("### 3. ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ë³¸ ì—°ì‚°")

            # ê¸°ë³¸ ì—°ì‚° ì˜ˆì œ
            original = np.full((50, 50), 128, dtype=np.uint8)

            operations = {
                "ì›ë³¸": original,
                "ë°ê¸° +50": np.clip(original + 50, 0, 255).astype(np.uint8),
                "ë°ê¸° -50": np.clip(original - 50, 0, 255).astype(np.uint8),
                "ëŒ€ë¹„ x1.5": np.clip(original * 1.5, 0, 255).astype(np.uint8),
                "ëŒ€ë¹„ x0.5": np.clip(original * 0.5, 0, 255).astype(np.uint8),
                "ê°ë§ˆ 2.2": (np.power(original / 255.0, 2.2) * 255).astype(np.uint8)
            }

            cols = st.columns(3)
            for idx, (name, img) in enumerate(operations.items()):
                with cols[idx % 3]:
                    st.image(img, caption=name, use_container_width=True)
                    st.caption(f"í”½ì…€ ê°’: {img[0, 0]}")

            st.markdown("---")
            st.markdown("### 4. ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ì™€ ì†ì„±")

            # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± ë° ë©”íƒ€ë°ì´í„° í‘œì‹œ
            sample_meta_img = gradient_img

            col1, col2, col3 = st.columns(3)

            with col1:
                st.write("**ğŸ“Š ê¸°ë³¸ ì†ì„±**")
                st.code(f"""
í•´ìƒë„: {sample_meta_img.shape[1]}x{sample_meta_img.shape[0]}
ìƒ‰ìƒ ì±„ë„: {sample_meta_img.shape[2]}
ë¹„íŠ¸ ê¹Šì´: 8ë¹„íŠ¸ (0-255)
í”½ì…€ ìˆ˜: {sample_meta_img.shape[0] * sample_meta_img.shape[1]:,}
ë°ì´í„° íƒ€ì…: {sample_meta_img.dtype}
                """, language="text")

            with col2:
                st.write("**ğŸ“· EXIF ì •ë³´ (ì˜ˆì‹œ)**")
                st.code("""
ì¹´ë©”ë¼: Canon EOS R5
ë Œì¦ˆ: 24-70mm f/2.8
ISO: 400
ì…”í„° ì†ë„: 1/125s
ì¡°ë¦¬ê°œ: f/5.6
ì´¬ì˜ ì¼ì‹œ: 2024-01-15
                """, language="text")

            with col3:
                st.write("**ğŸŒ GPS ì •ë³´ (ì˜ˆì‹œ)**")
                st.code("""
ìœ„ë„: 37.5665Â° N
ê²½ë„: 126.9780Â° E
ê³ ë„: 38m
ìœ„ì¹˜: ì„œìš¸, ëŒ€í•œë¯¼êµ­
                """, language="text")

            st.markdown("---")
            st.markdown("### 5. ì´ë¯¸ì§€ ì••ì¶•ê³¼ íŒŒì¼ í˜•ì‹")

            # íŒŒì¼ í˜•ì‹ë³„ ë¹„êµ
            col1, col2 = st.columns(2)

            with col1:
                st.write("**ğŸ“ ë¬´ì†ì‹¤ ì••ì¶• í˜•ì‹**")

                # PNG ì˜ˆì‹œ
                st.markdown("**PNG (Portable Network Graphics)**")
                st.success("""
                âœ… íˆ¬ëª…ë„(ì•ŒíŒŒ ì±„ë„) ì§€ì›
                âœ… ë¬´ì†ì‹¤ ì••ì¶•
                âœ… ì›¹ ê·¸ë˜í”½, ë¡œê³ , ì•„ì´ì½˜ì— ì í•©
                âŒ íŒŒì¼ í¬ê¸°ê°€ í¼
                """)

                # BMP ì˜ˆì‹œ
                st.markdown("**BMP (Bitmap)**")
                st.warning("""
                âœ… ì••ì¶• ì—†ìŒ, ì›ë³¸ í’ˆì§ˆ ìœ ì§€
                âœ… ê°„ë‹¨í•œ êµ¬ì¡°
                âŒ ë§¤ìš° í° íŒŒì¼ í¬ê¸°
                âŒ ì›¹ì—ì„œ ì˜ ì§€ì› ì•ˆ ë¨
                """)

            with col2:
                st.write("**ğŸ“‰ ì†ì‹¤ ì••ì¶• í˜•ì‹**")

                # JPEG ì˜ˆì‹œ
                st.markdown("**JPEG (Joint Photographic Experts Group)**")
                st.info("""
                âœ… ë†’ì€ ì••ì¶•ë¥  (10:1 ~ 20:1)
                âœ… ì‚¬ì§„ì— ìµœì í™”
                âœ… ì‘ì€ íŒŒì¼ í¬ê¸°
                âŒ íˆ¬ëª…ë„ ë¯¸ì§€ì›
                âŒ í…ìŠ¤íŠ¸/ì„  ì´ë¯¸ì§€ì— ë¶€ì í•©
                """)

                # WebP ì˜ˆì‹œ
                st.markdown("**WebP (Google)**")
                st.success("""
                âœ… JPEGë³´ë‹¤ 25-35% ì‘ì€ í¬ê¸°
                âœ… ë¬´ì†ì‹¤/ì†ì‹¤ ëª¨ë‘ ì§€ì›
                âœ… íˆ¬ëª…ë„ ì§€ì›
                âŒ êµ¬í˜• ë¸Œë¼ìš°ì € ë¯¸ì§€ì›
                """)

            # íŒŒì¼ í¬ê¸° ë¹„êµ ì‹œë®¬ë ˆì´ì…˜
            st.markdown("#### ğŸ“Š íŒŒì¼ í¬ê¸° ë¹„êµ (100x100 RGB ì´ë¯¸ì§€)")

            file_sizes = {
                "BMP (ë¬´ì••ì¶•)": 30.0,  # KB
                "PNG (ë¬´ì†ì‹¤)": 12.5,
                "JPEG (í’ˆì§ˆ 90%)": 4.2,
                "JPEG (í’ˆì§ˆ 70%)": 2.1,
                "WebP (ë¬´ì†ì‹¤)": 8.3,
                "WebP (ì†ì‹¤)": 1.8
            }

            import matplotlib.pyplot as plt
            fig, ax = plt.subplots(figsize=(10, 4))
            formats = list(file_sizes.keys())
            sizes = list(file_sizes.values())
            colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dfe6e9']

            bars = ax.barh(formats, sizes, color=colors)
            ax.set_xlabel('íŒŒì¼ í¬ê¸° (KB)')
            ax.set_title('ì´ë¯¸ì§€ í¬ë§·ë³„ íŒŒì¼ í¬ê¸° ë¹„êµ')

            # ë§‰ëŒ€ ìœ„ì— í¬ê¸° í‘œì‹œ
            for bar, size in zip(bars, sizes):
                ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
                       f'{size} KB', va='center')

            plt.tight_layout()
            st.pyplot(fig)

            st.info("""
            ğŸ“– **í•™ìŠµ í¬ì¸íŠ¸**:
            - **í”½ì…€**: ì´ë¯¸ì§€ì˜ ìµœì†Œ ë‹¨ìœ„
            - **ì±„ë„**: ìƒ‰ìƒ ì •ë³´ë¥¼ ë‹´ëŠ” ë ˆì´ì–´ (RGB = 3ì±„ë„)
            - **ìƒ‰ìƒ ê³µê°„**: RGB, HSV, LAB ë“± ìƒ‰ìƒ í‘œí˜„ ë°©ì‹
            - **ë©”íƒ€ë°ì´í„°**: EXIF, GPS ë“± ì¶”ê°€ ì •ë³´
            - **ì••ì¶• ë°©ì‹**: ë¬´ì†ì‹¤ vs ì†ì‹¤ ì••ì¶•ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„
            - **íŒŒì¼ í˜•ì‹ ì„ íƒ**: ìš©ë„ì— ë”°ë¥¸ ìµœì  í¬ë§· ì„ íƒ
            """)

        with sub_tabs[1]:
            st.markdown("### ë‚´ ì´ë¯¸ì§€ë¡œ ì‹¤ìŠµí•˜ê¸°")

            uploaded_file = st.file_uploader(
                "ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)",
                type=['png', 'jpg', 'jpeg'],
                key="basics_upload"
            )

            if uploaded_file:
                image = Image.open(uploaded_file)
                img_array = np.array(image)

                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("ì›ë³¸ ì´ë¯¸ì§€")
                    st.image(image, use_container_width=True)

                    # ì´ë¯¸ì§€ ì •ë³´
                    st.subheader("ğŸ“Š ì´ë¯¸ì§€ ì •ë³´")
                    stats = self.get_image_stats(image)
                    for key, value in stats.items():
                        st.metric(key, value)

                with col2:
                    st.subheader("ì±„ë„ ë¶„ë¦¬")

                    if len(img_array.shape) == 3:
                        # RGB ì±„ë„ ë¶„ë¦¬
                        channels = ['Red', 'Green', 'Blue']
                        for i, channel in enumerate(channels):
                            st.write(f"**{channel} Channel**")
                            st.image(img_array[:, :, i], use_container_width=True)
                    else:
                        st.write("**Grayscale Image**")
                        st.image(img_array, use_container_width=True)
            else:
                st.info("ğŸ‘† ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì§ì ‘ ë¶„ì„í•´ë³´ì„¸ìš”!")

    def _render_filtering_tab(self):
        """í•„í„°ë§ íƒ­"""
        st.header("ğŸ¨ ì´ë¯¸ì§€ í•„í„°ë§")

        # íƒ­ ìƒì„±: ì˜ˆì œ í•™ìŠµê³¼ ì‚¬ìš©ì ì´ë¯¸ì§€ ë¶„ì„
        sub_tabs = st.tabs(["ğŸ“š í•„í„° ê°œë… í•™ìŠµ", "ğŸ” ë‚´ ì´ë¯¸ì§€ì— í•„í„° ì ìš©"])

        with sub_tabs[0]:
            st.markdown("### Convolution ì—°ì‚°ê³¼ í•„í„°")

            # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± (ì²´ì»¤ë³´ë“œ íŒ¨í„´)
            size = 100
            checkerboard = np.zeros((size, size), dtype=np.uint8)
            for i in range(0, size, 20):
                for j in range(0, size, 20):
                    if (i//20 + j//20) % 2 == 0:
                        checkerboard[i:i+20, j:j+20] = 255

            # í•„í„°ë³„ ê²°ê³¼ ë³´ì—¬ì£¼ê¸°
            st.markdown("#### ì£¼ìš” í•„í„°ì™€ íš¨ê³¼")

            filter_examples = {
                'Blur': ('í‰ê·  í•„í„°ë¡œ ë…¸ì´ì¦ˆ ì œê±°', np.ones((5, 5)) / 25),
                'Sharpen': ('ì´ë¯¸ì§€ ì„ ëª…ë„ ì¦ê°€', np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])),
                'Edge Detection': ('ê²½ê³„ì„  ê²€ì¶œ', np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]))
            }

            for filter_name, (description, kernel) in filter_examples.items():
                st.markdown(f"**{filter_name} - {description}**")

                cols = st.columns([1, 1, 2])

                with cols[0]:
                    st.image(checkerboard, caption="ì›ë³¸", use_container_width=True)

                with cols[1]:
                    filtered = cv2.filter2D(checkerboard, -1, kernel)
                    st.image(filtered, caption=f"{filter_name} ì ìš©", use_container_width=True)

                with cols[2]:
                    st.code(f"ì»¤ë„:\n{kernel}", language='python')
                    st.caption(f"ğŸ’¡ {description}")

                st.markdown("---")

            st.markdown("### Convolution ì—°ì‚° ê³¼ì •")

            # Convolution ê³¼ì • ì„¤ëª…
            st.info("""
            **Convolution ì—°ì‚° ë‹¨ê³„**:
            1. **ì»¤ë„ ë°°ì¹˜**: ì´ë¯¸ì§€ ìœ„ì— ì»¤ë„ì„ ë†“ìŠµë‹ˆë‹¤
            2. **ì›ì†Œë³„ ê³±ì…ˆ**: ê²¹ì¹˜ëŠ” í”½ì…€ê³¼ ì»¤ë„ ê°’ì„ ê³±í•©ë‹ˆë‹¤
            3. **í•©ì‚°**: ëª¨ë“  ê³±ì…ˆ ê²°ê³¼ë¥¼ ë”í•©ë‹ˆë‹¤
            4. **ì´ë™**: ì»¤ë„ì„ ë‹¤ìŒ ìœ„ì¹˜ë¡œ ì´ë™í•©ë‹ˆë‹¤
            5. **ë°˜ë³µ**: ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•´ ë°˜ë³µí•©ë‹ˆë‹¤
            """)

            # ì‹œê°ì  ì˜ˆì œ
            example_img = np.array([
                [10, 20, 30],
                [40, 50, 60],
                [70, 80, 90]
            ], dtype=np.uint8)

            example_kernel = np.array([
                [1, 0, -1],
                [2, 0, -2],
                [1, 0, -1]
            ])

            col1, col2, col3 = st.columns(3)

            with col1:
                st.write("**ì…ë ¥ ì´ë¯¸ì§€**")
                st.code(str(example_img))

            with col2:
                st.write("**Sobel X ì»¤ë„**")
                st.code(str(example_kernel))

            with col3:
                st.write("**ê²°ê³¼**")
                result = cv2.filter2D(example_img, -1, example_kernel)
                st.code(str(result))

            st.success("""
            ğŸ“– **í•™ìŠµ í¬ì¸íŠ¸**:
            - **ì»¤ë„/í•„í„°**: íŠ¹ì • íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì‘ì€ í–‰ë ¬
            - **Convolution**: ì»¤ë„ì„ ì´ë¯¸ì§€ì— ì ìš©í•˜ëŠ” ì—°ì‚°
            - **ìš©ë„**: ë¸”ëŸ¬, ìƒ¤í”„ë‹, ì—£ì§€ ê²€ì¶œ, ë…¸ì´ì¦ˆ ì œê±° ë“±
            - **CNN ì—°ê²°**: CNNì˜ í•©ì„±ê³±ì¸µì€ ì´ ì›ë¦¬ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“  ê²ƒ!
            """)

        with sub_tabs[1]:
            st.markdown("### ë‚´ ì´ë¯¸ì§€ì— í•„í„° ì ìš©í•˜ê¸°")

            uploaded_file = st.file_uploader(
                "ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)",
                type=['png', 'jpg', 'jpeg'],
                key="filter_upload"
            )

            if uploaded_file:
                image = Image.open(uploaded_file)

                # í•„í„° ì„ íƒ
                filter_name = st.selectbox(
                    "í•„í„° ì„ íƒ",
                    list(self.filters.filters.keys())
                )

                # í•„í„° ì ìš©
                filtered_image = self.filters.apply_filter(image, filter_name)

                # ê²°ê³¼ í‘œì‹œ
                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("ì›ë³¸")
                    st.image(image, use_container_width=True)

                with col2:
                    st.subheader(f"{filter_name} ì ìš©")
                    st.image(filtered_image, use_container_width=True)

                # í•„í„° ì •ë³´ í‘œì‹œ
                if filter_name != 'None':
                    with st.expander("í•„í„° ìƒì„¸ ì •ë³´"):
                        kernel = self.filters.filters[filter_name]
                        st.code(str(kernel), language='python')

                        filter_info = self.filters.get_filter_info(filter_name)
                        if filter_info.get('description'):
                            st.write(f"**ì„¤ëª…**: {filter_info['description']}")
                        if filter_info.get('kernel_size'):
                            st.write(f"**ì»¤ë„ í¬ê¸°**: {filter_info['kernel_size']}")
            else:
                # ê¸°ë³¸ ì˜ˆì œ ì´ë¯¸ì§€ ì œê³µ
                st.info("ğŸ‘† ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì•„ë˜ ì˜ˆì œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”!")

                if st.button("ì˜ˆì œ ì´ë¯¸ì§€ ì‚¬ìš©"):
                    # ê·¸ë¼ë°ì´ì…˜ ì´ë¯¸ì§€ ìƒì„±
                    size = 200
                    example_img = np.zeros((size, size, 3), dtype=np.uint8)
                    for i in range(size):
                        for j in range(size):
                            example_img[i, j] = [i * 255 // size, j * 255 // size, 128]

                    st.session_state['example_image'] = Image.fromarray(example_img)
                    st.rerun()

    def _render_instagram_filter_tab(self):
        """ğŸ­ Instagram í•„í„° ì œì‘ì†Œ íƒ­"""
        st.header("ğŸ¨ Instagram í•„í„° ì œì‘ì†Œ")
        st.markdown("### ë”¥ëŸ¬ë‹ ì˜ìƒì²˜ë¦¬ - Week 2 ì‹¤ìŠµ")
        
        st.markdown("""
        ğŸ“± **ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” Instagram, Snapchat ê°™ì€ í•„í„°ë¥¼ ì§ì ‘ ë§Œë“¤ì–´ë³¼ ê±°ì˜ˆìš”!**
        
        ğŸ¯ **í•™ìŠµ ëª©í‘œ:**
        - ğŸ“· ë””ì§€í„¸ ì‚¬ì§„ì´ ì»´í“¨í„°ì—ì„œ ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ ì²´í—˜í•˜ê¸°
        - ğŸ­ Instagramì²˜ëŸ¼ ë‹¤ì–‘í•œ í•„í„° íš¨ê³¼ ì§ì ‘ ë§Œë“¤ì–´ë³´ê¸°
        - ğŸ” ì»´í“¨í„°ê°€ ì‚¬ì§„ì—ì„œ ë¬¼ì²´ì˜ ê²½ê³„ë¥¼ ì°¾ëŠ” ë°©ë²• ì´í•´í•˜ê¸°
        """)
        
        # Instagram í•„í„° ì œì‘ì†Œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        if 'instagram_filter_maker' not in st.session_state:
            st.session_state.instagram_filter_maker = InstagramFilterMaker()
        
        filter_maker = st.session_state.instagram_filter_maker
        
        # ì„œë¸Œ íƒ­ ìƒì„±
        sub_tabs = st.tabs([
            "ğŸ“¸ ì‚¬ì§„ ì—…ë¡œë“œ", 
            "ğŸŒˆ ìƒ‰ê¹” ë³€ì‹ ì‡¼", 
            "ğŸ­ í•„í„° ì²´í—˜", 
            "ğŸ•µï¸ ê²½ê³„ì„  íƒì •", 
            "ğŸ® ì—°ìŠµë¬¸ì œ"
        ])
        
        with sub_tabs[0]:
            st.header("ğŸ“¸ ì‚¬ì§„ ì—…ë¡œë“œ")
            
            uploaded_file = st.file_uploader(
                "ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                type=['png', 'jpg', 'jpeg'],
                help="PNG, JPG, JPEG í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤",
                key="instagram_upload"
            )
            
            if uploaded_file is not None:
                if filter_maker.load_image_from_upload(uploaded_file):
                    st.success("âœ… ì‚¬ì§„ ì—…ë¡œë“œ ì„±ê³µ!")
                    
                    # ì´ë¯¸ì§€ í‘œì‹œ
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("ğŸ–¼ï¸ ì—…ë¡œë“œëœ ì‚¬ì§„")
                        # BGRì„ RGBë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
                        if filter_maker.current_image is not None:
                            display_image = cv2.cvtColor(filter_maker.current_image, cv2.COLOR_BGR2RGB)
                            st.image(display_image, use_column_width=True)
                    
                    with col2:
                        st.subheader("ğŸ” ì‚¬ì§„ ì •ë³´")
                        info = filter_maker.get_image_info()
                        if info:
                            st.write(f"ğŸ“ í¬ê¸°: {info['width']} Ã— {info['height']} í”½ì…€")
                            st.write(f"ğŸ¨ ìƒ‰ìƒ ì±„ë„: {info['channels']}ê°œ")
                            st.write(f"ğŸ“Š í”½ì…€ê°’ ë²”ìœ„: {info['min_value']} ~ {info['max_value']}")
                            st.write(f"âœ¨ í‰ê·  ë°ê¸°: {info['mean_brightness']:.1f}/255")
                            
                            # ë°ê¸° íŒì •
                            avg_brightness = info['mean_brightness']
                            if avg_brightness > 180:
                                st.write("â˜€ï¸ ë§¤ìš° ë°ì€ ì‚¬ì§„ì´ì—ìš”!")
                            elif avg_brightness > 120:
                                st.write("ğŸŒ¤ï¸ ì ë‹¹íˆ ë°ì€ ì‚¬ì§„ì´ì—ìš”!")
                            elif avg_brightness > 60:
                                st.write("ğŸŒ¥ï¸ ì¡°ê¸ˆ ì–´ë‘ìš´ ì‚¬ì§„ì´ì—ìš”!")
                            else:
                                st.write("ğŸŒ™ ë§¤ìš° ì–´ë‘ìš´ ì‚¬ì§„ì´ì—ìš”!")
        
        with sub_tabs[1]:
            st.header("ğŸŒˆ ìƒ‰ê¹”ì˜ ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹")
            
            if filter_maker.current_image is not None:
                if st.button("ğŸ­ ìƒ‰ê¹” ë³€ì‹ ì‡¼ ì‹œì‘!", key="color_transform"):
                    with st.spinner("ğŸŒˆ ìƒ‰ê¹” ë³€ì‹  ì¤‘..."):
                        fig = filter_maker.display_color_spaces()
                        if fig:
                            st.pyplot(fig)
                            st.markdown("""
                            **ğŸ¨ ìƒ‰ê¹” í‘œí˜„ ë°©ì‹ ì„¤ëª…:**
                            - **RGB**: ë¹¨ê°•(Red) + ì´ˆë¡(Green) + íŒŒë‘(Blue) ì¡°í•©
                            - **HSV**: ìƒ‰ì¡°(Hue) + ì±„ë„(Saturation) + ëª…ë„(Value)
                            - **LAB**: ê³¼í•™ì  ìƒ‰ìƒ í‘œí˜„ ë°©ì‹
                            - **í‘ë°±**: ìƒ‰ìƒ ì •ë³´ ì œê±°, ë°ê¸°ë§Œ ìœ ì§€
                            """)
            else:
                st.info("ë¨¼ì € ğŸ“¸ ì‚¬ì§„ ì—…ë¡œë“œ íƒ­ì—ì„œ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
        
        with sub_tabs[2]:
            st.header("ğŸ­ Instagram í•„í„° 6ì¢… ì„¸íŠ¸")
            
            if filter_maker.current_image is not None:
                if st.button("ğŸª í•„í„° ë§ˆìˆ ì‡¼ ì‹œì‘!", key="filter_show"):
                    with st.spinner("ğŸ­ í•„í„° ì ìš© ì¤‘..."):
                        fig, results = filter_maker.apply_convolution_filters()
                        if fig:
                            st.pyplot(fig)
                            
                            st.markdown("""
                            **ğŸ¨ í•„í„° ì„¤ëª…:**
                            - **âœ¨ ì„ ëª…**: ì´ë¯¸ì§€ì˜ ê²½ê³„ë¥¼ ë” ë˜ë ·í•˜ê²Œ ë§Œë“¦
                            - **ğŸ’« ëª½í™˜**: ì´ë¯¸ì§€ë¥¼ ë¶€ë“œëŸ½ê²Œ ë¸”ëŸ¬ ì²˜ë¦¬
                            - **ğŸ” ê²½ê³„**: ë¬¼ì²´ì˜ ìœ¤ê³½ì„ ë§Œ ì¶”ì¶œ
                            - **ğŸ“ ì„¸ë¡œì„ **: ì„¸ë¡œ ë°©í–¥ ê²½ê³„ ê°•ì¡°
                            - **ğŸ“ ê°€ë¡œì„ **: ê°€ë¡œ ë°©í–¥ ê²½ê³„ ê°•ì¡°
                            """)
            else:
                st.info("ë¨¼ì € ğŸ“¸ ì‚¬ì§„ ì—…ë¡œë“œ íƒ­ì—ì„œ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
        
        with sub_tabs[3]:
            st.header("ğŸ•µï¸ ê²½ê³„ì„  ì°¾ê¸° íƒì • ëŒ€íšŒ")
            
            if filter_maker.current_image is not None:
                if st.button("ğŸ” íƒì • ëŒ€íšŒ ì‹œì‘!", key="edge_detection"):
                    with st.spinner("ğŸ•µï¸ ê²½ê³„ì„  ì°¾ëŠ” ì¤‘..."):
                        fig = filter_maker.edge_detection_comparison()
                        if fig:
                            st.pyplot(fig)
                            
                            st.markdown("""
                            **ğŸ•µï¸ íƒì • ë°©ë²• ì„¤ëª…:**
                            - **ğŸ“ ì„¸ë¡œíƒì • (Sobel X)**: ì„¸ë¡œ ë°©í–¥ ê²½ê³„ì„  ì „ë¬¸
                            - **ğŸ“ ê°€ë¡œíƒì • (Sobel Y)**: ê°€ë¡œ ë°©í–¥ ê²½ê³„ì„  ì „ë¬¸  
                            - **ğŸ¯ í•©ì²´íƒì •**: ì„¸ë¡œ+ê°€ë¡œ íƒì •ì˜ í•©ì²´
                            - **ğŸ’« ì „ë°©í–¥íƒì • (Laplacian)**: ëª¨ë“  ë°©í–¥ ë™ì‹œ íƒì§€
                            - **ğŸ† ì²œì¬íƒì • (Canny)**: ê°€ì¥ ì •í™•í•œ ê²½ê³„ì„  ê²€ì¶œ
                            """)
            else:
                st.info("ë¨¼ì € ğŸ“¸ ì‚¬ì§„ ì—…ë¡œë“œ íƒ­ì—ì„œ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
        
        with sub_tabs[4]:
            st.header("ğŸ® ì—°ìŠµë¬¸ì œ")
            
            # ì—°ìŠµë¬¸ì œ 1: ê¸°ë³¸ ì—°ì‚°
            st.subheader("ğŸ¨ ì—°ìŠµë¬¸ì œ 1: ë””ì§€í„¸ ì•„íŠ¸ ì°½ì‘")
            
            ex1_col1, ex1_col2, ex1_col3 = st.columns(3)
            
            with ex1_col1:
                st.write("**ğŸ ì²´ìŠ¤íŒ ë§Œë“¤ê¸°**")
                size = st.slider("ì²´ìŠ¤íŒ í¬ê¸°", 50, 200, 100, key="instagram_checkerboard_size")
                square_size = st.slider("ì‚¬ê°í˜• í¬ê¸°", 5, 20, 10, key="instagram_square_size")
                
                if st.button("ì²´ìŠ¤íŒ ìƒì„±", key="instagram_create_checkerboard"):
                    checkerboard = self._create_checkerboard(size, square_size)
                    st.image(checkerboard, caption="ìƒì„±ëœ ì²´ìŠ¤íŒ")
            
            with ex1_col2:
                st.write("**â˜€ï¸ ë°ê¸° ì¡°ì ˆ**")
                if filter_maker.current_image is not None:
                    brightness_factor = st.slider("ë°ê¸° ì¡°ì ˆ", 0.1, 3.0, 1.0, 0.1, key="instagram_brightness")
                    
                    if st.button("ë°ê¸° ì ìš©", key="instagram_apply_brightness"):
                        adjusted = self._adjust_brightness(filter_maker.current_image, brightness_factor)
                        display_adjusted = cv2.cvtColor(adjusted, cv2.COLOR_BGR2RGB)
                        st.image(display_adjusted, caption=f"ë°ê¸° {brightness_factor}x")
                else:
                    st.info("ì‚¬ì§„ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”")
            
            with ex1_col3:
                st.write("**ğŸŒªï¸ ì´ë¯¸ì§€ íšŒì „**")
                if filter_maker.current_image is not None:
                    rotation = st.selectbox("íšŒì „ ê°ë„", [0, 90, 180, 270], key="instagram_rotation")
                    
                    if st.button("íšŒì „ ì ìš©", key="instagram_apply_rotation"):
                        rotated = self._rotate_image(filter_maker.current_image, rotation)
                        display_rotated = cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB)
                        st.image(display_rotated, caption=f"{rotation}ë„ íšŒì „")
                else:
                    st.info("ì‚¬ì§„ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”")
            
            st.markdown("---")
            
            # ì—°ìŠµë¬¸ì œ 2: íŠ¹ìˆ˜ íš¨ê³¼
            st.subheader("ğŸ¬ ì—°ìŠµë¬¸ì œ 2: íŠ¹ìˆ˜ íš¨ê³¼ ì œì‘ì†Œ")
            
            ex2_col1, ex2_col2 = st.columns(2)
            
            with ex2_col1:
                st.write("**ğŸº ì— ë³´ì‹± íš¨ê³¼**")
                if filter_maker.current_image is not None and st.button("ì— ë³´ì‹± ì ìš©", key="instagram_apply_emboss"):
                    embossed = self._emboss_filter(filter_maker.current_image)
                    st.image(embossed, caption="ì— ë³´ì‹± íš¨ê³¼", cmap='gray')
                elif filter_maker.current_image is None:
                    st.info("ì‚¬ì§„ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”")
            
            with ex2_col2:
                st.write("**ğŸ’« ëª¨ì…˜ ë¸”ëŸ¬**")
                if filter_maker.current_image is not None:
                    blur_size = st.slider("ë¸”ëŸ¬ ê°•ë„", 5, 25, 15, key="instagram_blur_size")
                    blur_angle = st.selectbox("ë¸”ëŸ¬ ë°©í–¥", [0, 45, 90], 
                                            format_func=lambda x: f"{x}ë„ ({'ìˆ˜í‰' if x==0 else 'ëŒ€ê°ì„ ' if x==45 else 'ìˆ˜ì§'})",
                                            key="instagram_blur_angle")
                    
                    if st.button("ëª¨ì…˜ ë¸”ëŸ¬ ì ìš©", key="instagram_apply_motion_blur"):
                        motion_blurred = self._motion_blur_filter(filter_maker.current_image, blur_size, blur_angle)
                        st.image(motion_blurred, caption="ëª¨ì…˜ ë¸”ëŸ¬ íš¨ê³¼", cmap='gray')
                else:
                    st.info("ì‚¬ì§„ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”")

    def _create_checkerboard(self, size=100, square_size=10):
        """ğŸ ì²´ìŠ¤íŒ ë§Œë“¤ê¸°"""
        checkerboard = np.zeros((size, size), dtype=np.uint8)
        for i in range(0, size, square_size):
            for j in range(0, size, square_size):
                if (i // square_size + j // square_size) % 2 == 0:
                    checkerboard[i:i+square_size, j:j+square_size] = 255
        return checkerboard

    def _adjust_brightness(self, image, factor):
        """â˜€ï¸ ì‚¬ì§„ ë°ê¸° ì¡°ì ˆ"""
        adjusted = image.astype(np.float32) * factor
        adjusted = np.clip(adjusted, 0, 255)
        return adjusted.astype(np.uint8)

    def _rotate_image(self, image, angle_90):
        """ğŸŒªï¸ ì´ë¯¸ì§€ íšŒì „ (90ë„ ë‹¨ìœ„)"""
        if angle_90 == 90:
            return cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
        elif angle_90 == 180:
            return cv2.rotate(image, cv2.ROTATE_180)
        elif angle_90 == 270:
            return cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)
        else:
            return image

    def _emboss_filter(self, image):
        """ğŸº ì— ë³´ì‹± í•„í„°"""
        kernel = np.array([[-2, -1, 0],
                           [-1, 1, 1],
                           [0, 1, 2]])
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        return cv2.filter2D(gray, -1, kernel)

    def _motion_blur_filter(self, image, size=15, angle=45):
        """ğŸ’« ëª¨ì…˜ ë¸”ëŸ¬ í•„í„°"""
        kernel = np.zeros((size, size))
        if angle == 0:  # ìˆ˜í‰
            kernel[size//2, :] = 1
        elif angle == 45:  # ëŒ€ê°ì„ 
            np.fill_diagonal(kernel, 1)
        elif angle == 90:  # ìˆ˜ì§
            kernel[:, size//2] = 1
        
        kernel = kernel / kernel.sum()
        
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        return cv2.filter2D(gray, -1, kernel)

    def _render_cnn_visualization_tab(self):
        """CNN ì‹œê°í™” íƒ­"""
        st.header("ğŸ¤– CNN ì‹œê°í™”")

        # íƒ­ ìƒì„±
        sub_tabs = st.tabs(["ğŸ“š CNN êµ¬ì¡° ì´í•´í•˜ê¸°", "ğŸ”¬ íŠ¹ì§• ë§µ ì‹œê°í™”"])

        with sub_tabs[0]:
            st.markdown("### CNNì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ")

            # CNN êµ¬ì¡° ì„¤ëª…
            st.markdown("""
            #### 1ï¸âƒ£ **Convolutional Layer (í•©ì„±ê³±ì¸µ)**
            """)

            col1, col2 = st.columns(2)

            with col1:
                # í•©ì„±ê³± ì—°ì‚° ì˜ˆì œ
                conv_input = np.random.randn(5, 5) * 50 + 128
                conv_input = np.clip(conv_input, 0, 255).astype(np.uint8)
                conv_kernel = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])
                conv_output = cv2.filter2D(conv_input, -1, conv_kernel)

                st.image(conv_input, caption="ì…ë ¥ (5x5)", width=150)
                st.caption("ğŸ’¡ ì…ë ¥ ì´ë¯¸ì§€ ë˜ëŠ” ì´ì „ ì¸µì˜ íŠ¹ì§• ë§µ")

            with col2:
                st.image(conv_output, caption="í•©ì„±ê³± ê²°ê³¼ (5x5)", width=150)
                st.caption("ğŸ’¡ ì—£ì§€, í…ìŠ¤ì²˜ ë“± íŠ¹ì§• ì¶”ì¶œ")

            st.info("""
            **í•µì‹¬ ê°œë…**:
            - **í•„í„°/ì»¤ë„**: í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬
            - **íŠ¹ì§• ì¶”ì¶œ**: ì´ë¯¸ì§€ì—ì„œ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ í•™ìŠµ
            - **íŒŒë¼ë¯¸í„° ê³µìœ **: ê°™ì€ í•„í„°ë¥¼ ì „ì²´ ì´ë¯¸ì§€ì— ì ìš©
            """)

            st.markdown("""
            #### 2ï¸âƒ£ **Activation Function (í™œì„±í™” í•¨ìˆ˜)**
            """)

            # ReLU ì‹œê°í™”
            x = np.linspace(-5, 5, 100)
            relu_y = np.maximum(0, x)

            col1, col2 = st.columns(2)

            with col1:
                st.write("**ReLU (Rectified Linear Unit)**")
                st.line_chart({"x": x, "ReLU(x)": relu_y})
                st.caption("ğŸ’¡ ìŒìˆ˜ë¥¼ 0ìœ¼ë¡œ, ì–‘ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ")

            with col2:
                st.write("**ReLUì˜ íš¨ê³¼**")
                st.markdown("""
                - âœ… ê³„ì‚° íš¨ìœ¨ì 
                - âœ… ë¹„ì„ í˜•ì„± ì¶”ê°€
                - âœ… ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ì™„í™”
                - âœ… í¬ì†Œì„± ìœ ë„
                """)

            st.markdown("""
            #### 3ï¸âƒ£ **Pooling Layer (í’€ë§ì¸µ)**
            """)

            # Max Pooling ì˜ˆì œ
            pool_input = np.array([
                [1, 2, 3, 4],
                [5, 6, 7, 8],
                [9, 10, 11, 12],
                [13, 14, 15, 16]
            ], dtype=np.float32)

            # 2x2 Max Pooling
            pool_output = np.array([
                [np.max(pool_input[0:2, 0:2]), np.max(pool_input[0:2, 2:4])],
                [np.max(pool_input[2:4, 0:2]), np.max(pool_input[2:4, 2:4])]
            ])

            col1, col2 = st.columns(2)

            with col1:
                st.write("**ì…ë ¥ (4x4)**")
                st.code(str(pool_input.astype(int)))

            with col2:
                st.write("**Max Pooling 2x2 ê²°ê³¼**")
                st.code(str(pool_output.astype(int)))
                st.caption("ğŸ’¡ í¬ê¸° ì¶•ì†Œ + ì¤‘ìš” íŠ¹ì§• ë³´ì¡´")

            st.markdown("""
            #### 4ï¸âƒ£ **Fully Connected Layer (ì™„ì „ì—°ê²°ì¸µ)**
            """)

            st.info("""
            **ì—­í• **:
            - ì¶”ì¶œëœ íŠ¹ì§•ì„ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë³€í™˜
            - ë¶„ë¥˜ ì‘ì—…: í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ ì¶œë ¥ ë‰´ëŸ°
            - íšŒê·€ ì‘ì—…: 1ê°œ ë˜ëŠ” ì—¬ëŸ¬ ê°œ ì¶œë ¥ ê°’
            """)

            # CNN ì „ì²´ êµ¬ì¡°
            st.markdown("### ì „ì²´ CNN ì•„í‚¤í…ì²˜")

            st.code("""
            ì…ë ¥ ì´ë¯¸ì§€ (28x28x1)
                â†“
            Conv2d(1, 16, 3) â†’ (26x26x16)
                â†“
            ReLU() â†’ (26x26x16)
                â†“
            MaxPool2d(2) â†’ (13x13x16)
                â†“
            Conv2d(16, 32, 3) â†’ (11x11x32)
                â†“
            ReLU() â†’ (11x11x32)
                â†“
            MaxPool2d(2) â†’ (5x5x32)
                â†“
            Flatten() â†’ (800,)
                â†“
            Linear(800, 10) â†’ (10,)
                â†“
            ì¶œë ¥ (10ê°œ í´ë˜ìŠ¤)
            """, language="text")

            st.success("""
            ğŸ“– **í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸**:
            - **ê³„ì¸µì  íŠ¹ì§• í•™ìŠµ**: ë‚®ì€ ì¸µ â†’ ë‹¨ìˆœ íŠ¹ì§•, ë†’ì€ ì¸µ â†’ ë³µì¡í•œ íŠ¹ì§•
            - **íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±**: í•©ì„±ê³±ê³¼ í’€ë§ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œ
            - **ìœ„ì¹˜ ë¶ˆë³€ì„±**: í’€ë§ì„ í†µí•œ ì‘ì€ ì´ë™ì— ëŒ€í•œ ê°•ê±´ì„±
            - **ìë™ íŠ¹ì§• ì¶”ì¶œ**: ìˆ˜ë™ íŠ¹ì§• ì„¤ê³„ ë¶ˆí•„ìš”
            """)

        with sub_tabs[1]:
            st.markdown("### íŠ¹ì§• ë§µ ì‹œê°í™”")

            # ê°„ë‹¨í•œ CNN ëª¨ë¸ ìƒì„±
            model = self._create_simple_cnn()

            st.info("""
            **íŠ¹ì§• ë§µ(Feature Map)**ì´ë€?
            - CNNì˜ ê° ì¸µì´ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ íŠ¹ì§•ë“¤
            - ì´ˆê¸° ì¸µ: ì—£ì§€, ìƒ‰ìƒ ë“± ë‹¨ìˆœ íŠ¹ì§•
            - ê¹Šì€ ì¸µ: í…ìŠ¤ì²˜, íŒ¨í„´ ë“± ë³µì¡í•œ íŠ¹ì§•
            """)

            # íŠ¹ì§• ë§µ ìƒì„± ì˜µì…˜
            col1, col2 = st.columns(2)

            with col1:
                input_type = st.radio(
                    "ì…ë ¥ ì„ íƒ",
                    ["ëœë¤ ë…¸ì´ì¦ˆ", "ìˆ«ì íŒ¨í„´", "ì²´ì»¤ë³´ë“œ"]
                )

            with col2:
                if st.button("íŠ¹ì§• ë§µ ìƒì„±", key="generate_features"):
                    # ì…ë ¥ í…ì„œ ìƒì„±
                    if input_type == "ëœë¤ ë…¸ì´ì¦ˆ":
                        input_tensor = torch.randn(1, 1, 28, 28)
                    elif input_type == "ìˆ«ì íŒ¨í„´":
                        # ê°„ë‹¨í•œ ìˆ«ì 7 íŒ¨í„´
                        pattern = np.zeros((28, 28), dtype=np.float32)
                        pattern[5:8, 8:20] = 1.0  # ìœ„ ê°€ë¡œì„ 
                        pattern[8:20, 17:20] = 1.0  # ëŒ€ê°ì„ 
                        input_tensor = torch.tensor(pattern).unsqueeze(0).unsqueeze(0)
                    else:  # ì²´ì»¤ë³´ë“œ
                        pattern = np.zeros((28, 28), dtype=np.float32)
                        for i in range(0, 28, 4):
                            for j in range(0, 28, 4):
                                if (i//4 + j//4) % 2 == 0:
                                    pattern[i:i+4, j:j+4] = 1.0
                        input_tensor = torch.tensor(pattern).unsqueeze(0).unsqueeze(0)

                    # íŠ¹ì§• ì¶”ì¶œ
                    features = self._extract_features(model, input_tensor)

                    # ì…ë ¥ í‘œì‹œ
                    st.subheader("ì…ë ¥ ì´ë¯¸ì§€")
                    input_img = input_tensor[0, 0].numpy()
                    # ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
                    input_img = (input_img - input_img.min()) / (input_img.max() - input_img.min() + 1e-8)
                    st.image(input_img, caption="ì…ë ¥ (28x28)", width=200)

                    # ê° ë ˆì´ì–´ì˜ íŠ¹ì§• ë§µ í‘œì‹œ
                    st.subheader("ê° ë ˆì´ì–´ì˜ íŠ¹ì§• ë§µ")

                    for name, feature in features.items():
                        st.write(f"**{name}** - Shape: {tuple(feature.shape)}")

                        # íŠ¹ì§• ë§µ ì‹œê°í™”
                        if len(feature.shape) == 4:
                            # ì²˜ìŒ 4ê°œ ì±„ë„ë§Œ í‘œì‹œ
                            cols = st.columns(min(4, feature.shape[1]))
                            for i in range(min(4, feature.shape[1])):
                                with cols[i]:
                                    img = feature[0, i].detach().numpy()
                                    # ì •ê·œí™”
                                    img = (img - img.min()) / (img.max() - img.min() + 1e-8)
                                    st.image(img, caption=f"ì±„ë„ {i+1}", use_container_width=True)

                            if feature.shape[1] > 4:
                                st.caption(f"... ì´ {feature.shape[1]}ê°œ ì±„ë„ ì¤‘ 4ê°œë§Œ í‘œì‹œ")

                        st.markdown("---")

    def _render_huggingface_tab(self):
        """HuggingFace íƒ­"""
        st.header("ğŸš€ HuggingFace ëª¨ë¸")

        uploaded_file = st.file_uploader(
            "ì´ë¯¸ì§€ ì—…ë¡œë“œ",
            type=['png', 'jpg', 'jpeg'],
            key="hf_upload"
        )

        if uploaded_file:
            image = Image.open(uploaded_file)
            st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_container_width=True)

            # ëª¨ë¸ ì„ íƒ
            task = st.selectbox(
                "ì‘ì—… ì„ íƒ",
                ["ì´ë¯¸ì§€ ë¶„ë¥˜", "ê°ì²´ ê²€ì¶œ", "ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜"]
            )

            if task == "ì´ë¯¸ì§€ ë¶„ë¥˜":
                if st.button("ë¶„ë¥˜ ì‹¤í–‰", key="classify"):
                    with st.spinner("ë¶„ë¥˜ ì¤‘..."):
                        results = self.ai_manager.classify_image(image)
                        st.success("ë¶„ë¥˜ ì™„ë£Œ!")

                        for result in results[:5]:
                            st.write(f"**{result['label']}**: {result['score']:.2%}")

            elif task == "ê°ì²´ ê²€ì¶œ":
                if st.button("ê²€ì¶œ ì‹¤í–‰", key="detect"):
                    with st.spinner("ê²€ì¶œ ì¤‘..."):
                        results = self.ai_manager.detect_objects(image)
                        st.success(f"{len(results)}ê°œ ê°ì²´ ê²€ì¶œ!")

                        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                        img_with_boxes = self.ai_manager.draw_detection_boxes(image, results)
                        st.image(img_with_boxes, caption="ê²€ì¶œ ê²°ê³¼", use_container_width=True)

                        for obj in results:
                            st.write(f"- **{obj['label']}**: {obj['score']:.2%}")

            elif task == "ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜":
                if st.button("ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤í–‰", key="segment"):
                    with st.spinner("ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¤‘..."):
                        results = self.ai_manager.segment_image(image)
                        st.success("ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ!")

                        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ í‘œì‹œ
                        if results:
                            for i, result in enumerate(results[:3]):  # ìµœëŒ€ 3ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ í‘œì‹œ
                                label = result.get('label', f'Segment {i+1}')
                                st.write(f"**{label}**: ë§ˆìŠ¤í¬ í¬ê¸° {np.array(result['mask']).shape}")

                                # ë§ˆìŠ¤í¬ ì‹œê°í™”
                                mask = np.array(result['mask'])
                                if len(mask.shape) == 3:
                                    mask = mask[:,:,0]  # ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©
                                st.image(mask, caption=f"{label} ë§ˆìŠ¤í¬", use_container_width=True)
                        else:
                            st.warning("ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def _render_integrated_analysis_tab(self):
        """í†µí•© ë¶„ì„ íƒ­"""
        st.header("ğŸ“Š í†µí•© ì´ë¯¸ì§€ ë¶„ì„")

        uploaded_file = st.file_uploader(
            "ì´ë¯¸ì§€ ì—…ë¡œë“œ",
            type=['png', 'jpg', 'jpeg'],
            key="integrated_upload"
        )

        if uploaded_file:
            image = Image.open(uploaded_file)

            # ë¶„ì„ ì˜µì…˜
            options = st.multiselect(
                "ë¶„ì„ ì˜µì…˜ ì„ íƒ",
                ["ê¸°ë³¸ ì •ë³´", "íˆìŠ¤í† ê·¸ë¨", "í•„í„° ì ìš©", "AI ë¶„ì„", "ì—£ì§€ ê²€ì¶œ"]
            )

            if st.button("í†µí•© ë¶„ì„ ì‹¤í–‰", key="run_integrated"):
                results = {}

                if "ê¸°ë³¸ ì •ë³´" in options:
                    results["stats"] = self.get_image_stats(image)

                if "íˆìŠ¤í† ê·¸ë¨" in options:
                    results["histogram"] = self.calculate_histogram(np.array(image))

                if "í•„í„° ì ìš©" in options:
                    results["filtered"] = {
                        name: self.filters.apply_filter(image, name)
                        for name in ["Blur", "Edge Detection", "Sharpen"]
                    }

                if "AI ë¶„ì„" in options:
                    with st.spinner("AI ë¶„ì„ ì¤‘..."):
                        results["classification"] = self.ai_manager.classify_image(image)[:3]

                if "ì—£ì§€ ê²€ì¶œ" in options:
                    gray = self.convert_to_grayscale(image)
                    results["edges"] = cv2.Canny(gray, 50, 150)

                # ê²°ê³¼ í‘œì‹œ
                self._display_integrated_results(image, results, options)

    def _display_integrated_results(self, image, results, options):
        """í†µí•© ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        st.success("ë¶„ì„ ì™„ë£Œ!")

        if "ê¸°ë³¸ ì •ë³´" in options:
            st.subheader("ğŸ“Š ê¸°ë³¸ ì •ë³´")
            cols = st.columns(4)
            for idx, (key, value) in enumerate(results["stats"].items()):
                cols[idx % 4].metric(key, value)

        if "íˆìŠ¤í† ê·¸ë¨" in options:
            st.subheader("ğŸ“ˆ íˆìŠ¤í† ê·¸ë¨")
            import matplotlib.pyplot as plt
            fig, ax = plt.subplots(figsize=(10, 4))
            for channel, hist in results["histogram"].items():
                ax.plot(hist, label=channel, alpha=0.7)
            ax.legend()
            ax.set_xlabel("Pixel Value")
            ax.set_ylabel("Frequency")
            st.pyplot(fig)

        if "í•„í„° ì ìš©" in options:
            st.subheader("ğŸ¨ í•„í„° ê²°ê³¼")
            cols = st.columns(len(results["filtered"]))
            for idx, (name, img) in enumerate(results["filtered"].items()):
                cols[idx].write(name)
                cols[idx].image(img, use_container_width=True)

        if "AI ë¶„ì„" in options:
            st.subheader("ğŸ¤– AI ë¶„ë¥˜ ê²°ê³¼")
            for result in results["classification"]:
                st.write(f"- **{result['label']}**: {result['score']:.2%}")

        if "ì—£ì§€ ê²€ì¶œ" in options:
            st.subheader("ğŸ” ì—£ì§€ ê²€ì¶œ")
            st.image(results["edges"], use_container_width=True)

    def _create_simple_cnn(self):
        """ê°„ë‹¨í•œ CNN ëª¨ë¸ ìƒì„±"""
        class SimpleCNN(nn.Module):
            def __init__(self):
                super(SimpleCNN, self).__init__()
                self.conv1 = nn.Conv2d(1, 16, 3, padding=1)
                self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
                self.pool = nn.MaxPool2d(2, 2)
                self.fc = nn.Linear(32 * 7 * 7, 10)

            def forward(self, x):
                x = self.pool(F.relu(self.conv1(x)))
                x = self.pool(F.relu(self.conv2(x)))
                x = x.view(-1, 32 * 7 * 7)
                x = self.fc(x)
                return x

        return SimpleCNN()

    def _extract_features(self, model, input_tensor):
        """íŠ¹ì§• ë§µ ì¶”ì¶œ"""
        features = {}

        x = input_tensor
        x = F.relu(model.conv1(x))
        features['Conv1'] = x.clone()

        x = model.pool(x)
        features['Pool1'] = x.clone()

        x = F.relu(model.conv2(x))
        features['Conv2'] = x.clone()

        x = model.pool(x)
        features['Pool2'] = x.clone()

        return features