"""
Week 3: Transfer Learning & Multi-modal API ëª¨ë“ˆ
Transfer Learningê³¼ Multi-modal API ê´€ë ¨ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import streamlit as st
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from typing import Dict, List, Tuple, Optional
import cv2
import matplotlib.pyplot as plt
import os
import sys

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from core.base_processor import BaseImageProcessor
from core.ai_models import AIModelManager
from .transfer_helpers import TransferLearningHelper
from .multimodal_helpers import MultiModalHelper


class TransferLearningModule(BaseImageProcessor):
    """Transfer Learning ë° Multi-modal API í•™ìŠµ ëª¨ë“ˆ"""

    def __init__(self):
        super().__init__()
        self.ai_manager = AIModelManager()
        self.transfer_helper = TransferLearningHelper()
        self.multimodal_helper = MultiModalHelper()

    def render(self):
        """Week 3 ëª¨ë“ˆ UI ë Œë”ë§ - Week 2ì™€ ë™ì¼í•œ ë©”ì„œë“œëª…"""
        self.render_ui()

    def render_ui(self):
        """Week 3 ëª¨ë“ˆ UI ë Œë”ë§"""
        st.title("ğŸ”„ Week 3: Transfer Learning & Multi-modal API")
        st.markdown("---")

        # íƒ­ ìƒì„±
        tabs = st.tabs([
            "ğŸ“š ì´ë¡ ",
            "ğŸ”„ Transfer Learning",
            "ğŸ–¼ï¸ CLIP ê²€ìƒ‰",
            "ğŸ¨ íŠ¹ì§• ì¶”ì¶œ",
            "ğŸ“Š í†µí•© ë¶„ì„",
            "ğŸš€ ì‹¤ì „ í”„ë¡œì íŠ¸",
            "ğŸ” API ë¹„êµ"
        ])

        with tabs[0]:
            self._render_theory_tab()

        with tabs[1]:
            self._render_transfer_learning_tab()

        with tabs[2]:
            self._render_clip_search_tab()

        with tabs[3]:
            self._render_feature_extraction_tab()

        with tabs[4]:
            self._render_integrated_analysis_tab()

        with tabs[5]:
            self._render_project_tab()

        with tabs[6]:
            self._render_api_comparison_tab()

    def _render_theory_tab(self):
        """ì´ë¡  íƒ­"""
        st.header("ğŸ“– Transfer Learning & Multi-modal ì´ë¡ ")

        # 1. Transfer Learning ì´ë¡ ê³¼ ì‹¤ìŠµ
        with st.expander("ğŸ“š Transfer Learning ì´ë¡ ê³¼ ì‹¤ìŠµ", expanded=True):
            st.markdown("""
            ### ğŸ”„ Transfer Learning
            **ê°œë…**: ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì˜ ì§€ì‹ì„ ìƒˆë¡œìš´ ì‘ì—…ì— ì „ì´í•˜ëŠ” ë°©ë²•ë¡ 

            **í•µì‹¬ ì›ë¦¬**:
            - ì‚¬ì „í•™ìŠµ ëª¨ë¸(ResNet, VGG, EfficientNet ë“±)ì˜ í•™ìŠµëœ íŠ¹ì§•ì„ ì¬í™œìš©
            - ImageNet ê°™ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµí•œ ì¼ë°˜ì  íŠ¹ì§•ì„ í™œìš©
            - ìƒˆë¡œìš´ ì‘ì—…ì— ë§ê²Œ ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ êµì²´í•˜ê±°ë‚˜ ì „ì²´ ëª¨ë¸ì„ ë¯¸ì„¸ì¡°ì •

            **ë‘ ê°€ì§€ ì£¼ìš” ì ‘ê·¼ë²•**:
            1. **Feature Extraction (íŠ¹ì§• ì¶”ì¶œ)**:
               - ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ê³ ì •í•˜ê³  íŠ¹ì§• ì¶”ì¶œê¸°ë¡œë§Œ ì‚¬ìš©
               - ë¹ ë¥´ê³  íš¨ìœ¨ì , ì ì€ ë°ì´í„°ë¡œë„ ê°€ëŠ¥

            2. **Fine-tuning (ë¯¸ì„¸ ì¡°ì •)**:
               - ì‚¬ì „í•™ìŠµ ëª¨ë¸ì˜ ì¼ë¶€ ë˜ëŠ” ì „ì²´ë¥¼ ì¬í•™ìŠµ
               - ë” ë†’ì€ ì„±ëŠ¥, ë§ì€ ë°ì´í„°ì™€ ì‹œê°„ í•„ìš”

            **ì‹¤ì œ í™œìš© ì˜ˆì‹œ**:
            - ImageNet (1000 í´ë˜ìŠ¤) â†’ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜ (2-10 í´ë˜ìŠ¤)
            - ì¼ë°˜ ê°ì²´ ì¸ì‹ â†’ íŠ¹ì • ì œí’ˆ í’ˆì§ˆ ê²€ì‚¬
            - ìì—° ì´ë¯¸ì§€ â†’ ì‚°ì—… ê²°í•¨ ê²€ì¶œ
            """)

        # 2. CLIP ì´ë¯¸ì§€ ê²€ìƒ‰ì´ë€?
        with st.expander("ğŸ–¼ï¸ CLIP ì´ë¯¸ì§€ ê²€ìƒ‰ì´ë€?", expanded=True):
            st.markdown("""
            ### ğŸ¤– CLIP (Contrastive Language-Image Pre-training)
            OpenAIê°€ ê°œë°œí•œ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ë¡œ, ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•©ë‹ˆë‹¤.

            #### ğŸ¯ í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰ ê¸°ëŠ¥
            - **ê°œë…**: ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•˜ëŠ” í˜ì‹ ì ì¸ ê¸°ìˆ 
            - **ì‘ë™ ì›ë¦¬**:
              1. í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (í…ìŠ¤íŠ¸ ì„ë² ë”©)
              2. ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ì„ë² ë”©)
              3. ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
              4. ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ë°˜í™˜

            #### ğŸš€ ê¸°ì¡´ ê²€ìƒ‰ê³¼ì˜ ì°¨ì´ì 
            | ê¸°ì¡´ ê²€ìƒ‰ | CLIP ê²€ìƒ‰ |
            |----------|----------|
            | íƒœê·¸/ë©”íƒ€ë°ì´í„° ê¸°ë°˜ | ì´ë¯¸ì§€ ë‚´ìš© ì§ì ‘ ì´í•´ |
            | ì •í™•í•œ í‚¤ì›Œë“œ í•„ìš” | ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ê°€ëŠ¥ |
            | ë¯¸ë¦¬ ë¼ë²¨ë§ í•„ìš” | ë¼ë²¨ë§ ë¶ˆí•„ìš” |
            | ì œí•œì  ê²€ìƒ‰ | ì°½ì˜ì  ê²€ìƒ‰ ê°€ëŠ¥ |

            #### ğŸ“‹ í™œìš© ì˜ˆì‹œ
            - **ì „ììƒê±°ë˜**: "íŒŒë€ìƒ‰ ìŠ¤íŠ¸ë¼ì´í”„ ì…”ì¸ " â†’ ê´€ë ¨ ì œí’ˆ ì´ë¯¸ì§€
            - **ê°¤ëŸ¬ë¦¬**: "ì¼ëª°ì´ ìˆëŠ” í•´ë³€ í’ê²½" â†’ ê´€ë ¨ ì‚¬ì§„ ê²€ìƒ‰
            - **ì˜ë£Œ**: "íì— ê²°ì ˆì´ ìˆëŠ” X-ray" â†’ ìœ ì‚¬ ì˜ë£Œ ì´ë¯¸ì§€
            - **SNS**: "ê·€ì—¬ìš´ ê°•ì•„ì§€ê°€ ê³µë†€ì´ í•˜ëŠ” ëª¨ìŠµ" â†’ ê´€ë ¨ ê²Œì‹œë¬¼
            """)

        # 3. Transfer Learningê³¼ CLIPì˜ ì°¨ì´ì 
        with st.expander("ğŸ¤” Transfer Learningê³¼ CLIPì˜ ì°¨ì´ì ", expanded=True):
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("""
                ### ğŸ”„ Transfer Learning
                **ë²”ì£¼**: ê¸°ë²•/ë°©ë²•ë¡ 

                **íŠ¹ì§•**:
                - ë‹¤ì–‘í•œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì„ íƒ ê°€ëŠ¥
                - **ë‹¨ì¼ ëª¨ë‹¬**(ì£¼ë¡œ ì´ë¯¸ì§€)ì— ì§‘ì¤‘
                - Fine-tuningì´ë‚˜ Feature Extraction ì‚¬ìš©
                - ëª©ì : ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±

                **í•„ìš”í•œ ê²ƒ**:
                - ë ˆì´ë¸”ëœ íƒ€ê²Ÿ ë°ì´í„°ì…‹
                - ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì„ íƒ
                - GPU (ê¶Œì¥)
                """)

            with col2:
                st.markdown("""
                ### ğŸ¤– CLIP
                **ë²”ì£¼**: íŠ¹ì • ëª¨ë¸

                **íŠ¹ì§•**:
                - OpenAIê°€ ê°œë°œí•œ íŠ¹ì • ëª¨ë¸
                - **ë©€í‹°ëª¨ë‹¬**(ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸) í†µí•©
                - Contrastive Learningìœ¼ë¡œ í•™ìŠµ
                - ëª©ì : Zero-shot ì´ë¯¸ì§€ ì´í•´

                **í•„ìš”í•œ ê²ƒ**:
                - í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
                - CLIP ëª¨ë¸
                - ë ˆì´ë¸” ë¶ˆí•„ìš” (Zero-shot)
                """)

            st.markdown("---")
            st.markdown("""
            ### ğŸ“Š í•µì‹¬ ì°¨ì´ì  ìš”ì•½

            | êµ¬ë¶„ | Transfer Learning | CLIP |
            |------|------------------|------|
            | **ë²”ìœ„** | ê¸°ë²•/ë°©ë²•ë¡  | íŠ¹ì • ëª¨ë¸ |
            | **ëª¨ë‹¬ë¦¬í‹°** | ë‹¨ì¼ (ì´ë¯¸ì§€) | ë©€í‹° (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸) |
            | **í•™ìŠµ ë°©ì‹** | Supervised | Contrastive |
            | **ë°ì´í„° í•„ìš”** | ë ˆì´ë¸”ëœ ë°ì´í„° í•„ìš” | Zero-shot ê°€ëŠ¥ |
            | **í™œìš©** | Fine-tuning í•„ìš” | ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥ |
            | **ìœ ì—°ì„±** | ë‹¤ì–‘í•œ ëª¨ë¸ ì„ íƒ | CLIP ëª¨ë¸ë§Œ |

            ### ğŸ”— ê´€ê³„
            - **CLIPë„ Transfer Learning ê°€ëŠ¥**: CLIPì„ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ Fine-tuning ê°€ëŠ¥
            - **ìƒí˜¸ë³´ì™„ì **: Transfer Learningìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ + CLIPì˜ í…ìŠ¤íŠ¸ ì´í•´ ê²°í•© ê°€ëŠ¥
            - **ì„ íƒ ê¸°ì¤€**:
              - íŠ¹ì • í´ë˜ìŠ¤ ë¶„ë¥˜ â†’ Transfer Learning
              - ìì—°ì–´ ê²€ìƒ‰/ì„¤ëª… â†’ CLIP
            """)

        # 4. íŠ¹ì§• ì¶”ì¶œ(Feature Extraction)ì´ë€?
        with st.expander("ğŸ¨ íŠ¹ì§• ì¶”ì¶œ(Feature Extraction)ì´ë€?", expanded=True):
            st.markdown("""
            ### íŠ¹ì§• ì¶”ì¶œ (Feature Extraction)
            CNNì˜ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ í•™ìŠµëœ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ì‹œê°í™”í•˜ê³  ë¶„ì„í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

            #### â“ ì™œ íŠ¹ì§• ì¶”ì¶œì„ í•˜ëŠ”ê°€?

            **1. ğŸ” ë¸”ë™ë°•ìŠ¤ ë¬¸ì œ í•´ê²°**
            - **ë¬¸ì œ**: CNNì€ ìˆ˜ë°±ë§Œ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ë¸”ë™ë°•ìŠ¤
            - **í•´ê²°**: íŠ¹ì§• ì‹œê°í™”ë¡œ "ê³ ì–‘ì´ë¥¼ ë³´ê³  ë­˜ í•™ìŠµí–ˆëŠ”ì§€" ì§ì ‘ í™•ì¸
            - **ì˜ˆì‹œ**: ê³ ì–‘ì´ ì‚¬ì§„ â†’ ê·€ ëª¨ì–‘, ìˆ˜ì—¼, í„¸ íŒ¨í„´ì„ ê°ì§€í•˜ëŠ”ì§€ ê²€ì¦

            **2. ğŸ’° ë¹„ìš© ì ˆê°ì„ ìœ„í•œ ì¬í™œìš©**
            - **ë¬¸ì œ**: ì²˜ìŒë¶€í„° ëª¨ë¸ í•™ìŠµí•˜ë©´ ìˆ˜ë°±ë§Œì›ì˜ GPU ë¹„ìš©
            - **í•´ê²°**: ImageNetìœ¼ë¡œ í•™ìŠµëœ íŠ¹ì§•ì„ ê·¸ëŒ€ë¡œ ì¬í™œìš©
            - **ì˜ˆì‹œ**: ê³ ì–‘ì´/ê°œ ë¶„ë¥˜ê¸°ì˜ íŠ¹ì§• â†’ í’ˆì¢… ë¶„ë¥˜ì— ì¬ì‚¬ìš©

            **3. ğŸ› ëª¨ë¸ ë””ë²„ê¹…ê³¼ ê°œì„ **
            - **ë¬¸ì œ**: ëª¨ë¸ì´ ì™œ í‹€ë ¸ëŠ”ì§€ ëª¨ë¦„
            - **í•´ê²°**: ì˜ëª» í•™ìŠµëœ íŠ¹ì§• ë°œê²¬í•˜ê³  ìˆ˜ì •
            - **ì˜ˆì‹œ**: ë°°ê²½ë§Œ ë³´ê³  ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ ë°œê²¬ â†’ ë°ì´í„° ê°œì„ 

            #### ğŸ–¼ï¸ ì‹¤ì œ íŠ¹ì§• ì‹œê°í™” ì˜ˆì‹œ (ê³ ì–‘ì´ ì´ë¯¸ì§€)

            | ë ˆì´ì–´ | ë³´ëŠ” ê²ƒ | ì‹¤ì œ ìš©ë„ |
            |--------|---------|----------|
            | **Layer 1-2** | ì„ , ëª¨ì„œë¦¬ | ì»µ í…Œë‘ë¦¬, ê³ ì–‘ì´ ìœ¤ê³½ì„  ê°ì§€ |
            | **Layer 3-4** | ì§ˆê°, íŒ¨í„´ | ê³ ì–‘ì´ í„¸ ë¬´ëŠ¬, ì¹´í« ì§ˆê° ì¸ì‹ |
            | **Layer 5-6** | ë¶€ë¶„ í˜•íƒœ | ê³ ì–‘ì´ ê·€, ì»µ ëª¨ì–‘ íŒŒì•… |
            | **Final** | ì „ì²´ ê°ì²´ | "ê³ ì–‘ì´", "ì¢…ì´ì»µ" ìµœì¢… ì¸ì‹ |

            #### ğŸ’¼ ì‹¤ë¬´ í™œìš© ì‚¬ë¡€

            **1. ì˜ë£Œ AI**
            - X-rayì—ì„œ íë ´ íŠ¹ì§• ì¶”ì¶œ â†’ ì–´ëŠ ë¶€ë¶„ì´ ì´ìƒì¸ì§€ ì˜ì‚¬ì—ê²Œ í‘œì‹œ
            - ì¢…ì–‘ ê²€ì¶œ ëª¨ë¸ â†’ ì–´ë–¤ íŒ¨í„´ì„ ì¢…ì–‘ìœ¼ë¡œ ì¸ì‹í•˜ëŠ”ì§€ ê²€ì¦

            **2. ììœ¨ì£¼í–‰**
            - ë„ë¡œ í‘œì§€íŒ ì¸ì‹ â†’ ì–´ë–¤ íŠ¹ì§•ìœ¼ë¡œ ì •ì§€ ì‹ í˜¸ë¥¼ êµ¬ë¶„í•˜ëŠ”ì§€
            - ë³´í–‰ì ê°ì§€ â†’ ì‚¬ëŒì˜ ì–´ë–¤ íŠ¹ì§•ì„ í•™ìŠµí–ˆëŠ”ì§€ í™•ì¸

            **3. í’ˆì§ˆ ê´€ë¦¬**
            - ì œí’ˆ ê²°í•¨ ê²€ì‚¬ â†’ ìŠ¤í¬ë˜ì¹˜, ì°Œê·¸ëŸ¬ì§ ë“± ê²°í•¨ íŒ¨í„´ í•™ìŠµ í™•ì¸
            - ì •ìƒ/ë¶ˆëŸ‰ íŒë³„ ê·¼ê±°ë¥¼ ì‹œê°ì ìœ¼ë¡œ ì œì‹œ

            **4. Transfer Learning í™œìš©**
            - ImageNet í•™ìŠµ ëª¨ë¸ â†’ ì˜ë£Œ ì´ë¯¸ì§€ì— ì ìš©
            - ì¼ë°˜ íŠ¹ì§•(ì—£ì§€, ì§ˆê°) â†’ íŠ¹ìˆ˜ ëª©ì (ë³‘ë³€, ê²°í•¨)ìœ¼ë¡œ ì „ì´

            #### ğŸ¯ í•µì‹¬ ê°€ì¹˜
            > **"íŠ¹ì§• ì¶”ì¶œ = AIì˜ ì‚¬ê³  ê³¼ì •ì„ ë“¤ì—¬ë‹¤ë³´ëŠ” ì°½"**

            - ë‹¨ìˆœíˆ "ì •í™•ë„ 95%"ê°€ ì•„ë‹Œ, "ì™œ 95%ì¸ì§€" ì„¤ëª… ê°€ëŠ¥
            - ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ì´ìœ ë¡œ íŒë‹¨í•˜ëŠ”ì§€ ê²€ì¦
            - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI êµ¬ì¶•ì˜ í•„ìˆ˜ ìš”ì†Œ
            """)

        # ì„œë¸Œ íƒ­ ìƒì„± (ìƒì„¸ ì´ë¡ )
        st.markdown("---")
        st.subheader("ğŸ“š ìƒì„¸ ì´ë¡  í•™ìŠµ")
        theory_tabs = st.tabs([
            "ğŸ“š Transfer Learning ìƒì„¸",
            "ğŸ¤– CLIP ìƒì„¸",
            "ğŸ”¬ ìˆ˜í•™ì  ê¸°ì´ˆ",
            "ğŸ’¡ ì‹¤ì „ ê°€ì´ë“œ"
        ])

        with theory_tabs[0]:
            self._render_transfer_learning_theory()

        with theory_tabs[1]:
            self._render_clip_theory()

        with theory_tabs[2]:
            self._render_mathematical_foundation()

        with theory_tabs[3]:
            self._render_practical_guide()

    def _render_transfer_learning_theory(self):
        """Transfer Learning ìƒì„¸ ì´ë¡ """
        st.markdown("## ğŸ“š Transfer Learning ì´ë¡ ê³¼ ì‹¤ìŠµ")

        # ê°œë…ê³¼ ë°°ê²½
        with st.expander("### 1. Transfer Learningì˜ íƒ„ìƒ ë°°ê²½ê³¼ í•µì‹¬ ê°œë…", expanded=True):
            st.markdown("""
            #### ğŸŒ± íƒ„ìƒ ë°°ê²½
            - **ë¬¸ì œ**: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì—ëŠ” ë§‰ëŒ€í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì› í•„ìš”
            - **í•´ê²°ì±…**: ì´ë¯¸ í•™ìŠµëœ ì§€ì‹ì„ ì¬í™œìš©í•˜ì!
            - **ì˜ê°**: ì¸ê°„ì˜ í•™ìŠµ ë°©ì‹ (ìì „ê±° â†’ ì˜¤í† ë°”ì´ ìš´ì „)

            #### ğŸ¯ í•µì‹¬ ê°œë…
            **Transfer Learning = ì§€ì‹ ì „ì´ í•™ìŠµ**
            ```
            Source Domain (ì›ì²œ ë„ë©”ì¸) â†’ Target Domain (ëª©í‘œ ë„ë©”ì¸)
            ImageNet 1000ê°œ í´ë˜ìŠ¤    â†’    ê°œ/ê³ ì–‘ì´ 2ê°œ í´ë˜ìŠ¤
            ```

            #### ğŸ“Š ì‘ë™ ì›ë¦¬
            1. **Low-level Features** (í•˜ìœ„ ë ˆì´ì–´)
               - Edge, Corner, Texture ë“± ì¼ë°˜ì  íŠ¹ì§•
               - ëŒ€ë¶€ë¶„ì˜ ì´ë¯¸ì§€ ì‘ì—…ì— ê³µí†µì ìœ¼ë¡œ ìœ ìš©
               - ë³´í†µ ë™ê²°(Freeze)í•˜ì—¬ ì¬ì‚¬ìš©

            2. **High-level Features** (ìƒìœ„ ë ˆì´ì–´)
               - í´ë˜ìŠ¤ë³„ íŠ¹í™”ëœ íŠ¹ì§•
               - Task-specificí•˜ë¯€ë¡œ ì¬í•™ìŠµ í•„ìš”
               - Fine-tuningì˜ ì£¼ìš” ëŒ€ìƒ

            #### ğŸš€ ì™œ íš¨ê³¼ì ì¸ê°€?
            - **Feature Hierarchy**: CNNì€ ê³„ì¸µì  íŠ¹ì§• í•™ìŠµ
            - **Universal Features**: í•˜ìœ„ ë ˆì´ì–´ëŠ” ë²”ìš©ì 
            - **Data Efficiency**: ì ì€ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥
            - **Convergence Speed**: ë¹ ë¥¸ ìˆ˜ë ´
            """)

        # Transfer Learning ë°©ë²•ë¡ 
        with st.expander("### 2. Transfer Learning ë°©ë²•ë¡  ìƒì„¸"):
            st.markdown("""
            #### ğŸ”§ ë°©ë²• 1: Feature Extraction (íŠ¹ì§• ì¶”ì¶œ)
            ```python
            # ëª¨ë“  ë ˆì´ì–´ ë™ê²°
            for param in model.parameters():
                param.requires_grad = False

            # ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ êµì²´
            model.fc = nn.Linear(2048, num_classes)
            ```
            - **ì¥ì **: ë¹ ë¦„, ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ
            - **ë‹¨ì **: ì„±ëŠ¥ í–¥ìƒ ì œí•œì 
            - **ì ìš©**: ë°ì´í„° ë§¤ìš° ì ì„ ë•Œ (< 1000ê°œ)

            #### ğŸ¨ ë°©ë²• 2: Fine-tuning (ë¯¸ì„¸ ì¡°ì •)
            ```python
            # ì´ˆê¸° ë ˆì´ì–´ë§Œ ë™ê²°
            for layer in model.layers[:-5]:
                layer.requires_grad = False

            # ìƒìœ„ ë ˆì´ì–´ëŠ” í•™ìŠµ ê°€ëŠ¥
            for layer in model.layers[-5:]:
                layer.requires_grad = True
            ```
            - **ì¥ì **: ë†’ì€ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥
            - **ë‹¨ì **: ê³¼ì í•© ìœ„í—˜, í•™ìŠµ ì‹œê°„ ì¦ê°€
            - **ì ìš©**: ì¶©ë¶„í•œ ë°ì´í„° (> 5000ê°œ)

            #### ğŸ”„ ë°©ë²• 3: Progressive Fine-tuning
            ```python
            # Step 1: ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ
            train_last_layer(epochs=10)

            # Step 2: ì ì§„ì ìœ¼ë¡œ ë” ë§ì€ ë ˆì´ì–´
            unfreeze_layers(n=2)
            train_model(epochs=5, lr=lr/10)

            # Step 3: ì „ì²´ ë¯¸ì„¸ì¡°ì •
            unfreeze_all()
            train_model(epochs=3, lr=lr/100)
            ```
            - **ì¥ì **: ì•ˆì •ì  í•™ìŠµ, ìµœê³  ì„±ëŠ¥
            - **ë‹¨ì **: ë³µì¡í•œ êµ¬í˜„, ì‹œê°„ ì†Œìš”
            - **ì ìš©**: ì¤‘ìš”í•œ í”„ë¡œì íŠ¸

            #### ğŸ“Š ë°©ë²• ì„ íƒ ê°€ì´ë“œ
            | ë°ì´í„° ì–‘ | ìœ ì‚¬ë„ | ì¶”ì²œ ë°©ë²• |
            |----------|--------|----------|
            | ì ìŒ + ë†’ìŒ | Feature Extraction |
            | ì ìŒ + ë‚®ìŒ | Fine-tuning (ìƒìœ„ ë ˆì´ì–´) |
            | ë§ìŒ + ë†’ìŒ | Fine-tuning (ì „ì²´) |
            | ë§ìŒ + ë‚®ìŒ | ì²˜ìŒë¶€í„° í•™ìŠµ or Progressive |
            """)

        # ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ
        with st.expander("### 3. ì‹¤ì œ êµ¬í˜„ ì½”ë“œ ì˜ˆì œ"):
            st.markdown("""
            #### ğŸ• ì˜ˆì œ: ê°œ í’ˆì¢… ë¶„ë¥˜ê¸° ë§Œë“¤ê¸°

            ```python
            import torch
            import torch.nn as nn
            import torchvision.models as models
            from torch.optim import Adam
            from torch.optim.lr_scheduler import StepLR

            class DogBreedClassifier:
                def __init__(self, num_breeds=120):
                    # 1. ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ
                    self.model = models.resnet50(pretrained=True)

                    # 2. Feature Extraction ì„¤ì •
                    for param in self.model.parameters():
                        param.requires_grad = False

                    # 3. ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ì¶”ê°€
                    num_features = self.model.fc.in_features
                    self.model.fc = nn.Sequential(
                        nn.Linear(num_features, 512),
                        nn.ReLU(),
                        nn.Dropout(0.3),
                        nn.Linear(512, num_breeds)
                    )

                def progressive_unfreeze(self, stage):
                    '''ì ì§„ì  ì–¸í”„ë¦¬ì§•'''
                    if stage == 1:  # ë§ˆì§€ë§‰ ë¸”ë¡ë§Œ
                        for param in self.model.layer4.parameters():
                            param.requires_grad = True
                    elif stage == 2:  # ë§ˆì§€ë§‰ 2ê°œ ë¸”ë¡
                        for param in self.model.layer3.parameters():
                            param.requires_grad = True
                    elif stage == 3:  # ì „ì²´
                        for param in self.model.parameters():
                            param.requires_grad = True

                def train_stage(self, dataloader, stage, epochs):
                    self.progressive_unfreeze(stage)

                    # í•™ìŠµë¥  ì¡°ì • (ì–¸í”„ë¦¬ì§•í• ìˆ˜ë¡ ë‚®ê²Œ)
                    lr = 1e-3 * (0.1 ** (stage - 1))
                    optimizer = Adam(
                        filter(lambda p: p.requires_grad,
                               self.model.parameters()),
                        lr=lr
                    )

                    for epoch in range(epochs):
                        # í•™ìŠµ ì½”ë“œ
                        pass

            # ì‚¬ìš© ì˜ˆì‹œ
            classifier = DogBreedClassifier(num_breeds=120)

            # Stage 1: Feature Extraction (10 epochs)
            classifier.train_stage(dataloader, stage=0, epochs=10)

            # Stage 2: Fine-tune ë§ˆì§€ë§‰ ë¸”ë¡ (5 epochs)
            classifier.train_stage(dataloader, stage=1, epochs=5)

            # Stage 3: Fine-tune ë” ë§ì€ ë ˆì´ì–´ (3 epochs)
            classifier.train_stage(dataloader, stage=2, epochs=3)
            ```
            """)

        # ê³ ê¸‰ ê¸°ë²•
        with st.expander("### 4. Transfer Learning ê³ ê¸‰ ê¸°ë²•"):
            st.markdown("""
            #### ğŸ­ Domain Adaptation (ë„ë©”ì¸ ì ì‘)
            - **ë¬¸ì œ**: Sourceì™€ Target ë„ë©”ì¸ì´ ë„ˆë¬´ ë‹¤ë¦„
            - **í•´ê²°**: Adversarial Training, MMD ë“± í™œìš©
            ```python
            # Domain Adversarial Neural Network (DANN)
            class DANN(nn.Module):
                def __init__(self):
                    self.feature_extractor = ResNet50()
                    self.label_classifier = LabelClassifier()
                    self.domain_classifier = DomainClassifier()
                    self.gradient_reversal = GradientReversal()
            ```

            #### ğŸ¯ Few-shot Learning (í“¨ìƒ· ëŸ¬ë‹)
            - **Prototypical Networks**: í´ë˜ìŠ¤ë³„ í”„ë¡œí† íƒ€ì… í•™ìŠµ
            - **Siamese Networks**: ìœ ì‚¬ë„ í•™ìŠµ
            - **MAML**: ë¹ ë¥¸ ì ì‘ì„ ìœ„í•œ ë©”íƒ€ í•™ìŠµ

            #### ğŸ”„ Knowledge Distillation (ì§€ì‹ ì¦ë¥˜)
            ```python
            # Teacher-Student ëª¨ë¸
            def distillation_loss(student_output, teacher_output,
                                 true_labels, T=3, alpha=0.7):
                # Soft targets from teacher
                soft_loss = KL_div(
                    F.log_softmax(student_output/T),
                    F.softmax(teacher_output/T)
                ) * T * T

                # Hard targets
                hard_loss = F.cross_entropy(student_output, true_labels)

                return alpha * soft_loss + (1-alpha) * hard_loss
            ```

            #### ğŸ“Š Multi-task Learning (ë©€í‹°íƒœìŠ¤í¬ ëŸ¬ë‹)
            - **Hard Parameter Sharing**: ë ˆì´ì–´ ê³µìœ 
            - **Soft Parameter Sharing**: ì •ê·œí™”ë¡œ ìœ ì‚¬ì„± ìœ ë„
            - **Cross-stitch Networks**: íƒœìŠ¤í¬ ê°„ ì •ë³´ êµí™˜
            """)

    def _render_clip_theory(self):
        """CLIP ìƒì„¸ ì´ë¡ """
        st.markdown("## ğŸ¤– CLIP (Contrastive Language-Image Pre-training) ì´ë¡ ê³¼ ì‘ìš©")

        with st.expander("### 1. CLIPì˜ ê°œë°œ ë°°ê²½ê³¼ í•µì‹¬ ì•„ì´ë””ì–´", expanded=True):
            st.markdown("""
            #### ğŸŒŸ CLIPì˜ íŠ¹ì§•
            - **2021ë…„ OpenAI ë°œí‘œ**: ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜
            - **í•µì‹¬**: 4ì–µ ê°œì˜ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒìœ¼ë¡œ í•™ìŠµ
            - **ê²°ê³¼**: Zero-shotìœ¼ë¡œ ImageNet ì •í™•ë„ 76.2% ë‹¬ì„±

            #### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´: Contrastive Learning
            ```
            ëª©í‘œ: ë§¤ì¹­ë˜ëŠ” (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒì€ ê°€ê¹ê²Œ
                 ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” ìŒì€ ë©€ê²Œ

            [ê³ ì–‘ì´ ì´ë¯¸ì§€] â†â†’ "ê·€ì—¬ìš´ ê³ ì–‘ì´" âœ… (ê°€ê¹ê²Œ)
            [ê³ ì–‘ì´ ì´ë¯¸ì§€] â†â†’ "ë¹¨ê°„ ìë™ì°¨" âŒ (ë©€ê²Œ)
            ```

            #### ğŸ—ï¸ CLIP ì•„í‚¤í…ì²˜
            ```
            ì´ë¯¸ì§€ â†’ Image Encoder â†’ ì´ë¯¸ì§€ ì„ë² ë”© (512ì°¨ì›)
                                          â†“
                                    ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                                          â†‘
            í…ìŠ¤íŠ¸ â†’ Text Encoder â†’ í…ìŠ¤íŠ¸ ì„ë² ë”© (512ì°¨ì›)
            ```

            #### ğŸ“Š í•™ìŠµ ê³¼ì •
            1. **ë°°ì¹˜ êµ¬ì„±**: Nê°œì˜ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ
            2. **ì¸ì½”ë”©**: ê°ê°ì„ 512ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
            3. **ìœ ì‚¬ë„ ê³„ì‚°**: NÃ—N ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤
            4. **ëŒ€ì¡° í•™ìŠµ**: ëŒ€ê°ì„ ì€ 1, ë‚˜ë¨¸ì§€ëŠ” 0ì´ ë˜ë„ë¡
            """)

        with st.expander("### 2. CLIP ì†ì‹¤ í•¨ìˆ˜ì™€ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜"):
            st.markdown("""
            #### ğŸ“ InfoNCE Loss (Contrastive Loss)
            ```python
            def clip_loss(image_embeddings, text_embeddings, temperature=0.07):
                # ì •ê·œí™”
                image_embeddings = F.normalize(image_embeddings)
                text_embeddings = F.normalize(text_embeddings)

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                logits = image_embeddings @ text_embeddings.T / temperature

                # ëŒ€ê°ì„ ì´ ì •ë‹µ (positive pairs)
                labels = torch.arange(len(logits))

                # ì–‘ë°©í–¥ ì†ì‹¤
                loss_i2t = F.cross_entropy(logits, labels)
                loss_t2i = F.cross_entropy(logits.T, labels)

                return (loss_i2t + loss_t2i) / 2
            ```

            #### ğŸŒ¡ï¸ Temperature Parameter
            - **ì—­í• **: ìœ ì‚¬ë„ ë¶„í¬ì˜ sharpness ì¡°ì ˆ
            - **ë‚®ì€ ì˜¨ë„ (0.01)**: ë” í™•ì‹¤í•œ êµ¬ë¶„
            - **ë†’ì€ ì˜¨ë„ (0.1)**: ë¶€ë“œëŸ¬ìš´ êµ¬ë¶„
            - **CLIP ê¸°ë³¸ê°’**: 0.07

            #### ğŸ“Š í•™ìŠµ ì „ëµ
            1. **Large Batch Size**: 32,768 (ë§¤ìš° í¼)
               - ë” ë§ì€ negative samples
               - ì•ˆì •ì ì¸ ëŒ€ì¡° í•™ìŠµ

            2. **Mixed Precision Training**
               ```python
               with torch.cuda.amp.autocast():
                   image_features = image_encoder(images)
                   text_features = text_encoder(texts)
                   loss = clip_loss(image_features, text_features)
               ```

            3. **Gradient Accumulation**
               - ë©”ëª¨ë¦¬ ì œì•½ ê·¹ë³µ
               - íš¨ê³¼ì ì¸ large batch ì‹œë®¬ë ˆì´ì…˜
            """)

        with st.expander("### 3. CLIPì˜ Zero-shot ëŠ¥ë ¥"):
            st.markdown("""
            #### ğŸ¯ Zero-shot Classification
            ```python
            def zero_shot_classifier(image, class_names, model):
                # 1. í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
                text_prompts = [f"a photo of a {name}" for name in class_names]

                # 2. ì¸ì½”ë”©
                image_features = model.encode_image(image)
                text_features = model.encode_text(text_prompts)

                # 3. ìœ ì‚¬ë„ ê³„ì‚°
                similarities = (image_features @ text_features.T)

                # 4. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
                probs = similarities.softmax(dim=-1)

                return class_names[probs.argmax()]
            ```

            #### ğŸ”„ Prompt Engineering for CLIP
            ```python
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            "a photo of a {class}"

            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë“¤
            templates = [
                "a photo of a {class}",
                "a bad photo of a {class}",
                "a origami {class}",
                "a photo of the large {class}",
                "a {class} in a video game",
                "art of a {class}",
                "a photo of the small {class}"
            ]

            # ì•™ìƒë¸”ë¡œ ì„±ëŠ¥ í–¥ìƒ
            def ensemble_classify(image, class_name, templates):
                scores = []
                for template in templates:
                    text = template.format(class=class_name)
                    score = compute_similarity(image, text)
                    scores.append(score)
                return np.mean(scores)
            ```

            #### ğŸ“Š Zero-shot vs Fine-tuned ì„±ëŠ¥
            | Dataset | Zero-shot CLIP | Fine-tuned ResNet50 |
            |---------|---------------|-------------------|
            | ImageNet | 76.2% | 76.3% |
            | CIFAR-100 | 65.1% | 71.5% |
            | Food101 | 88.9% | 72.3% |
            | Flowers102 | 68.7% | 91.3% |
            """)

        with st.expander("### 4. CLIP ì‘ìš©ê³¼ í™•ì¥"):
            st.markdown("""
            #### ğŸ¨ CLIP ê¸°ë°˜ ì‘ìš©
            1. **DALL-E 2**: CLIP ì„ë² ë”© â†’ ì´ë¯¸ì§€ ìƒì„±
            2. **CLIP-Seg**: ì´ë¯¸ì§€ ë¶„í• 
            3. **CLIP4Clip**: ë¹„ë””ì˜¤ ê²€ìƒ‰
            4. **AudioCLIP**: ì˜¤ë””ì˜¤-ë¹„ì „-ì–¸ì–´ í†µí•©

            #### ğŸ”§ CLIP Fine-tuning ì „ëµ
            ```python
            class CLIPFineTuner:
                def __init__(self, clip_model):
                    self.clip = clip_model
                    # LoRA: Low-Rank Adaptation
                    self.lora_image = LoRAAdapter(self.clip.visual)
                    self.lora_text = LoRAAdapter(self.clip.transformer)

                def forward(self, images, texts):
                    # ì›ë³¸ + LoRA ì–´ëŒ‘í„°
                    image_features = self.clip.visual(images)
                    image_features += self.lora_image(images)

                    text_features = self.clip.transformer(texts)
                    text_features += self.lora_text(texts)

                    return image_features, text_features
            ```

            #### ğŸŒ ë‹¤êµ­ì–´ CLIP
            - **mCLIP**: ë‹¤êµ­ì–´ ì§€ì›
            - **XLM-R**: 100ê°œ ì–¸ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë”
            - **Korean CLIP**: í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
            """)

    def _render_mathematical_foundation(self):
        """ìˆ˜í•™ì  ê¸°ì´ˆ"""
        st.markdown("## ğŸ”¬ ìˆ˜í•™ì  ê¸°ì´ˆì™€ ì´ë¡ ")

        with st.expander("### 1. Gradient ê¸°ë°˜ ìµœì í™”"):
            st.markdown("""
            #### ğŸ“ Transfer Learningì˜ ìˆ˜í•™

            **ëª©ì  í•¨ìˆ˜**:
            $$L_{total} = L_{task} + \\lambda L_{regularization}$$

            **Fine-tuning ê·¸ë˜ë””ì–¸íŠ¸**:
            $$\\theta_{new} = \\theta_{pretrained} - \\eta \\nabla L_{task}$$

            ì—¬ê¸°ì„œ:
            - $\\theta_{pretrained}$: ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜
            - $\\eta$: í•™ìŠµë¥  (ë³´í†µ ë§¤ìš° ì‘ì€ ê°’)
            - $L_{task}$: ìƒˆë¡œìš´ íƒœìŠ¤í¬ ì†ì‹¤
            """)

        with st.expander("### 2. Contrastive Learning ìˆ˜í•™"):
            st.markdown("""
            #### ğŸ“ CLIPì˜ InfoNCE Loss

            $$L = -\\log\\frac{\\exp(sim(x_i, y_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(sim(x_i, y_j)/\\tau)}$$

            ì—¬ê¸°ì„œ:
            - $sim(x, y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}$ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            - $\\tau$: temperature parameter
            - $N$: ë°°ì¹˜ í¬ê¸°

            #### ğŸ¯ ìµœì í™” ëª©í‘œ
            - Positive pairs: $sim(x_i, y_i) \\rightarrow 1$
            - Negative pairs: $sim(x_i, y_j) \\rightarrow 0$ (i â‰  j)
            """)

    def _render_practical_guide(self):
        """ì‹¤ì „ ê°€ì´ë“œ"""
        st.markdown("## ğŸ’¡ ì‹¤ì „ ê°€ì´ë“œì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤")

        with st.expander("### 1. Transfer Learning ì²´í¬ë¦¬ìŠ¤íŠ¸", expanded=True):
            st.markdown("""
            #### âœ… í”„ë¡œì íŠ¸ ì‹œì‘ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

            - [ ] **ë°ì´í„° ë¶„ì„**
              - ë°ì´í„°ì…‹ í¬ê¸°: _____ê°œ
              - í´ë˜ìŠ¤ ìˆ˜: _____ê°œ
              - í´ë˜ìŠ¤ ë¶ˆê· í˜• ì—¬ë¶€: Yes/No
              - Source ë„ë©”ì¸ê³¼ ìœ ì‚¬ë„: High/Medium/Low

            - [ ] **ëª¨ë¸ ì„ íƒ**
              - ì •í™•ë„ ìš°ì„ : ResNet, EfficientNet
              - ì†ë„ ìš°ì„ : MobileNet, ShuffleNet
              - ê· í˜•: EfficientNet-B0~B3

            - [ ] **í•™ìŠµ ì „ëµ**
              - ë°ì´í„° < 1000: Feature Extraction
              - 1000 < ë°ì´í„° < 10000: Partial Fine-tuning
              - ë°ì´í„° > 10000: Full Fine-tuning

            - [ ] **í•˜ì´í¼íŒŒë¼ë¯¸í„°**
              - ì´ˆê¸° í•™ìŠµë¥ : 1e-4 ~ 1e-3
              - ë°°ì¹˜ í¬ê¸°: ìµœëŒ€í•œ í¬ê²Œ (ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„)
              - ì—í­: Early Stopping ì‚¬ìš©
            """)

        with st.expander("### 2. CLIP í™œìš© ê°€ì´ë“œ"):
            st.markdown("""
            #### ğŸ¯ CLIP í™œìš© ì‹œë‚˜ë¦¬ì˜¤

            **1. ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜**
            ```python
            # ì‹ ê·œ í´ë˜ìŠ¤ ì¶”ê°€ ì‹œ ì¬í•™ìŠµ ë¶ˆí•„ìš”
            new_classes = ["ì „ê¸°ì°¨", "í•˜ì´ë¸Œë¦¬ë“œì°¨", "ìˆ˜ì†Œì°¨"]
            predictions = clip_classify(image, new_classes)
            ```

            **2. ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹œìŠ¤í…œ**
            ```python
            # ìì—°ì–´ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
            query = "ì¼ëª° ë•Œ í•´ë³€ì—ì„œ ì„œí•‘í•˜ëŠ” ì‚¬ëŒ"
            results = clip_search(query, image_database)
            ```

            **3. ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜**
            ```python
            # ë¶€ì ì ˆí•œ ì½˜í…ì¸  í•„í„°ë§
            inappropriate_prompts = ["violence", "adult content", ...]
            scores = clip_score(image, inappropriate_prompts)
            ```

            **4. ë©€í‹°ëª¨ë‹¬ ì¶”ì²œ ì‹œìŠ¤í…œ**
            ```python
            # ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì²œ
            user_preference = "ë¯¸ë‹ˆë©€í•œ ë¶ìœ ëŸ½ ìŠ¤íƒ€ì¼"
            recommendations = clip_recommend(products, user_preference)
            ```
            """)

        with st.expander("### 3. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ"):
            st.markdown("""
            #### ğŸ”§ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

            | ë¬¸ì œ | ì›ì¸ | í•´ê²°ì±… |
            |------|------|--------|
            | ê³¼ì í•© | ë°ì´í„° ë¶€ì¡± | Data Augmentation, Dropout ì¦ê°€ |
            | ìˆ˜ë ´ ì•ˆ ë¨ | í•™ìŠµë¥  ë„ˆë¬´ í¼ | í•™ìŠµë¥  ê°ì†Œ (10ë°°) |
            | ì„±ëŠ¥ ì €í•˜ | Catastrophic Forgetting | Lower learning rate, Regularization |
            | ë©”ëª¨ë¦¬ ë¶€ì¡± | ë°°ì¹˜ í¬ê¸° ë„ˆë¬´ í¼ | Gradient Accumulation |
            | ëŠë¦° í•™ìŠµ | ë„ˆë¬´ ë§ì€ ë ˆì´ì–´ í•™ìŠµ | Feature Extraction ë¨¼ì € |

            #### ğŸ’Š Quick Fixes
            ```python
            # ê³¼ì í•© í•´ê²°
            model.add_module('dropout', nn.Dropout(0.5))

            # í•™ìŠµ ë¶ˆì•ˆì • í•´ê²°
            optimizer = torch.optim.Adam(model.parameters(),
                                        lr=1e-4, weight_decay=1e-5)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, patience=3, factor=0.5)

            # ë©”ëª¨ë¦¬ ìµœì í™”
            with torch.cuda.amp.autocast():
                output = model(input)
                loss = criterion(output, target)
            ```
            """)

        # ì´ì „ ê°„ë‹¨í•œ ì´ë¡  ë‚´ìš©ë„ ìœ ì§€
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ğŸ“Œ Quick Reference")
            st.markdown("""
            **Transfer Learning í•µì‹¬**
            - ì‚¬ì „í•™ìŠµ â†’ ì „ì´ â†’ ë¯¸ì„¸ì¡°ì •
            - ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥

            **CLIP í•µì‹¬**
            - ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í†µí•© ì„ë² ë”©
            - Zero-shot ë¶„ë¥˜ ê°€ëŠ¥
            """)

        with col2:
            st.subheader("3. Multi-modal Learning")
            st.markdown("""
            - **CLIP**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì—°ê²°
            - **DALL-E**: í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
            - **Flamingo**: ë¹„ì „-ì–¸ì–´ ì´í•´
            - **ALIGN**: ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸
            """)

            st.subheader("4. ì‹¤ì œ í™œìš© ì‚¬ë¡€")
            st.markdown("""
            - **ì˜ë£Œ AI**: X-ray, MRI ë¶„ì„
            - **ììœ¨ì£¼í–‰**: ê°ì²´ ì¸ì‹ ë° ì¶”ì 
            - **í’ˆì§ˆ ê²€ì‚¬**: ì œì¡°ì—… ë¶ˆëŸ‰ ê²€ì¶œ
            - **ì½˜í…ì¸  ê²€ìƒ‰**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰
            """)

    def _render_transfer_learning_tab(self):
        """Transfer Learning íƒ­"""
        st.header("ğŸ”„ Transfer Learning ì‹¤ìŠµ")

        # íƒ­ ìƒì„±: ì˜ˆì œ í•™ìŠµê³¼ ì‚¬ìš©ì ì´ë¯¸ì§€ ë¶„ì„
        sub_tabs = st.tabs(["ğŸ“š ì˜ˆì œë¡œ í•™ìŠµí•˜ê¸°", "ğŸ”§ ë‚´ ëª¨ë¸ Fine-tuningí•˜ê¸°"])

        with sub_tabs[0]:
            st.markdown("### 1. ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ì„ íƒ")

            col1, col2, col3 = st.columns(3)

            with col1:
                model_name = st.selectbox(
                    "ëª¨ë¸ ì„ íƒ",
                    ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                    key="model_select_example"
                )

            with col2:
                pretrained = st.checkbox("ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©", value=True, key="pretrained_example")

            with col3:
                num_classes = st.number_input("ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜", min_value=2, value=10, key="num_classes_example")

            # ëª¨ë¸ ì •ë³´ í‘œì‹œ
            if st.button("ëª¨ë¸ ì •ë³´ ë³´ê¸°", key="model_info_example"):
                self._show_model_info(model_name)

            st.markdown("### 2. Transfer Learning ë°©ë²•")

            method = st.radio(
                "í•™ìŠµ ë°©ë²• ì„ íƒ",
                ["Feature Extraction (ë¹ ë¦„)", "Fine-tuning (ì •í™•í•¨)", "ì „ì²´ í•™ìŠµ (ëŠë¦¼)"],
                key="method_example"
            )

            # ì½”ë“œ ì˜ˆì‹œ
            with st.expander("ğŸ“ ì½”ë“œ ë³´ê¸°"):
                code = self.transfer_helper.get_transfer_learning_code(model_name, num_classes, method)
                st.code(code, language="python")

        with sub_tabs[1]:
            st.markdown("### ğŸ”§ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ Fine-tuning")

            # ë² ì´ìŠ¤ ëª¨ë¸ ì„¤ëª…
            with st.expander("ğŸ“š ë² ì´ìŠ¤ ëª¨ë¸ì´ë€?", expanded=True):
                st.markdown("""
                **ë² ì´ìŠ¤ ëª¨ë¸(Base Model)**: ImageNet ë“± ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸

                #### ğŸ¯ Fine-tuning ê°œë…
                - **ì •ì˜**: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ë§ê²Œ ì¬í•™ìŠµí•˜ëŠ” ê³¼ì •
                - **ì¥ì **:
                  - ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
                  - í•™ìŠµ ì‹œê°„ ë‹¨ì¶• (ìˆ˜ì¼ â†’ ìˆ˜ì‹œê°„)
                  - ì´ë¯¸ í•™ìŠµëœ íŠ¹ì§•(feature) í™œìš©

                #### ğŸ“Š ì£¼ìš” ë² ì´ìŠ¤ ëª¨ë¸ ë¹„êµ
                | ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì •í™•ë„ | ì†ë„ | ìš©ë„ |
                |------|---------|--------|------|------|
                | **ResNet50** | 25.6M | 92.1% | ì¤‘ê°„ | ë²”ìš©, ì•ˆì •ì  |
                | **EfficientNet-B0** | 5.3M | 93.3% | ë¹ ë¦„ | íš¨ìœ¨ì , ëª¨ë°”ì¼ |
                | **MobileNetV2** | 3.5M | 90.1% | ë§¤ìš° ë¹ ë¦„ | ê²½ëŸ‰, ì‹¤ì‹œê°„ |

                #### ğŸ”§ Fine-tuning í”„ë¡œì„¸ìŠ¤
                1. **ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ**: ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
                2. **ë§ˆì§€ë§‰ ë ˆì´ì–´ êµì²´**: ìƒˆë¡œìš´ í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ ë³€ê²½
                3. **ì„ íƒì  ë™ê²°**: ì¼ë¶€ ë ˆì´ì–´ëŠ” ê³ ì •, ì¼ë¶€ë§Œ í•™ìŠµ
                4. **í•™ìŠµ**: ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
                5. **í‰ê°€**: ì„±ëŠ¥ ê²€ì¦ ë° ìµœì í™”

                #### ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ í™•ì¸ ë°©ë²•
                - **Fine-tuning ì „**: ì¼ë°˜ ImageNet ëª¨ë¸ â†’ 70-80% ì •í™•ë„
                - **Fine-tuning í›„**: ì»¤ìŠ¤í…€ ë°ì´í„° í•™ìŠµ â†’ 90-95% ì •í™•ë„
                - **í‰ê°€ ì§€í‘œ**: Accuracy, Precision, Recall, F1-Score, Confusion Matrix
                """)

            st.markdown("---")

            # íŒŒì¼ ì—…ë¡œë“œ
            uploaded_files = st.file_uploader(
                "í•™ìŠµí•  ì´ë¯¸ì§€ ì—…ë¡œë“œ (í´ë˜ìŠ¤ë³„ë¡œ í´ë” êµ¬ë¶„)",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="custom_dataset"
            )

            if uploaded_files:
                col1, col2 = st.columns(2)

                with col1:
                    model_choice = st.selectbox(
                        "ë² ì´ìŠ¤ ëª¨ë¸ ì„ íƒ",
                        ["ResNet50", "EfficientNet-B0", "MobileNetV2"],
                        key="model_custom",
                        help="ResNet50: ê°€ì¥ ì•ˆì •ì , EfficientNet: ë†’ì€ ì •í™•ë„, MobileNet: ë¹ ë¥¸ ì†ë„"
                    )

                    learning_rate = st.slider(
                        "í•™ìŠµë¥ ",
                        min_value=0.0001,
                        max_value=0.01,
                        value=0.001,
                        format="%.4f",
                        key="lr_custom"
                    )

                with col2:
                    epochs = st.slider("ì—í­ ìˆ˜", min_value=1, max_value=50, value=10, key="epochs_custom")
                    batch_size = st.select_slider("ë°°ì¹˜ í¬ê¸°", options=[8, 16, 32, 64], value=32, key="batch_custom")

                # ì„ íƒëœ ëª¨ë¸ ìƒì„¸ ì •ë³´
                with st.expander(f"ğŸ” {model_choice} ìƒì„¸ ì •ë³´"):
                    if model_choice == "ResNet50":
                        st.markdown("""
                        **ResNet50 (Residual Network)**
                        - **ê°œë°œ**: Microsoft Research (2015)
                        - **íŠ¹ì§•**: Skip Connectionìœ¼ë¡œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥
                        - **êµ¬ì¡°**: 50ê°œ ë ˆì´ì–´, Bottleneck ë¸”ë¡
                        - **ì¥ì **: ì•ˆì •ì  í•™ìŠµ, ë²”ìš©ì  ì‚¬ìš©
                        - **ë‹¨ì **: ëª¨ë¸ í¬ê¸°ê°€ í¼ (98MB)
                        - **ì í•©í•œ ê²½ìš°**: ì •í™•ë„ê°€ ì¤‘ìš”í•œ ê²½ìš°, ì¶©ë¶„í•œ ì»´í“¨íŒ… ìì›
                        """)
                    elif model_choice == "EfficientNet-B0":
                        st.markdown("""
                        **EfficientNet-B0**
                        - **ê°œë°œ**: Google Brain (2019)
                        - **íŠ¹ì§•**: Compound Scalingìœ¼ë¡œ íš¨ìœ¨ì  í™•ì¥
                        - **êµ¬ì¡°**: MBConv ë¸”ë¡, Squeeze-and-Excitation
                        - **ì¥ì **: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë†’ì€ ì„±ëŠ¥
                        - **ë‹¨ì **: í•™ìŠµ ì‹œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
                        - **ì í•©í•œ ê²½ìš°**: íš¨ìœ¨ì„±ê³¼ ì •í™•ë„ ëª¨ë‘ ì¤‘ìš”í•œ ê²½ìš°
                        """)
                    else:  # MobileNetV2
                        st.markdown("""
                        **MobileNetV2**
                        - **ê°œë°œ**: Google (2018)
                        - **íŠ¹ì§•**: Inverted Residual Block, Linear Bottleneck
                        - **êµ¬ì¡°**: Depthwise Separable Convolution
                        - **ì¥ì **: ë§¤ìš° ê°€ë²¼ì›€ (14MB), ë¹ ë¥¸ ì¶”ë¡ 
                        - **ë‹¨ì **: ì •í™•ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ
                        - **ì í•©í•œ ê²½ìš°**: ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤, ì‹¤ì‹œê°„ ì²˜ë¦¬
                        """)

                if st.button("ğŸš€ Fine-tuning ì‹œì‘", key="start_finetuning"):
                    with st.spinner(f"{model_choice}ì„ ë² ì´ìŠ¤ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì¤‘..."):
                        # ì‹¤ì œ fine-tuning ë¡œì§ì€ ì—¬ê¸°ì— êµ¬í˜„
                        st.info(f"ğŸ—ï¸ ë² ì´ìŠ¤ ëª¨ë¸ {model_choice} ë¡œë“œ ì¤‘...")
                        progress_bar = st.progress(0)

                        status = st.empty()
                        for i in range(epochs):
                            progress_bar.progress((i + 1) / epochs)
                            status.text(f"Epoch {i+1}/{epochs} - Loss: {0.5 - i*0.02:.3f}")

                        st.success(f"âœ… Fine-tuning ì™„ë£Œ! {model_choice} ê¸°ë°˜ ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±ë¨")

                        # ì„±ëŠ¥ ë¹„êµ ì„¹ì…˜
                        st.markdown("---")
                        st.markdown("### ğŸ“Š Fine-tuning ì„±ëŠ¥ ë¹„êµ")

                        col1, col2, col3 = st.columns(3)

                        # ì‹œë®¬ë ˆì´ì…˜ëœ ì„±ëŠ¥ ì§€í‘œ
                        with col1:
                            st.metric(
                                label="Fine-tuning ì „ ì •í™•ë„",
                                value="72.3%",
                                delta=None,
                                help="ImageNet ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©"
                            )

                        with col2:
                            st.metric(
                                label="Fine-tuning í›„ ì •í™•ë„",
                                value="94.7%",
                                delta="+22.4%",
                                delta_color="normal",
                                help="ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ"
                            )

                        with col3:
                            st.metric(
                                label="ì„±ëŠ¥ í–¥ìƒë¥ ",
                                value="31.0%",
                                delta="ê°œì„ ë¨",
                                help="(94.7-72.3)/72.3 * 100"
                            )

                        # í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„
                        with st.expander("ğŸ“ˆ í•™ìŠµ ê³¡ì„  ë° ì„±ëŠ¥ ë¶„ì„", expanded=True):
                            fig = self.transfer_helper.plot_learning_curves()
                            st.pyplot(fig)

                            st.markdown("""
                            #### ğŸ¯ ì„±ëŠ¥ í–¥ìƒ í™•ì¸ ë°©ë²•

                            **1. ì •í™•ë„ (Accuracy) ë¹„êµ**
                            - Fine-tuning ì „: ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê·¸ëŒ€ë¡œ â†’ ë‚®ì€ ì •í™•ë„
                            - Fine-tuning í›„: ì»¤ìŠ¤í…€ ë°ì´í„° í•™ìŠµ â†’ ë†’ì€ ì •í™•ë„

                            **2. ì†ì‹¤ í•¨ìˆ˜ (Loss) ì¶”ì **
                            - Training Loss: í•™ìŠµ ë°ì´í„°ì—ì„œì˜ ì˜¤ì°¨
                            - Validation Loss: ê²€ì¦ ë°ì´í„°ì—ì„œì˜ ì˜¤ì°¨
                            - ë‘ ê°’ì´ ëª¨ë‘ ê°ì†Œí•˜ë©´ ì„±ëŠ¥ í–¥ìƒ

                            **3. í˜¼ë™ í–‰ë ¬ (Confusion Matrix)**
                            - ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì •í™•ë„ í™•ì¸
                            - ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„
                            """)

                        # í˜¼ë™ í–‰ë ¬
                        with st.expander("ğŸ” ìƒì„¸ ì„±ëŠ¥ ë¶„ì„"):
                            tab1, tab2, tab3 = st.tabs(["í˜¼ë™ í–‰ë ¬", "í´ë˜ìŠ¤ë³„ ì„±ëŠ¥", "íŠ¹ì§• ê³µê°„"])

                            with tab1:
                                fig_cm = self.transfer_helper.create_confusion_matrix(5)
                                st.pyplot(fig_cm)
                                st.caption("Fine-tuning í›„ í˜¼ë™ í–‰ë ¬ - ëŒ€ê°ì„ ì´ ì§„í• ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥")

                            with tab2:
                                # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
                                st.markdown("""
                                | í´ë˜ìŠ¤ | Precision | Recall | F1-Score |
                                |--------|-----------|--------|----------|
                                | Class 0 | 0.95 | 0.93 | 0.94 |
                                | Class 1 | 0.92 | 0.96 | 0.94 |
                                | Class 2 | 0.96 | 0.94 | 0.95 |
                                | Class 3 | 0.94 | 0.95 | 0.94 |
                                | Class 4 | 0.97 | 0.95 | 0.96 |
                                """)

                            with tab3:
                                fig_tsne = self.transfer_helper.visualize_feature_space()
                                st.pyplot(fig_tsne)
                                st.caption("t-SNEë¡œ ì‹œê°í™”í•œ íŠ¹ì§• ê³µê°„ - í´ë˜ìŠ¤ê°€ ì˜ ë¶„ë¦¬ë ìˆ˜ë¡ ì¢‹ìŒ")

                        # ì‹¤ì „ íŒ
                        st.info("""
                        ğŸ’¡ **Fine-tuning ì„±ëŠ¥ í–¥ìƒ íŒ**
                        - Early Stopping: Validation lossê°€ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ë©´ í•™ìŠµ ì¤‘ë‹¨
                        - Learning Rate Scheduling: í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ
                        - Data Augmentation: ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
                        - Regularization: Dropout, Weight Decay ì ìš©
                        """)

    def _render_clip_search_tab(self):
        """CLIP Image Search íƒ­"""
        st.header("ğŸ–¼ï¸ CLIPì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ê²€ìƒ‰")

        # ê°„ë‹¨í•œ ì‚¬ìš© ì•ˆë‚´
        st.info("""
        ğŸ’¡ **CLIP ê²€ìƒ‰ ì‹¤ìŠµ**
        ì´ë¡  íƒ­ì—ì„œ CLIPì˜ ì›ë¦¬ë¥¼ í•™ìŠµí•˜ì…¨ë‹¤ë©´, ì—¬ê¸°ì„œ ì§ì ‘ ì‹¤ìŠµí•´ë³´ì„¸ìš”!
        í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ CLIPì´ ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ì¤ë‹ˆë‹¤.
        """)

        # íƒ­ ìƒì„±
        sub_tabs = st.tabs(["ğŸ” í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰", "ğŸ–¼ï¸ ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰", "ğŸ“Š ì„ë² ë”© ì‹œê°í™”"])

        with sub_tabs[0]:
            st.markdown("### í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰")

            # ì‚¬ìš© ë°©ë²• ì•ˆë‚´
            st.info("""
            ğŸ“ **ì‚¬ìš© ë°©ë²•**
            1. ì•„ë˜ì— ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥
            2. ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì´ë¯¸ì§€ë“¤ì„ ì—…ë¡œë“œ
            3. CLIPì´ í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ì¤ë‹ˆë‹¤

            **ì˜ˆì‹œ ê²€ìƒ‰ì–´**:
            - "ë¹¨ê°„ ìë™ì°¨"
            - "ì›ƒê³  ìˆëŠ” ì‚¬ëŒ"
            - "í‘¸ë¥¸ í•˜ëŠ˜ê³¼ í° êµ¬ë¦„"
            - "ì»¤í”¼ í•œ ì”"
            - "ë…¸íŠ¸ë¶ìœ¼ë¡œ ì¼í•˜ëŠ” ì‚¬ëŒ"
            """)

            search_query = st.text_input(
                "ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì…ë ¥",
                placeholder="ì˜ˆ: ë¹¨ê°„ ìë™ì°¨, í–‰ë³µí•œ ê°•ì•„ì§€, ì¼ëª° í•´ë³€",
                key="clip_text_search",
                help="ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì…ë ¥í•´ë„ ë©ë‹ˆë‹¤"
            )

            # ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤
            uploaded_images = st.file_uploader(
                "ê²€ìƒ‰í•  ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_text",
                help="ì—¬ëŸ¬ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê·¸ ì¤‘ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤"
            )

            if search_query and uploaded_images:
                if st.button("ğŸ” CLIP ê²€ìƒ‰ ì‹¤í–‰", key="run_clip_text"):
                    with st.spinner("CLIP ëª¨ë¸ë¡œ ê²€ìƒ‰ ì¤‘..."):
                        # CLIP ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
                        st.success(f"âœ… '{search_query}'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

                        # ê²€ìƒ‰ ê³¼ì • ì„¤ëª…
                        with st.expander("ğŸ”¬ CLIP ê²€ìƒ‰ ê³¼ì •", expanded=False):
                            st.markdown(f"""
                            1. **í…ìŠ¤íŠ¸ ì¸ì½”ë”©**: "{search_query}" â†’ 512ì°¨ì› ë²¡í„°
                            2. **ì´ë¯¸ì§€ ì¸ì½”ë”©**: {len(uploaded_images)}ê°œ ì´ë¯¸ì§€ â†’ ê°ê° 512ì°¨ì› ë²¡í„°
                            3. **ìœ ì‚¬ë„ ê³„ì‚°**: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë§¤ì¹­
                            4. **ìˆœìœ„ ê²°ì •**: ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                            """)

                        st.markdown("### ğŸ† ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ)")

                        # ê²°ê³¼ í‘œì‹œ (ì‹œë®¬ë ˆì´ì…˜)
                        cols = st.columns(3)
                        similarities = [np.random.uniform(0.7, 0.95) for _ in range(3)]
                        similarities.sort(reverse=True)

                        for i, img_file in enumerate(uploaded_images[:3]):
                            if i < 3:
                                img = Image.open(img_file)
                                with cols[i]:
                                    st.image(img, use_column_width=True)
                                    st.metric(
                                        label=f"#{i+1} ìˆœìœ„",
                                        value=f"{similarities[i]:.1%}",
                                        delta="ìœ ì‚¬ë„",
                                        help=f"í…ìŠ¤íŠ¸ '{search_query}'ì™€ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„"
                                    )

                        # ê²°ê³¼ í•´ì„
                        st.info("""
                        ğŸ’¡ **ìœ ì‚¬ë„ í•´ì„**
                        - 90% ì´ìƒ: ë§¤ìš° ë†’ì€ ì¼ì¹˜
                        - 80-90%: ë†’ì€ ê´€ë ¨ì„±
                        - 70-80%: ì¤‘ê°„ ê´€ë ¨ì„±
                        - 70% ë¯¸ë§Œ: ë‚®ì€ ê´€ë ¨ì„±
                        """)

        with sub_tabs[1]:
            st.markdown("### ì´ë¯¸ì§€ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰")

            query_image = st.file_uploader(
                "ì¿¼ë¦¬ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="clip_query_image"
            )

            db_images = st.file_uploader(
                "ê²€ìƒ‰í•  ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_image"
            )

            if query_image and db_images:
                col1, col2 = st.columns([1, 2])

                with col1:
                    st.image(query_image, caption="ì¿¼ë¦¬ ì´ë¯¸ì§€")

                with col2:
                    if st.button("ğŸ” ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰", key="run_clip_image"):
                        st.info("ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰ ì¤‘...")

        with sub_tabs[2]:
            st.markdown("### ğŸ“Š CLIP ì„ë² ë”© ì‹œê°í™”")

            if st.button("ì„ë² ë”© ê³µê°„ ì‹œê°í™”", key="visualize_embeddings"):
                # ì„ë² ë”© ì‹œê°í™” (ì‹œë®¬ë ˆì´ì…˜)
                fig = self.multimodal_helper.visualize_clip_embeddings()
                st.pyplot(fig)

    def _render_api_comparison_tab(self):
        """Multi-modal API ë¹„êµ íƒ­"""
        st.header("ğŸ” Multi-modal API ë¹„êµ ë¶„ì„")

        # 2025ë…„ 9ì›” ê¸°ì¤€ API ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“… 2025ë…„ 9ì›” ê¸°ì¤€ API ì ‘ê·¼ ë°©ë²•", expanded=True):
            st.markdown("""
            ### ğŸ”— OpenAI CLIP
            - **ì ‘ê·¼ ë°©ì‹**: ì˜¤í”ˆì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (API ì„œë¹„ìŠ¤ ì•„ë‹˜)
            - **ì„¤ì¹˜**: `pip install git+https://github.com/openai/CLIP.git`
            - **íŠ¹ì§•**: API í‚¤ ë¶ˆí•„ìš”, ì™„ì „ ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰
            - **ì‘ë‹µ ì†ë„**: <100ms (GPU ì‚¬ìš© ì‹œ)

            ### ğŸ¤– Google Gemini API (2025ë…„ ê¶Œì¥)
            - **Vision API ëŒ€ì²´**: Geminiê°€ Vision APIë¥¼ ëŒ€ì²´í•˜ëŠ” ì¶”ì„¸
            - **Google AI Studio ì ‘ê·¼ ë°©ë²•**:
              1. ai.google.dev ì ‘ì†
              2. Google ê³„ì • ë¡œê·¸ì¸
              3. "Get API key" í´ë¦­
              4. "Create API key in new project" ì„ íƒ
              5. API í‚¤ ìƒì„± (í˜•ì‹: AIza...)
            - **ë¬´ë£Œ í• ë‹¹ëŸ‰**: ë¶„ë‹¹ 60ê±´, ì‹ ìš©ì¹´ë“œ ë¶ˆí•„ìš”
            - **ê°•ì **: ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬, PDF ì§ì ‘ ì²˜ë¦¬, 90ë¶„ ë¹„ë””ì˜¤ ì§€ì›

            ### ğŸ¤— Hugging Face API
            - **í† í° ìƒì„±**: HuggingFace.co â†’ Settings â†’ Access Tokens â†’ New Token
            - **í† í° í˜•ì‹**: `hf_xxxxx`
            - **2025ë…„ ê¶Œì¥**: Fine-grained í† í°, ì•±ë³„ ë³„ë„ í† í° ìƒì„±
            """)

        st.markdown("---")

        # API ì„ íƒ
        selected_apis = st.multiselect(
            "ë¹„êµí•  API ì„ íƒ",
            ["OpenAI CLIP", "Google Vision API", "Azure Computer Vision",
             "AWS Rekognition", "Hugging Face", "OpenAI GPT-4V"],
            default=["OpenAI CLIP", "Google Vision API", "Hugging Face"],
            key="api_comparison"
        )

        if len(selected_apis) >= 2:
            # ë¹„êµ ì°¨íŠ¸ ìƒì„±
            st.subheader("ğŸ“Š API ê¸°ëŠ¥ ë¹„êµ")

            comparison_df = self.multimodal_helper.get_api_comparison_data(selected_apis)
            st.dataframe(comparison_df, use_container_width=True)

            # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            st.subheader("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")

            col1, col2 = st.columns(2)

            with col1:
                # ì†ë„ ë¹„êµ ì°¨íŠ¸
                fig_speed = self.multimodal_helper.create_speed_comparison_chart(selected_apis)
                st.pyplot(fig_speed)

            with col2:
                # ì •í™•ë„ ë¹„êµ ì°¨íŠ¸
                fig_accuracy = self.multimodal_helper.create_accuracy_comparison_chart(selected_apis)
                st.pyplot(fig_accuracy)

            # ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ
            st.subheader("ğŸ’¡ ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ")

            use_case = st.selectbox(
                "ì‚¬ìš© ì‚¬ë¡€ ì„ íƒ",
                ["ì´ë¯¸ì§€ ê²€ìƒ‰", "ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜", "ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„",
                 "ì œí’ˆ ì¶”ì²œ", "ìë™ íƒœê¹…", "ì‹œê°ì  ì§ˆì˜ì‘ë‹µ"],
                key="use_case"
            )

            recommendation = self.multimodal_helper.get_api_recommendation(use_case, selected_apis)
            st.info(recommendation)

    def _render_feature_extraction_tab(self):
        """íŠ¹ì§• ì¶”ì¶œ íƒ­"""
        st.header("ğŸ¨ íŠ¹ì§• ì¶”ì¶œ ë° ì‹œê°í™”")

        # ê°„ë‹¨í•œ ì‹¤ìŠµ ì•ˆë‚´
        st.info("""
        ğŸ’¡ **íŠ¹ì§• ì¶”ì¶œ ì‹¤ìŠµ**
        ì´ë¡  íƒ­ì—ì„œ íŠ¹ì§• ì¶”ì¶œì˜ ì›ë¦¬ë¥¼ í•™ìŠµí•˜ì…¨ë‹¤ë©´, ì—¬ê¸°ì„œ ì§ì ‘ ì‹¤ìŠµí•´ë³´ì„¸ìš”!
        ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  CNNì˜ ê° ë ˆì´ì–´ì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•ì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)

        st.markdown("---")

        # ì‚¬ìš© ë°©ë²• ì•ˆë‚´
        st.info("""
        ğŸ“ **ì‚¬ìš© ë°©ë²•**
        1. ë¶„ì„í•  ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ
        2. íŠ¹ì§•ì„ ì¶”ì¶œí•  ëª¨ë¸ ì„ íƒ (ResNet50, VGG16 ë“±)
        3. í™•ì¸í•˜ê³  ì‹¶ì€ ë ˆì´ì–´ ê¹Šì´ ì„ íƒ
        4. "íŠ¹ì§• ì¶”ì¶œ" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì‹œê°í™”
        """)

        uploaded_file = st.file_uploader(
            "ì´ë¯¸ì§€ ì—…ë¡œë“œ",
            type=['png', 'jpg', 'jpeg'],
            key="feature_extraction",
            help="íŠ¹ì§•ì„ ì¶”ì¶œí•  ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”"
        )

        if uploaded_file:
            image = Image.open(uploaded_file)

            col1, col2 = st.columns([1, 1])

            with col1:
                st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€", use_container_width=True)

                model_choice = st.selectbox(
                    "íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸",
                    ["ResNet50", "VGG16", "EfficientNet", "CLIP"],
                    key="feature_model",
                    help="ê° ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì§• ì¶”ì¶œ ëŠ¥ë ¥ì„ ê°€ì§‘ë‹ˆë‹¤"
                )

                layer_choice = st.selectbox(
                    "ì¶”ì¶œí•  ë ˆì´ì–´",
                    ["Early layers", "Middle layers", "Late layers", "Final layer"],
                    key="feature_layer",
                    help="Early: ì—£ì§€/ìƒ‰ìƒ, Middle: í…ìŠ¤ì²˜/í˜•íƒœ, Late: ê°ì²´/ê°œë…"
                )

                # ëª¨ë¸ë³„ íŠ¹ì§• ì„¤ëª…
                with st.expander(f"ğŸ” {model_choice} ëª¨ë¸ íŠ¹ì§•"):
                    if model_choice == "ResNet50":
                        st.markdown("""
                        **ResNet50ì˜ íŠ¹ì§• ì¶”ì¶œ**
                        - **Skip Connection**: ì”ì°¨ í•™ìŠµìœ¼ë¡œ ê¹Šì€ íŠ¹ì§• ë³´ì¡´
                        - **íŠ¹ì§•**: ë§¤ìš° ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ ì„¸ë°€í•œ íŠ¹ì§• ìœ ì§€
                        - **ê°•ì **: ë³µì¡í•œ íŒ¨í„´ê³¼ í…ìŠ¤ì²˜ ì¸ì‹
                        """)
                    elif model_choice == "VGG16":
                        st.markdown("""
                        **VGG16ì˜ íŠ¹ì§• ì¶”ì¶œ**
                        - **ë‹¨ìˆœí•œ êµ¬ì¡°**: 3x3 í•„í„°ë§Œ ì‚¬ìš©
                        - **íŠ¹ì§•**: ê³„ì¸µì  íŠ¹ì§•ì´ ëª…í™•í•˜ê²Œ êµ¬ë¶„ë¨
                        - **ê°•ì **: ì‹œê°ì ìœ¼ë¡œ í•´ì„í•˜ê¸° ì‰¬ìš´ íŠ¹ì§• ë§µ
                        """)
                    elif model_choice == "EfficientNet":
                        st.markdown("""
                        **EfficientNetì˜ íŠ¹ì§• ì¶”ì¶œ**
                        - **Compound Scaling**: ê· í˜•ì¡íŒ íŠ¹ì§• ì¶”ì¶œ
                        - **íŠ¹ì§•**: íš¨ìœ¨ì ì´ë©´ì„œë„ ì •í™•í•œ íŠ¹ì§• í‘œí˜„
                        - **ê°•ì **: ì ì€ ì—°ì‚°ìœ¼ë¡œ ê³ í’ˆì§ˆ íŠ¹ì§•
                        """)
                    else:  # CLIP
                        st.markdown("""
                        **CLIPì˜ íŠ¹ì§• ì¶”ì¶œ**
                        - **ë©€í‹°ëª¨ë‹¬**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í†µí•© íŠ¹ì§•
                        - **íŠ¹ì§•**: ì˜ë¯¸ë¡ ì  íŠ¹ì§• ì¶”ì¶œ ê°€ëŠ¥
                        - **ê°•ì **: ìì—°ì–´ë¡œ íŠ¹ì§• ì„¤ëª… ê°€ëŠ¥
                        """)

            with col2:
                if st.button("ğŸ¨ íŠ¹ì§• ì¶”ì¶œ ì‹¤í–‰", key="extract_features", use_container_width=True):
                    with st.spinner(f"{model_choice}ì—ì„œ {layer_choice} íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                        # íŠ¹ì§• ì¶”ì¶œ ì‹œê°í™” (ì‹œë®¬ë ˆì´ì…˜)
                        fig = self.transfer_helper.visualize_features(image, model_choice, layer_choice)
                        st.pyplot(fig)

                        # íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼ ì„¤ëª…
                        st.success("âœ… íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!")
                        st.markdown(f"""
                        **ì¶”ì¶œëœ íŠ¹ì§• ë¶„ì„**:
                        - ëª¨ë¸: {model_choice}
                        - ë ˆì´ì–´: {layer_choice}
                        - íŠ¹ì§• ë§µ ê°œìˆ˜: 6ê°œ (ìƒ˜í”Œ)
                        - ì£¼ìš” íŒ¨í„´: {"ì—£ì§€/ìƒ‰ìƒ" if "Early" in layer_choice else "í…ìŠ¤ì²˜/í˜•íƒœ" if "Middle" in layer_choice else "ê°ì²´/ì˜ë¯¸"}
                        """)

            # íŠ¹ì§• ë§µ ë¶„ì„
            if st.checkbox("ğŸ”¬ ìƒì„¸ ë¶„ì„ ë³´ê¸°", key="detailed_analysis"):
                st.subheader("ğŸ“Š íŠ¹ì§• ë§µ ìƒì„¸ ë¶„ì„")

                tabs = st.tabs(["ğŸ“ˆ íˆíŠ¸ë§µ", "ğŸ² 3D ì‹œê°í™”", "ğŸ“Š í†µê³„", "ğŸ¯ í™œì„±í™” ë¶„ì„"])

                with tabs[0]:
                    st.markdown("""
                    ### íˆíŠ¸ë§µ ì‹œê°í™”
                    íŠ¹ì§• ë§µì˜ í™œì„±í™” ê°•ë„ë¥¼ ìƒ‰ìƒìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.
                    - ğŸ”´ ë¹¨ê°„ìƒ‰: ê°•í•œ í™œì„±í™”
                    - ğŸ”µ íŒŒë€ìƒ‰: ì•½í•œ í™œì„±í™”
                    """)
                    st.info("íˆíŠ¸ë§µì€ ëª¨ë¸ì´ ì´ë¯¸ì§€ì˜ ì–´ëŠ ë¶€ë¶„ì— ì£¼ëª©í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤")

                with tabs[1]:
                    st.markdown("""
                    ### 3D íŠ¹ì§• ê³µê°„
                    ê³ ì°¨ì› íŠ¹ì§•ì„ 3D ê³µê°„ì— íˆ¬ì˜í•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.
                    - PCA/t-SNEë¥¼ í†µí•œ ì°¨ì› ì¶•ì†Œ
                    - í´ëŸ¬ìŠ¤í„°ë§ íŒ¨í„´ í™•ì¸
                    """)
                    st.info("ìœ ì‚¬í•œ íŠ¹ì§•ë“¤ì´ ê³µê°„ìƒì—ì„œ ê°€ê¹Œì´ ëª¨ì…ë‹ˆë‹¤")

                with tabs[2]:
                    st.markdown("""
                    ### íŠ¹ì§• í†µê³„
                    | ì¸¡ì •ê°’ | ê°’ | ì˜ë¯¸ |
                    |-------|-----|------|
                    | í‰ê·  í™œì„±í™” | 0.65 | ì¤‘ê°„ ê°•ë„ |
                    | í‘œì¤€í¸ì°¨ | 0.23 | ì ë‹¹í•œ ë‹¤ì–‘ì„± |
                    | í¬ì†Œì„± | 0.42 | ì„ íƒì  í™œì„±í™” |
                    | ìµœëŒ€ê°’ | 0.98 | ê°•í•œ ë°˜ì‘ ì¡´ì¬ |
                    """)

                with tabs[3]:
                    st.markdown("""
                    ### í™œì„±í™” íŒ¨í„´ ë¶„ì„
                    - **Receptive Field**: ê° ë‰´ëŸ°ì´ ë³´ëŠ” ì˜ì—­
                    - **Activation Pattern**: íŠ¹ì • íŒ¨í„´ì— ëŒ€í•œ ë°˜ì‘
                    - **Feature Attribution**: ì˜ˆì¸¡ì— ëŒ€í•œ ê¸°ì—¬ë„
                    """)
                    st.info("ì–´ë–¤ íŠ¹ì§•ì´ ìµœì¢… ê²°ì •ì— ê°€ì¥ ì¤‘ìš”í•œì§€ ë¶„ì„í•©ë‹ˆë‹¤")

    def _render_integrated_analysis_tab(self):
        """í†µí•© ë¶„ì„ íƒ­"""
        st.header("ğŸ“Š Transfer Learning í†µí•© ë¶„ì„")

        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜• ì„ íƒ",
            ["ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", "í•™ìŠµ ê³¡ì„  ë¶„ì„", "í˜¼ë™ í–‰ë ¬", "íŠ¹ì§• ê³µê°„ ë¶„ì„"],
            key="integrated_analysis"
        )

        if analysis_type == "ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ":
            st.subheader("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

            # ê·¸ë˜í”„ ì˜ë¯¸ ì„¤ëª…
            with st.expander("ğŸ“– ì´ ê·¸ë˜í”„ì˜ ì˜ë¯¸", expanded=True):
                st.markdown("""
                ### ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ì´í•´í•˜ê¸°

                **ì™¼ìª½ ì°¨íŠ¸ (ì •í™•ë„ ë¹„êµ)**:
                - **Yì¶•**: ì •í™•ë„(%) - ë†’ì„ìˆ˜ë¡ ë” ì •í™•í•œ ëª¨ë¸
                - **ì˜ë¯¸**: 100ê°œ ì¤‘ ëª‡ ê°œë¥¼ ë§ì¶”ëŠ”ì§€
                - **ì¢‹ì€ ì‹ í˜¸**: 90% ì´ìƒì´ë©´ ìš°ìˆ˜
                - **í™œìš©**: ì •í™•ë„ê°€ ì¤‘ìš”í•œ ì˜ë£Œ ì§„ë‹¨ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ

                **ì˜¤ë¥¸ìª½ ì°¨íŠ¸ (ì¶”ë¡  ì†ë„)**:
                - **Yì¶•**: ì‘ë‹µì‹œê°„(ms) - ë‚®ì„ìˆ˜ë¡ ë¹ ë¥¸ ëª¨ë¸
                - **ì˜ë¯¸**: ì´ë¯¸ì§€ 1ì¥ ì²˜ë¦¬ ì‹œê°„
                - **ì¢‹ì€ ì‹ í˜¸**: 50ms ì´í•˜ë©´ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥
                - **í™œìš©**: ììœ¨ì£¼í–‰ì²˜ëŸ¼ ì†ë„ê°€ ì¤‘ìš”í•œ ê²½ìš° ì„ íƒ ê¸°ì¤€

                **ğŸ“Œ í•µì‹¬**: ResNet50ì€ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼, MobileNetì€ ë¹ ë¥´ì§€ë§Œ ëœ ì •í™•
                """)

            # ëª¨ë¸ ì„ íƒ
            models = st.multiselect(
                "ë¹„êµí•  ëª¨ë¸",
                ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                default=["ResNet50", "EfficientNet"],
                key="model_comparison"
            )

            if len(models) >= 2:
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‘œì‹œ
                metrics_df = self.transfer_helper.get_model_metrics(models)
                st.dataframe(metrics_df, use_container_width=True)

                # ì°¨íŠ¸ ìƒì„±
                fig = self.transfer_helper.create_performance_chart(models)
                st.pyplot(fig)

        elif analysis_type == "í•™ìŠµ ê³¡ì„  ë¶„ì„":
            st.subheader("ğŸ“ˆ í•™ìŠµ ê³¡ì„  ë¶„ì„")

            # ê·¸ë˜í”„ ì˜ë¯¸ ì„¤ëª…
            with st.expander("ğŸ“– ì´ ê·¸ë˜í”„ì˜ ì˜ë¯¸", expanded=True):
                st.markdown("""
                ### í•™ìŠµ ê³¡ì„  ì´í•´í•˜ê¸°

                **íŒŒë€ì„  (Training Loss)**:
                - **ì˜ë¯¸**: í•™ìŠµ ë°ì´í„°ì—ì„œì˜ ì˜¤ì°¨
                - **ì´ìƒì  íŒ¨í„´**: ì ì§„ì ìœ¼ë¡œ ê°ì†Œ
                - **ë¬¸ì œ ì‹ í˜¸**: ê°‘ìê¸° ì¦ê°€ = í•™ìŠµë¥  ë„ˆë¬´ í¼

                **ë¹¨ê°„ì„  (Validation Loss)**:
                - **ì˜ë¯¸**: ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œì˜ ì‹¤ì œ ì„±ëŠ¥
                - **ì´ìƒì  íŒ¨í„´**: Training Lossì™€ í•¨ê»˜ ê°ì†Œ
                - **ë¬¸ì œ ì‹ í˜¸**: ì¦ê°€í•˜ê¸° ì‹œì‘ = ê³¼ì í•© ë°œìƒ

                **ë…¹ìƒ‰ ì  (Best Model)**:
                - **ì˜ë¯¸**: ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ ì‹œì 
                - **í™œìš©**: ì´ ì‹œì ì˜ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ì‚¬ìš©

                **âš ï¸ ê³¼ì í•© ì§„ë‹¨**:
                - ë‘ ì„ ì˜ ê°„ê²©ì´ ë²Œì–´ì§ â†’ ê³¼ì í•©
                - í•´ê²°ì±…: Early Stopping, Dropout ì¦ê°€

                **âš ï¸ ê³¼ì†Œì í•© ì§„ë‹¨**:
                - ë‘ ì„ ì´ ëª¨ë‘ ë†’ì€ ê°’ ìœ ì§€ â†’ ê³¼ì†Œì í•©
                - í•´ê²°ì±…: ëª¨ë¸ ë³µì¡ë„ ì¦ê°€, í•™ìŠµ ì‹œê°„ ì—°ì¥
                """)

            # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
            fig = self.transfer_helper.plot_learning_curves()
            st.pyplot(fig)

        elif analysis_type == "í˜¼ë™ í–‰ë ¬":
            st.subheader("ğŸ”¢ í˜¼ë™ í–‰ë ¬ ë¶„ì„")

            # ê·¸ë˜í”„ ì˜ë¯¸ ì„¤ëª…
            with st.expander("ğŸ“– ì´ ê·¸ë˜í”„ì˜ ì˜ë¯¸", expanded=True):
                st.markdown("""
                ### í˜¼ë™ í–‰ë ¬ ì´í•´í•˜ê¸°

                **í–‰ë ¬ì˜ êµ¬ì¡°**:
                - **Yì¶• (True Label)**: ì‹¤ì œ ì •ë‹µ
                - **Xì¶• (Predicted Label)**: ëª¨ë¸ì˜ ì˜ˆì¸¡
                - **ëŒ€ê°ì„ **: ì •ë‹µì„ ë§ì¶˜ ê²½ìš° (ì§„í•œ ìƒ‰ì¼ìˆ˜ë¡ ì¢‹ìŒ)
                - **ë¹„ëŒ€ê°ì„ **: ì˜¤ë‹µ (ì—°í•œ ìƒ‰ì¼ìˆ˜ë¡ ì¢‹ìŒ)

                **ìƒ‰ìƒì˜ ì˜ë¯¸**:
                - **ì§„í•œ íŒŒë€ìƒ‰**: ë§ì€ ìƒ˜í”Œì´ ì—¬ê¸° ì†í•¨
                - **ì—°í•œ ìƒ‰/í°ìƒ‰**: ì ì€ ìƒ˜í”Œ
                - **ìˆ«ì**: í•´ë‹¹ ì¹¸ì˜ ìƒ˜í”Œ ê°œìˆ˜

                **ì‹¤ì œ í™œìš© ì˜ˆì‹œ**:
                - ê°œ/ê³ ì–‘ì´ ë¶„ë¥˜ì—ì„œ ê°œë¥¼ ê³ ì–‘ì´ë¡œ ì°©ê°: (0,1) ìœ„ì¹˜
                - ê³ ì–‘ì´ë¥¼ ê°œë¡œ ì°©ê°: (1,0) ìœ„ì¹˜

                **ì„±ëŠ¥ í‰ê°€**:
                - ëŒ€ê°ì„ ì´ ì§„í•˜ê³  ë‚˜ë¨¸ì§€ê°€ ì—°í•¨ = ìš°ìˆ˜í•œ ëª¨ë¸
                - íŠ¹ì • ì¹¸ì´ ì§„í•¨ = ê·¸ ì˜¤ë¥˜ë¥¼ ìì£¼ ë²”í•¨
                - í•´ê²°ì±…: í•´ë‹¹ í´ë˜ìŠ¤ ë°ì´í„° ë³´ê°•

                **ğŸ’¡ Tip**: ì˜ë£Œ ì§„ë‹¨ì—ì„œëŠ” False Negative(ë†“ì¹œ í™˜ì)ê°€ ë” ìœ„í—˜
                """)

            # í´ë˜ìŠ¤ ìˆ˜ ì„ íƒ
            num_classes = st.slider("í´ë˜ìŠ¤ ìˆ˜", min_value=2, max_value=10, value=5, key="confusion_classes")

            # í˜¼ë™ í–‰ë ¬ ìƒì„± ë° í‘œì‹œ
            fig = self.transfer_helper.create_confusion_matrix(num_classes)
            st.pyplot(fig)

        else:  # íŠ¹ì§• ê³µê°„ ë¶„ì„
            st.subheader("ğŸŒŒ íŠ¹ì§• ê³µê°„ ë¶„ì„")

            # ê·¸ë˜í”„ ì˜ë¯¸ ì„¤ëª…
            with st.expander("ğŸ“– ì´ ê·¸ë˜í”„ì˜ ì˜ë¯¸", expanded=True):
                st.markdown("""
                ### t-SNE íŠ¹ì§• ê³µê°„ ì‹œê°í™” ì´í•´í•˜ê¸°

                **ê·¸ë˜í”„ì˜ ì˜ë¯¸**:
                - CNNì´ í•™ìŠµí•œ ê³ ì°¨ì› íŠ¹ì§•ì„ 2Dë¡œ ì••ì¶•í•œ ì§€ë„
                - ë¹„ìŠ·í•œ ì´ë¯¸ì§€ëŠ” ê°€ê¹Œì´, ë‹¤ë¥¸ ì´ë¯¸ì§€ëŠ” ë©€ë¦¬ ë°°ì¹˜

                **ì ê³¼ í´ëŸ¬ìŠ¤í„°**:
                - **ê° ì **: í•˜ë‚˜ì˜ ì´ë¯¸ì§€
                - **ìƒ‰ìƒ**: í´ë˜ìŠ¤ (ê³ ì–‘ì´, ê°œ, ìë™ì°¨ ë“±)
                - **í´ëŸ¬ìŠ¤í„°**: ê°™ì€ ìƒ‰ ì ë“¤ì˜ ëª¨ì„

                **ì¢‹ì€ ì‹ í˜¸**:
                - ê°™ì€ ìƒ‰(í´ë˜ìŠ¤) ì ë“¤ì´ ë­‰ì³ìˆìŒ âœ…
                - ë‹¤ë¥¸ ìƒ‰ í´ëŸ¬ìŠ¤í„°ë“¤ì´ ì„œë¡œ ë¶„ë¦¬ë¨ âœ…
                - í´ëŸ¬ìŠ¤í„°ê°€ ëª…í™•í•œ ê²½ê³„ë¥¼ ê°€ì§ âœ…

                **ë‚˜ìœ ì‹ í˜¸**:
                - ìƒ‰ìƒì´ ë’¤ì„ì—¬ ìˆìŒ âŒ â†’ ëª¨ë¸ì´ êµ¬ë¶„ ëª»í•¨
                - í´ëŸ¬ìŠ¤í„°ê°€ ê²¹ì¹¨ âŒ â†’ í˜¼ë™í•˜ê¸° ì‰¬ìš´ í´ë˜ìŠ¤

                **ì‹¤ì œ í™œìš©**:
                - ê³ ì–‘ì´ì™€ ê°œê°€ ê²¹ì¹¨ â†’ ë” ë§ì€ êµ¬ë³„ íŠ¹ì§• í•™ìŠµ í•„ìš”
                - íŠ¹ì • í´ë˜ìŠ¤ë§Œ í©ì–´ì§ â†’ í•´ë‹¹ í´ë˜ìŠ¤ ë°ì´í„° í’ˆì§ˆ í™•ì¸

                **ğŸ’¡ ì¸ì‚¬ì´íŠ¸**:
                - Transfer Learning í›„ í´ëŸ¬ìŠ¤í„°ê°€ ë” ëª…í™•í•´ì§
                - Fine-tuningì´ ì˜ ë˜ì—ˆëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥
                """)

            # t-SNE ì‹œê°í™”
            fig = self.transfer_helper.visualize_feature_space()
            st.pyplot(fig)

    def _render_project_tab(self):
        """ì‹¤ì „ í”„ë¡œì íŠ¸ íƒ­"""
        st.header("ğŸš€ ì‹¤ì „ Transfer Learning í”„ë¡œì íŠ¸")

        project_type = st.selectbox(
            "í”„ë¡œì íŠ¸ ì„ íƒ",
            ["ğŸ¥ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜", "ğŸ­ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬", "ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´", "ğŸ” ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ"],
            key="project_type"
        )

        if project_type == "ğŸ¥ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜":
            self._render_medical_project()
        elif project_type == "ğŸ­ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬":
            self._render_quality_control_project()
        elif project_type == "ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´":
            self._render_style_transfer_project()
        else:
            self._render_product_search_project()

    def _render_medical_project(self):
        """ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ¥ X-ray ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹œìŠ¤í…œ")

        # API ì‚¬ìš© ì˜µì…˜
        use_api = st.checkbox("ğŸ¤– Google Gemini API ì‚¬ìš© (ì‹¤ì œ ë¶„ì„)", key="use_api_medical")

        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“š ë°ì´í„°ì…‹ & API ì‚¬ìš©ë²•", expanded=False):
            st.markdown("""
            ### 1. Chest X-Ray Dataset
            - **Kaggle**: [X-ray Pneumonia](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)
            - **ì´ë¯¸ì§€**: 5,863ê°œ (ì •ìƒ: 1,583ê°œ, íë ´: 4,273ê°œ)

            ### 2. Google Gemini Vision API
            ```python
            import google.generativeai as genai
            from PIL import Image
            import os

            # API ì„¤ì •
            genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
            model = genai.GenerativeModel('gemini-1.5-flash')

            # ì´ë¯¸ì§€ ë¶„ì„
            img = Image.open('xray.jpg')
            response = model.generate_content([
                "ì´ X-ray ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. íë ´ ê°€ëŠ¥ì„±ì´ ìˆë‚˜ìš”?",
                img
            ])
            print(response.text)
            ```

            ### 3. Transfer Learning (PyTorch)
            ```python
            model = torchvision.models.resnet50(pretrained=True)
            model.fc = nn.Linear(2048, 2)  # ì •ìƒ/íë ´
            ```
            """)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **í”„ë¡œì íŠ¸ ëª©í‘œ**:
            - í‰ë¶€ X-rayì—ì„œ íë ´ ê²€ì¶œ
            - Transfer Learningìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
            - ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
            """)

            uploaded_xray = st.file_uploader(
                "X-ray ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="xray_upload"
            )

            st.info("""
            ğŸ’¡ **í…ŒìŠ¤íŠ¸ ë°©ë²•**:
            Kaggleì—ì„œ ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”
            """)

        with col2:
            if uploaded_xray:
                st.image(uploaded_xray, caption="ì—…ë¡œë“œëœ X-ray")

                if st.button("ğŸ” ì§„ë‹¨ ì‹œì‘", key="diagnose"):
                    with st.spinner("AI ë¶„ì„ ì¤‘..."):
                        if use_api:
                            # Google Gemini API ì‚¬ìš©
                            try:
                                import os
                                import google.generativeai as genai
                                from PIL import Image

                                # API ì„¤ì •
                                api_key = os.getenv('GOOGLE_API_KEY')
                                if api_key:
                                    genai.configure(api_key=api_key)
                                    model = genai.GenerativeModel('gemini-1.5-flash')

                                    # ì´ë¯¸ì§€ ì—´ê¸°
                                    img = Image.open(uploaded_xray)

                                    # API í˜¸ì¶œ
                                    prompt = """
                                    ì´ í‰ë¶€ X-ray ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
                                    1. íë ´ ê°€ëŠ¥ì„± (í¼ì„¼íŠ¸)
                                    2. ì£¼ìš” ì†Œê²¬
                                    3. ê¶Œì¥ì‚¬í•­
                                    ì°¸ê³ : ì´ê²ƒì€ êµìœ¡ ëª©ì ì´ë©° ì˜ë£Œ ì§„ë‹¨ì´ ì•„ë‹™ë‹ˆë‹¤.
                                    """
                                    response = model.generate_content([prompt, img])

                                    st.success("âœ… API ë¶„ì„ ì™„ë£Œ!")
                                    st.write("**Gemini ë¶„ì„ ê²°ê³¼:**")
                                    st.info(response.text)
                                else:
                                    st.error("ğŸ”´ API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"API ì˜¤ë¥˜: {str(e)}")
                                st.info("ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                                # ì‹œë®¬ë ˆì´ì…˜ í´ë°±
                                import random
                                normal_prob = random.randint(10, 90)
                                pneumonia_prob = 100 - normal_prob
                                st.metric("ì •ìƒ í™•ë¥ ", f"{normal_prob}%")
                                st.metric("íë ´ í™•ë¥ ", f"{pneumonia_prob}%")
                        else:
                            # ì‹œë®¬ë ˆì´ì…˜
                            import random
                            normal_prob = random.randint(10, 90)
                            pneumonia_prob = 100 - normal_prob

                            st.success("ë¶„ì„ ì™„ë£Œ!")
                            st.metric("ì •ìƒ í™•ë¥ ", f"{normal_prob}%")
                            st.metric("íë ´ í™•ë¥ ", f"{pneumonia_prob}%",
                                    delta="ì£¼ì˜ í•„ìš”" if pneumonia_prob > 60 else "ì •ìƒ ë²”ìœ„")

                        st.caption("âš ï¸ êµìœ¡ ëª©ì ì…ë‹ˆë‹¤. ì‹¤ì œ ì˜ë£Œ ì§„ë‹¨ì´ ì•„ë‹™ë‹ˆë‹¤.")

    def _render_quality_control_project(self):
        """ì œì¡°ì—… í˜ˆì§ˆ ê²€ì‚¬ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ­ PCB ë¶ˆëŸ‰ ê²€ì¶œ ì‹œìŠ¤í…œ")

        # API ì‚¬ìš© ì˜µì…˜
        use_api = st.checkbox("ğŸ¤– Google Vision API ì‚¬ìš© (ê°ì²´ ê²€ì¶œ)", key="use_api_pcb")

        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“š ë°ì´í„°ì…‹ & API ì‚¬ìš©ë²•", expanded=False):
            st.markdown("""
            ### PCB Defect Detection Dataset
            - **Kaggle**: [PCB Defects](https://www.kaggle.com/datasets/akhatova/pcb-defects)
            - **GitHub**: [DeepPCB](https://github.com/tangsanli5201/DeepPCB)
            - **ì´ë¯¸ì§€ ìˆ˜**: 1,386ê°œ (6ê°€ì§€ ë¶ˆëŸ‰ ìœ í˜•)
            - **ë¶ˆëŸ‰ ìœ í˜•**: êµ¬ë© ëˆ„ë½, ë‹¨ë½, ê°œë°© íšŒë¡œ, ìŠ¤í¼, ë§ˆìš°ìŠ¤ ë°”ì´íŠ¸, ì´ë¬¼ì§ˆ

            **ğŸ’¡ ì‹¤ì œ êµ¬í˜„ ì‹œ**:
            ```python
            # YOLOë¥¼ ì‚¬ìš©í•œ PCB ë¶ˆëŸ‰ ê²€ì¶œ
            model = YOLOv5('yolov5s')
            model.load_state_dict(pretrained_weights)
            # PCB ë°ì´í„°ë¡œ Fine-tuning
            ```
            """)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **ì‹œìŠ¤í…œ íŠ¹ì§•**:
            - ì‹¤ì‹œê°„ ë¶ˆëŸ‰í’ˆ ê²€ì¶œ
            - 6ê°€ì§€ ë¶ˆëŸ‰ ìœ í˜• ë¶„ë¥˜
            - Transfer Learningìœ¼ë¡œ ë¹ ë¥¸ ë°°í¬
            """)

            uploaded_pcb = st.file_uploader(
                "PCB ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="pcb_upload"
            )

            # ë¶ˆëŸ‰ ìœ í˜• ì„¤ì •
            defect_types = st.multiselect(
                "ê²€ì¶œí•  ë¶ˆëŸ‰ ìœ í˜•",
                ["êµ¬ë© ëˆ„ë½", "ë‹¨ë½", "ê°œë°© íšŒë¡œ", "ìŠ¤í¼", "ë§ˆìš°ìŠ¤ ë°”ì´íŠ¸", "ì´ë¬¼ì§ˆ"],
                default=["ë‹¨ë½", "ê°œë°© íšŒë¡œ"],
                key="defect_types"
            )

        with col2:
            if uploaded_pcb:
                st.image(uploaded_pcb, caption="ì—…ë¡œë“œëœ PCB")

                if st.button("ğŸ” ê²€ì‚¬ ì‹œì‘", key="start_qc"):
                    with st.spinner("PCB ë¶„ì„ ì¤‘..."):
                        if use_api:
                            # Google Vision API ì‚¬ìš©
                            try:
                                import os
                                import google.generativeai as genai
                                from PIL import Image

                                api_key = os.getenv('GOOGLE_API_KEY')
                                if api_key:
                                    genai.configure(api_key=api_key)
                                    model = genai.GenerativeModel('gemini-1.5-flash')

                                    img = Image.open(uploaded_pcb)
                                    prompt = f"""
                                    ì´ PCB ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë¶ˆëŸ‰ì„ ê²€ì¶œí•´ì£¼ì„¸ìš”:
                                    {", ".join(defect_types)}

                                    ê²°ê³¼ í˜•ì‹:
                                    1. ë¶ˆëŸ‰ ì¢…ë¥˜
                                    2. ìœ„ì¹˜ (ê°€ëŠ¥í•˜ë©´)
                                    3. ì‹¬ê°ë„
                                    """

                                    response = model.generate_content([prompt, img])
                                    st.success("âœ… API ê²€ì‚¬ ì™„ë£Œ!")
                                    st.write("**Gemini ë¶„ì„ ê²°ê³¼:**")
                                    st.info(response.text)
                                else:
                                    st.error("API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"API ì˜¤ë¥˜: {str(e)}")
                        else:
                            # ì‹œë®¬ë ˆì´ì…˜
                            import random
                            st.success("ê²€ì‚¬ ì™„ë£Œ!")

                            if random.random() > 0.3:  # 70% í™•ë¥ ë¡œ ë¶ˆëŸ‰
                                defect = random.choice(defect_types) if defect_types else "ë‹¨ë½"
                                st.error(f"âš ï¸ ë¶ˆëŸ‰ ê²€ì¶œ: {defect}")
                                st.metric("ë¶ˆëŸ‰ ìœ„ì¹˜", "(234, 567) í”½ì…€")
                                st.metric("ì‹ ë¢°ë„", f"{random.randint(85, 99)}%")
                            else:
                                st.success("âœ… ì •ìƒ ì œí’ˆ")

                        st.caption("âš ï¸ êµìœ¡ ëª©ì ì…ë‹ˆë‹¤.")

    def _render_style_transfer_project(self):
        """ìŠ¤íƒ€ì¼ ì „ì´ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ¨ Neural Style Transfer")

        # API ì‚¬ìš© ì˜µì…˜
        use_api = st.checkbox("ğŸ¤– Google Gemini API ì‚¬ìš© (ìŠ¤íƒ€ì¼ ë¶„ì„)", key="use_api_style")

        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“š ìœ ëª… ì˜ˆìˆ  ì‘í’ˆ ìŠ¤íƒ€ì¼ & API ì‚¬ìš©ë²•", expanded=False):
            st.markdown("""
            ### ê³µê°œ ë„ë©”ì¸ ì˜ˆìˆ  ì‘í’ˆ
            - **Van Gogh - Starry Night**: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg)
            - **Monet ì‘í’ˆë“¤**: ì¸ìƒíŒŒ ìŠ¤íƒ€ì¼
            - **Picasso ì‘í’ˆë“¤**: íë¹„ì¦˜ ìŠ¤íƒ€ì¼

            **ğŸ’¡ ì‹¤ì œ êµ¬í˜„ ì‹œ (PyTorch)**:
            ```python
            # VGG19 ëª¨ë¸ ì‚¬ìš©
            vgg = torchvision.models.vgg19(pretrained=True).features
            # Content loss + Style loss ìµœì í™”
            total_loss = content_weight * content_loss + style_weight * style_loss
            ```

            **ğŸ¤– Gemini API ì‚¬ìš© (ì´ë¯¸ì§€ ë¶„ì„)**:
            ```python
            import google.generativeai as genai
            genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
            model = genai.GenerativeModel('gemini-2.5-flash-image-preview')

            prompt = f"ì´ ì´ë¯¸ì§€ë¥¼ {style_name} ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”"
            response = model.generate_content([prompt, content_img])
            ```
            """)

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("**ì½˜í…ì¸  ì´ë¯¸ì§€**")
            content_image = st.file_uploader(
                "ì‚¬ì§„ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="content_img"
            )
            if content_image:
                st.image(content_image, caption="ì½˜í…ì¸ ")

        with col2:
            st.markdown("**ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€**")
            style_choice = st.radio(
                "ìŠ¤íƒ€ì¼ ì„ íƒ",
                ["ì§ì ‘ ì—…ë¡œë“œ", "Van Gogh", "Monet", "Picasso"],
                key="style_choice"
            )

            if style_choice == "ì§ì ‘ ì—…ë¡œë“œ":
                style_image = st.file_uploader(
                    "ìŠ¤íƒ€ì¼ ì—…ë¡œë“œ",
                    type=['png', 'jpg', 'jpeg'],
                    key="style_img"
                )
                if style_image:
                    st.image(style_image, caption="ìŠ¤íƒ€ì¼")
            else:
                st.info(f"ğŸ¨ {style_choice} ìŠ¤íƒ€ì¼ì´ ì„ íƒë¨")
                style_image = True  # ë”ë¯¸ ê°’

        with col3:
            st.markdown("**ê²°ê³¼ ì´ë¯¸ì§€**")
            if content_image and style_image:
                style_weight = st.slider("ìŠ¤íƒ€ì¼ ê°•ë„", 0.0, 1.0, 0.5, key="style_weight")

                if st.button("ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´", key="transfer_style"):
                    with st.spinner("ìŠ¤íƒ€ì¼ ì „ì´ ì¤‘..."):
                        if use_api:
                            # Google Gemini API ì‚¬ìš©
                            try:
                                import os
                                import google.generativeai as genai
                                from PIL import Image

                                api_key = os.getenv('GOOGLE_API_KEY')
                                if api_key:
                                    genai.configure(api_key=api_key)
                                    model = genai.GenerativeModel('gemini-2.5-flash-image-preview')

                                    img = Image.open(content_image)
                                    style_name = style_choice if style_choice != "ì§ì ‘ ì—…ë¡œë“œ" else "ì—…ë¡œë“œëœ ìŠ¤íƒ€ì¼"

                                    prompt = f"""
                                    ì´ ì´ë¯¸ì§€ë¥¼ {style_name} ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
                                    ì•„í‹°ìŠ¤íŠ¸ì˜ íŠ¹ì§•ì ì¸ ë¶“í„°ì¹˜, ìƒ‰ìƒ íŒ”ë ˆíŠ¸, êµ¬ë„ë¥¼ ì ìš©í•´ì£¼ì„¸ìš”.
                                    ìŠ¤íƒ€ì¼ ê°•ë„: {style_weight * 100}%
                                    """

                                    response = model.generate_content([prompt, img])
                                    st.success("âœ… API ìŠ¤íƒ€ì¼ ë¶„ì„ ì™„ë£Œ!")
                                    st.write("**Gemini ìŠ¤íƒ€ì¼ ë¶„ì„ ê²°ê³¼:**")
                                    st.info(response.text)
                                    st.caption("ğŸ’¡ ì°¸ê³ : GeminiëŠ” ìŠ¤íƒ€ì¼ ë¶„ì„ ë° ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±ì€ ë³„ë„ì˜ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                                else:
                                    st.error("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"API ì˜¤ë¥˜: {str(e)}")
                                st.info("ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                                import time
                                time.sleep(2)
                                st.image(content_image, caption="ê²°ê³¼ (ì‹œë®¬ë ˆì´ì…˜)")
                        else:
                            # ì‹œë®¬ë ˆì´ì…˜
                            import time
                            time.sleep(2)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                            st.success("ì™„ë£Œ!")
                            st.image(content_image, caption="ê²°ê³¼ (ì‹œë®¬ë ˆì´ì…˜)")
                            st.caption("âš ï¸ ì‹¤ì œ ìŠ¤íƒ€ì¼ ì „ì´ê°€ ì•„ë‹Œ ì‹œë®¬ë ˆì´ì…˜ì…ë‹ˆë‹¤")

    def _render_product_search_project(self):
        """ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ” ì‹œê°ì  ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ")

        # API ì‚¬ìš© ì˜µì…˜
        use_api = st.checkbox("ğŸ¤– Google Gemini API ì‚¬ìš© (ì‹¤ì œ ê²€ìƒ‰)", key="use_api_search")

        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ & API ì‚¬ìš©ë²•", expanded=False):
            st.markdown("""
            ### Fashion-MNIST Dataset
            - **GitHub**: [zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)
            - **Kaggle**: [Fashion MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)
            - **ì´ë¯¸ì§€ ìˆ˜**: 70,000ê°œ (28x28 í‘ë°±)
            - **ì¹´í…Œê³ ë¦¬**: í‹°ì…”ì¸ , ë°”ì§€, ë“œë ˆìŠ¤, ì½”íŠ¸, ìƒŒë“¤, ì…”ì¸ , ìŠ¤ë‹ˆì»¤ì¦ˆ, ê°€ë°©, ë¶€ì¸ 

            **ğŸ’¡ ì‹¤ì œ êµ¬í˜„ ì‹œ (CLIP)**:
            ```python
            # CLIP ëª¨ë¸ ì‚¬ìš© (í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë§¤ì¹­)
            import clip
            model, preprocess = clip.load("ViT-B/32")
            # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„ë² ë”© ë¹„êµ
            ```

            **ğŸ¤– Gemini API ì‚¬ìš© (ì´ë¯¸ì§€ ë¶„ì„ & ê²€ìƒ‰)**:
            ```python
            import google.generativeai as genai
            genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
            model = genai.GenerativeModel('gemini-1.5-flash')

            # ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
            response = model.generate_content(["ì´ ìƒí’ˆì„ ì„¤ëª…í•´ì£¼ì„¸ìš”", image])
            # ìœ ì‚¬ ìƒí’ˆ ì¶”ì²œ
            ```
            """)

        search_method = st.radio(
            "ê²€ìƒ‰ ë°©ë²•",
            ["í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰", "ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"],
            key="search_method"
        )

        col1, col2 = st.columns(2)

        with col1:
            if search_method == "í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰":
                query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥", placeholder="ë¹¨ê°„ ìš´ë™í™”", key="text_query")
                if st.button("ğŸ” ê²€ìƒ‰", key="search_text"):
                    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                        if use_api and query:
                            try:
                                import os
                                import google.generativeai as genai

                                api_key = os.getenv('GOOGLE_API_KEY')
                                if api_key:
                                    genai.configure(api_key=api_key)
                                    model = genai.GenerativeModel('gemini-1.5-flash')

                                    prompt = f"""
                                    íŒ¨ì…˜ ìƒí’ˆ ê²€ìƒ‰: "{query}"

                                    ì´ ê²€ìƒ‰ì–´ì™€ ê°€ì¥ ì˜ ë§¤ì¹­ë˜ëŠ” ìƒí’ˆ 3ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
                                    ê° ìƒí’ˆì— ëŒ€í•´:
                                    1. ìƒí’ˆëª…
                                    2. ë§¤ì¹­ ì ìˆ˜ (0-100%)
                                    3. ì¶”ì²œ ì´ìœ 
                                    """

                                    response = model.generate_content(prompt)
                                    st.success("âœ… API ê²€ìƒ‰ ì™„ë£Œ!")
                                    st.write("**Gemini ê²€ìƒ‰ ê²°ê³¼:**")
                                    st.info(response.text)
                                else:
                                    st.error("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"API ì˜¤ë¥˜: {str(e)}")
                        else:
                            st.success("ìœ ì‚¬í•œ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

            elif search_method == "ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰":
                query_img = st.file_uploader("ì°¸ì¡° ì´ë¯¸ì§€", type=['png', 'jpg', 'jpeg'], key="img_query")
                if query_img:
                    st.image(query_img, caption="ê²€ìƒ‰ ì´ë¯¸ì§€")
                    if st.button("ğŸ” ê²€ìƒ‰", key="search_img"):
                        with st.spinner("ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘..."):
                            if use_api:
                                try:
                                    import os
                                    import google.generativeai as genai
                                    from PIL import Image

                                    api_key = os.getenv('GOOGLE_API_KEY')
                                    if api_key:
                                        genai.configure(api_key=api_key)
                                        model = genai.GenerativeModel('gemini-1.5-flash')

                                        img = Image.open(query_img)
                                        prompt = """
                                        ì´ ìƒí’ˆ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³ , ìœ ì‚¬í•œ ìƒí’ˆ 3ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
                                        ê° ìƒí’ˆì— ëŒ€í•´:
                                        1. ìƒí’ˆ ì¹´í…Œê³ ë¦¬
                                        2. ìœ ì‚¬ë„ ì ìˆ˜ (0-100%)
                                        3. ì¶”ì²œ ì´ìœ 
                                        """

                                        response = model.generate_content([prompt, img])
                                        st.success("âœ… API ê²€ìƒ‰ ì™„ë£Œ!")
                                        st.write("**Gemini ê²€ìƒ‰ ê²°ê³¼:**")
                                        st.info(response.text)
                                    else:
                                        st.error("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                except Exception as e:
                                    st.error(f"API ì˜¤ë¥˜: {str(e)}")
                            else:
                                st.success("ìœ ì‚¬í•œ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

            else:  # í•˜ì´ë¸Œë¦¬ë“œ
                text_q = st.text_input("í…ìŠ¤íŠ¸", placeholder="í¸ì•ˆí•œ ìš´ë™í™”", key="hybrid_text")
                img_q = st.file_uploader("ì´ë¯¸ì§€", type=['png', 'jpg', 'jpeg'], key="hybrid_img")
                if st.button("ğŸ” ê²€ìƒ‰", key="search_hybrid"):
                    with st.spinner("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤‘..."):
                        if use_api and text_q and img_q:
                            try:
                                import os
                                import google.generativeai as genai
                                from PIL import Image

                                api_key = os.getenv('GOOGLE_API_KEY')
                                if api_key:
                                    genai.configure(api_key=api_key)
                                    model = genai.GenerativeModel('gemini-1.5-flash')

                                    img = Image.open(img_q)
                                    prompt = f"""
                                    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰:
                                    - í…ìŠ¤íŠ¸ ì¿¼ë¦¬: "{text_q}"
                                    - ì´ë¯¸ì§€: ì—…ë¡œë“œëœ ì°¸ì¡° ì´ë¯¸ì§€

                                    í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ìƒí’ˆ 3ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
                                    ê° ìƒí’ˆì— ëŒ€í•´:
                                    1. ìƒí’ˆëª…
                                    2. ë§¤ì¹­ ì ìˆ˜ (0-100%)
                                    3. ì¶”ì²œ ì´ìœ 
                                    """

                                    response = model.generate_content([prompt, img])
                                    st.success("âœ… API ê²€ìƒ‰ ì™„ë£Œ!")
                                    st.write("**Gemini ê²€ìƒ‰ ê²°ê³¼:**")
                                    st.info(response.text)
                                else:
                                    st.error("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"API ì˜¤ë¥˜: {str(e)}")
                        else:
                            st.success("ìµœì ì˜ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

        with col2:
            st.markdown("**ê²€ìƒ‰ ê²°ê³¼**")
            # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
            if 'search' in st.session_state and any(key.startswith('search_') and st.session_state.get(key) for key in ['search_text', 'search_img', 'search_hybrid']):
                import random
                products = ["í‹°ì…”ì¸ ", "ìš´ë™í™”", "ê°€ë°©", "ë°”ì§€", "ë“œë ˆìŠ¤"]
                for i in range(3):
                    product = random.choice(products)
                    similarity = random.randint(75, 98)
                    st.metric(f"ìƒí’ˆ {i+1}", product, f"{similarity}% ìœ ì‚¬ë„")
                st.caption("âš ï¸ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì…ë‹ˆë‹¤.")

    def _show_model_info(self, model_name):
        """ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
        model_info = {
            "ResNet50": {
                "parameters": "25.6M",
                "layers": "50",
                "year": "2015",
                "accuracy": "92.1%"
            },
            "VGG16": {
                "parameters": "138M",
                "layers": "16",
                "year": "2014",
                "accuracy": "90.1%"
            },
            "EfficientNet": {
                "parameters": "5.3M",
                "layers": "Variable",
                "year": "2019",
                "accuracy": "91.7%"
            },
            "MobileNet": {
                "parameters": "4.2M",
                "layers": "28",
                "year": "2017",
                "accuracy": "89.5%"
            },
            "DenseNet": {
                "parameters": "25.6M",
                "layers": "121",
                "year": "2016",
                "accuracy": "91.8%"
            }
        }

        if model_name in model_info:
            info = model_info[model_name]
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Parameters", info["parameters"])
            col2.metric("Layers", info["layers"])
            col3.metric("Year", info["year"])
            col4.metric("ImageNet Top-5", info["accuracy"])