"""
Week 3: Transfer Learning & Multi-modal API ëª¨ë“ˆ
Transfer Learningê³¼ Multi-modal API ê´€ë ¨ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import streamlit as st
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from typing import Dict, List, Tuple, Optional
import cv2
import matplotlib.pyplot as plt
import os
import sys

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from core.base_processor import BaseImageProcessor
from core.ai_models import AIModelManager
from .transfer_helpers import TransferLearningHelper
from .multimodal_helpers import MultiModalHelper


class TransferLearningModule(BaseImageProcessor):
    """Transfer Learning ë° Multi-modal API í•™ìŠµ ëª¨ë“ˆ"""

    def __init__(self):
        super().__init__()
        self.ai_manager = AIModelManager()
        self.transfer_helper = TransferLearningHelper()
        self.multimodal_helper = MultiModalHelper()

    def render(self):
        """Week 3 ëª¨ë“ˆ UI ë Œë”ë§ - Week 2ì™€ ë™ì¼í•œ ë©”ì„œë“œëª…"""
        self.render_ui()

    def render_ui(self):
        """Week 3 ëª¨ë“ˆ UI ë Œë”ë§"""
        st.title("ğŸ”„ Week 3: Transfer Learning & Multi-modal API")
        st.markdown("---")

        # íƒ­ ìƒì„±
        tabs = st.tabs([
            "ğŸ“š ì´ë¡ ",
            "ğŸ”„ Transfer Learning",
            "ğŸ–¼ï¸ CLIP ê²€ìƒ‰",
            "ğŸ” API ë¹„êµ",
            "ğŸ¨ íŠ¹ì§• ì¶”ì¶œ",
            "ğŸ“Š í†µí•© ë¶„ì„",
            "ğŸš€ ì‹¤ì „ í”„ë¡œì íŠ¸"
        ])

        with tabs[0]:
            self._render_theory_tab()

        with tabs[1]:
            self._render_transfer_learning_tab()

        with tabs[2]:
            self._render_clip_search_tab()

        with tabs[3]:
            self._render_api_comparison_tab()

        with tabs[4]:
            self._render_feature_extraction_tab()

        with tabs[5]:
            self._render_integrated_analysis_tab()

        with tabs[6]:
            self._render_project_tab()

    def _render_theory_tab(self):
        """ì´ë¡  íƒ­"""
        st.header("ğŸ“– Transfer Learning & Multi-modal ì´ë¡ ")

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("1. Transfer Learningì´ë€?")
            st.markdown("""
            - **ì •ì˜**: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ìƒˆë¡œìš´ ì‘ì—…ì— í™œìš©
            - **ì¥ì **: ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥
            - **ë°©ë²•**: Feature Extraction, Fine-tuning
            - **ì‘ìš©**: ì˜ë£Œ ì˜ìƒ, ì œí’ˆ ê²€ì‚¬ ë“±
            """)

            st.subheader("2. ì£¼ìš” ê¸°ë²•")
            st.markdown("""
            - **Feature Extraction**: ë§ˆì§€ë§‰ ì¸µë§Œ í•™ìŠµ
            - **Fine-tuning**: ì „ì²´ ë˜ëŠ” ì¼ë¶€ ì¸µ ì¬í•™ìŠµ
            - **Domain Adaptation**: ë„ë©”ì¸ ê°„ ì§€ì‹ ì „ì´
            - **Few-shot Learning**: ë§¤ìš° ì ì€ ìƒ˜í”Œë¡œ í•™ìŠµ
            """)

        with col2:
            st.subheader("3. Multi-modal Learning")
            st.markdown("""
            - **CLIP**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì—°ê²°
            - **DALL-E**: í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
            - **Flamingo**: ë¹„ì „-ì–¸ì–´ ì´í•´
            - **ALIGN**: ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸
            """)

            st.subheader("4. ì‹¤ì œ í™œìš© ì‚¬ë¡€")
            st.markdown("""
            - **ì˜ë£Œ AI**: X-ray, MRI ë¶„ì„
            - **ììœ¨ì£¼í–‰**: ê°ì²´ ì¸ì‹ ë° ì¶”ì 
            - **í’ˆì§ˆ ê²€ì‚¬**: ì œì¡°ì—… ë¶ˆëŸ‰ ê²€ì¶œ
            - **ì½˜í…ì¸  ê²€ìƒ‰**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰
            """)

    def _render_transfer_learning_tab(self):
        """Transfer Learning íƒ­"""
        st.header("ğŸ”„ Transfer Learning ì‹¤ìŠµ")

        # íƒ­ ìƒì„±: ì˜ˆì œ í•™ìŠµê³¼ ì‚¬ìš©ì ì´ë¯¸ì§€ ë¶„ì„
        sub_tabs = st.tabs(["ğŸ“š ì˜ˆì œë¡œ í•™ìŠµí•˜ê¸°", "ğŸ”§ ë‚´ ëª¨ë¸ Fine-tuningí•˜ê¸°"])

        with sub_tabs[0]:
            st.markdown("### 1. ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ì„ íƒ")

            col1, col2, col3 = st.columns(3)

            with col1:
                model_name = st.selectbox(
                    "ëª¨ë¸ ì„ íƒ",
                    ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                    key="model_select_example"
                )

            with col2:
                pretrained = st.checkbox("ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©", value=True, key="pretrained_example")

            with col3:
                num_classes = st.number_input("ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜", min_value=2, value=10, key="num_classes_example")

            # ëª¨ë¸ ì •ë³´ í‘œì‹œ
            if st.button("ëª¨ë¸ ì •ë³´ ë³´ê¸°", key="model_info_example"):
                self._show_model_info(model_name)

            st.markdown("### 2. Transfer Learning ë°©ë²•")

            method = st.radio(
                "í•™ìŠµ ë°©ë²• ì„ íƒ",
                ["Feature Extraction (ë¹ ë¦„)", "Fine-tuning (ì •í™•í•¨)", "ì „ì²´ í•™ìŠµ (ëŠë¦¼)"],
                key="method_example"
            )

            # ì½”ë“œ ì˜ˆì‹œ
            with st.expander("ğŸ“ ì½”ë“œ ë³´ê¸°"):
                code = self.transfer_helper.get_transfer_learning_code(model_name, num_classes, method)
                st.code(code, language="python")

        with sub_tabs[1]:
            st.markdown("### ğŸ”§ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ Fine-tuning")

            # íŒŒì¼ ì—…ë¡œë“œ
            uploaded_files = st.file_uploader(
                "í•™ìŠµí•  ì´ë¯¸ì§€ ì—…ë¡œë“œ (í´ë˜ìŠ¤ë³„ë¡œ í´ë” êµ¬ë¶„)",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="custom_dataset"
            )

            if uploaded_files:
                col1, col2 = st.columns(2)

                with col1:
                    model_choice = st.selectbox(
                        "ë² ì´ìŠ¤ ëª¨ë¸",
                        ["ResNet50", "EfficientNet-B0", "MobileNetV2"],
                        key="model_custom"
                    )

                    learning_rate = st.slider(
                        "í•™ìŠµë¥ ",
                        min_value=0.0001,
                        max_value=0.01,
                        value=0.001,
                        format="%.4f",
                        key="lr_custom"
                    )

                with col2:
                    epochs = st.slider("ì—í­ ìˆ˜", min_value=1, max_value=50, value=10, key="epochs_custom")
                    batch_size = st.select_slider("ë°°ì¹˜ í¬ê¸°", options=[8, 16, 32, 64], value=32, key="batch_custom")

                if st.button("ğŸš€ Fine-tuning ì‹œì‘", key="start_finetuning"):
                    with st.spinner("ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì¤‘..."):
                        # ì‹¤ì œ fine-tuning ë¡œì§ì€ ì—¬ê¸°ì— êµ¬í˜„
                        st.info("Fine-tuning ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")
                        progress_bar = st.progress(0)
                        for i in range(epochs):
                            progress_bar.progress((i + 1) / epochs)
                        st.success("Fine-tuning ì™„ë£Œ!")

    def _render_clip_search_tab(self):
        """CLIP Image Search íƒ­"""
        st.header("ğŸ–¼ï¸ CLIPì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ê²€ìƒ‰")

        # íƒ­ ìƒì„±
        sub_tabs = st.tabs(["ğŸ” í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰", "ğŸ–¼ï¸ ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰", "ğŸ“Š ì„ë² ë”© ì‹œê°í™”"])

        with sub_tabs[0]:
            st.markdown("### í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰")

            search_query = st.text_input(
                "ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì…ë ¥",
                placeholder="ì˜ˆ: ë¹¨ê°„ ìë™ì°¨, í–‰ë³µí•œ ê°•ì•„ì§€, ì¼ëª° í•´ë³€",
                key="clip_text_search"
            )

            # ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤
            uploaded_images = st.file_uploader(
                "ê²€ìƒ‰í•  ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_text"
            )

            if search_query and uploaded_images:
                if st.button("ğŸ” CLIP ê²€ìƒ‰ ì‹¤í–‰", key="run_clip_text"):
                    with st.spinner("CLIP ëª¨ë¸ë¡œ ê²€ìƒ‰ ì¤‘..."):
                        # CLIP ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
                        st.info(f"'{search_query}'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ” ì¤‘...")

                        # ê²°ê³¼ í‘œì‹œ (ì‹œë®¬ë ˆì´ì…˜)
                        cols = st.columns(3)
                        for i, img_file in enumerate(uploaded_images[:3]):
                            if i < 3:
                                img = Image.open(img_file)
                                cols[i].image(img, caption=f"ìœ ì‚¬ë„: {np.random.uniform(0.7, 0.95):.2%}")

        with sub_tabs[1]:
            st.markdown("### ì´ë¯¸ì§€ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰")

            query_image = st.file_uploader(
                "ì¿¼ë¦¬ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="clip_query_image"
            )

            db_images = st.file_uploader(
                "ê²€ìƒ‰í•  ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_image"
            )

            if query_image and db_images:
                col1, col2 = st.columns([1, 2])

                with col1:
                    st.image(query_image, caption="ì¿¼ë¦¬ ì´ë¯¸ì§€")

                with col2:
                    if st.button("ğŸ” ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰", key="run_clip_image"):
                        st.info("ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰ ì¤‘...")

        with sub_tabs[2]:
            st.markdown("### ğŸ“Š CLIP ì„ë² ë”© ì‹œê°í™”")

            if st.button("ì„ë² ë”© ê³µê°„ ì‹œê°í™”", key="visualize_embeddings"):
                # ì„ë² ë”© ì‹œê°í™” (ì‹œë®¬ë ˆì´ì…˜)
                fig = self.multimodal_helper.visualize_clip_embeddings()
                st.pyplot(fig)

    def _render_api_comparison_tab(self):
        """Multi-modal API ë¹„êµ íƒ­"""
        st.header("ğŸ” Multi-modal API ë¹„êµ ë¶„ì„")

        # 2025ë…„ 9ì›” ê¸°ì¤€ API ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“… 2025ë…„ 9ì›” ê¸°ì¤€ API ì ‘ê·¼ ë°©ë²•", expanded=True):
            st.markdown("""
            ### ğŸ”— OpenAI CLIP
            - **ì ‘ê·¼ ë°©ì‹**: ì˜¤í”ˆì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (API ì„œë¹„ìŠ¤ ì•„ë‹˜)
            - **ì„¤ì¹˜**: `pip install git+https://github.com/openai/CLIP.git`
            - **íŠ¹ì§•**: API í‚¤ ë¶ˆí•„ìš”, ì™„ì „ ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰
            - **ì‘ë‹µ ì†ë„**: <100ms (GPU ì‚¬ìš© ì‹œ)

            ### ğŸ¤– Google Gemini API (2025ë…„ ê¶Œì¥)
            - **Vision API ëŒ€ì²´**: Geminiê°€ Vision APIë¥¼ ëŒ€ì²´í•˜ëŠ” ì¶”ì„¸
            - **Google AI Studio ì ‘ê·¼ ë°©ë²•**:
              1. ai.google.dev ì ‘ì†
              2. Google ê³„ì • ë¡œê·¸ì¸
              3. "Get API key" í´ë¦­
              4. "Create API key in new project" ì„ íƒ
              5. API í‚¤ ìƒì„± (í˜•ì‹: AIza...)
            - **ë¬´ë£Œ í• ë‹¹ëŸ‰**: ë¶„ë‹¹ 60ê±´, ì‹ ìš©ì¹´ë“œ ë¶ˆí•„ìš”
            - **ê°•ì **: ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬, PDF ì§ì ‘ ì²˜ë¦¬, 90ë¶„ ë¹„ë””ì˜¤ ì§€ì›

            ### ğŸ¤— Hugging Face API
            - **í† í° ìƒì„±**: HuggingFace.co â†’ Settings â†’ Access Tokens â†’ New Token
            - **í† í° í˜•ì‹**: `hf_xxxxx`
            - **2025ë…„ ê¶Œì¥**: Fine-grained í† í°, ì•±ë³„ ë³„ë„ í† í° ìƒì„±
            """)

        st.markdown("---")

        # API ì„ íƒ
        selected_apis = st.multiselect(
            "ë¹„êµí•  API ì„ íƒ",
            ["OpenAI CLIP", "Google Vision API", "Azure Computer Vision",
             "AWS Rekognition", "Hugging Face", "OpenAI GPT-4V"],
            default=["OpenAI CLIP", "Google Vision API", "Hugging Face"],
            key="api_comparison"
        )

        if len(selected_apis) >= 2:
            # ë¹„êµ ì°¨íŠ¸ ìƒì„±
            st.subheader("ğŸ“Š API ê¸°ëŠ¥ ë¹„êµ")

            comparison_df = self.multimodal_helper.get_api_comparison_data(selected_apis)
            st.dataframe(comparison_df, use_container_width=True)

            # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            st.subheader("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")

            col1, col2 = st.columns(2)

            with col1:
                # ì†ë„ ë¹„êµ ì°¨íŠ¸
                fig_speed = self.multimodal_helper.create_speed_comparison_chart(selected_apis)
                st.pyplot(fig_speed)

            with col2:
                # ì •í™•ë„ ë¹„êµ ì°¨íŠ¸
                fig_accuracy = self.multimodal_helper.create_accuracy_comparison_chart(selected_apis)
                st.pyplot(fig_accuracy)

            # ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ
            st.subheader("ğŸ’¡ ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ")

            use_case = st.selectbox(
                "ì‚¬ìš© ì‚¬ë¡€ ì„ íƒ",
                ["ì´ë¯¸ì§€ ê²€ìƒ‰", "ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜", "ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„",
                 "ì œí’ˆ ì¶”ì²œ", "ìë™ íƒœê¹…", "ì‹œê°ì  ì§ˆì˜ì‘ë‹µ"],
                key="use_case"
            )

            recommendation = self.multimodal_helper.get_api_recommendation(use_case, selected_apis)
            st.info(recommendation)

    def _render_feature_extraction_tab(self):
        """íŠ¹ì§• ì¶”ì¶œ íƒ­"""
        st.header("ğŸ¨ íŠ¹ì§• ì¶”ì¶œ ë° ì‹œê°í™”")

        uploaded_file = st.file_uploader(
            "ì´ë¯¸ì§€ ì—…ë¡œë“œ",
            type=['png', 'jpg', 'jpeg'],
            key="feature_extraction"
        )

        if uploaded_file:
            image = Image.open(uploaded_file)

            col1, col2 = st.columns([1, 1])

            with col1:
                st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€")

                model_choice = st.selectbox(
                    "íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸",
                    ["ResNet50", "VGG16", "EfficientNet", "CLIP"],
                    key="feature_model"
                )

                layer_choice = st.selectbox(
                    "ì¶”ì¶œí•  ë ˆì´ì–´",
                    ["Early layers", "Middle layers", "Late layers", "Final layer"],
                    key="feature_layer"
                )

            with col2:
                if st.button("ğŸ¨ íŠ¹ì§• ì¶”ì¶œ", key="extract_features"):
                    with st.spinner("íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                        # íŠ¹ì§• ì¶”ì¶œ ì‹œê°í™” (ì‹œë®¬ë ˆì´ì…˜)
                        fig = self.transfer_helper.visualize_features(image, model_choice, layer_choice)
                        st.pyplot(fig)

            # íŠ¹ì§• ë§µ ë¶„ì„
            if st.checkbox("ìƒì„¸ ë¶„ì„ ë³´ê¸°", key="detailed_analysis"):
                st.subheader("ğŸ“Š íŠ¹ì§• ë§µ ìƒì„¸ ë¶„ì„")

                tabs = st.tabs(["íˆíŠ¸ë§µ", "3D ì‹œê°í™”", "í†µê³„"])

                with tabs[0]:
                    st.info("íŠ¹ì§• ë§µ íˆíŠ¸ë§µ ì‹œê°í™”")
                    # íˆíŠ¸ë§µ ì‹œê°í™” ì½”ë“œ

                with tabs[1]:
                    st.info("3D íŠ¹ì§• ê³µê°„ ì‹œê°í™”")
                    # 3D ì‹œê°í™” ì½”ë“œ

                with tabs[2]:
                    st.info("íŠ¹ì§• í†µê³„ ë¶„ì„")
                    # í†µê³„ ë¶„ì„ ì½”ë“œ

    def _render_integrated_analysis_tab(self):
        """í†µí•© ë¶„ì„ íƒ­"""
        st.header("ğŸ“Š Transfer Learning í†µí•© ë¶„ì„")

        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜• ì„ íƒ",
            ["ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", "í•™ìŠµ ê³¡ì„  ë¶„ì„", "í˜¼ë™ í–‰ë ¬", "íŠ¹ì§• ê³µê°„ ë¶„ì„"],
            key="integrated_analysis"
        )

        if analysis_type == "ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ":
            st.subheader("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

            # ëª¨ë¸ ì„ íƒ
            models = st.multiselect(
                "ë¹„êµí•  ëª¨ë¸",
                ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                default=["ResNet50", "EfficientNet"],
                key="model_comparison"
            )

            if len(models) >= 2:
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‘œì‹œ
                metrics_df = self.transfer_helper.get_model_metrics(models)
                st.dataframe(metrics_df, use_container_width=True)

                # ì°¨íŠ¸ ìƒì„±
                fig = self.transfer_helper.create_performance_chart(models)
                st.pyplot(fig)

        elif analysis_type == "í•™ìŠµ ê³¡ì„  ë¶„ì„":
            st.subheader("ğŸ“ˆ í•™ìŠµ ê³¡ì„  ë¶„ì„")

            # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
            fig = self.transfer_helper.plot_learning_curves()
            st.pyplot(fig)

            # ë¶„ì„ ì¸ì‚¬ì´íŠ¸
            st.info("""
            **í•™ìŠµ ê³¡ì„  í•´ì„**:
            - í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ì˜ ì°¨ì´ê°€ í¬ë©´ ê³¼ì í•©
            - ë‘ ê³¡ì„ ì´ ëª¨ë‘ ë†’ìœ¼ë©´ ê³¼ì†Œì í•©
            - ìµœì ì ì€ ê²€ì¦ ì†ì‹¤ì´ ìµœì†Œì¸ ì§€ì 
            """)

        elif analysis_type == "í˜¼ë™ í–‰ë ¬":
            st.subheader("ğŸ”¢ í˜¼ë™ í–‰ë ¬ ë¶„ì„")

            # í´ë˜ìŠ¤ ìˆ˜ ì„ íƒ
            num_classes = st.slider("í´ë˜ìŠ¤ ìˆ˜", min_value=2, max_value=10, value=5, key="confusion_classes")

            # í˜¼ë™ í–‰ë ¬ ìƒì„± ë° í‘œì‹œ
            fig = self.transfer_helper.create_confusion_matrix(num_classes)
            st.pyplot(fig)

        else:  # íŠ¹ì§• ê³µê°„ ë¶„ì„
            st.subheader("ğŸŒŒ íŠ¹ì§• ê³µê°„ ë¶„ì„")

            # t-SNE ì‹œê°í™”
            fig = self.transfer_helper.visualize_feature_space()
            st.pyplot(fig)

    def _render_project_tab(self):
        """ì‹¤ì „ í”„ë¡œì íŠ¸ íƒ­"""
        st.header("ğŸš€ ì‹¤ì „ Transfer Learning í”„ë¡œì íŠ¸")

        project_type = st.selectbox(
            "í”„ë¡œì íŠ¸ ì„ íƒ",
            ["ğŸ¥ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜", "ğŸ­ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬", "ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´", "ğŸ” ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ"],
            key="project_type"
        )

        if project_type == "ğŸ¥ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜":
            self._render_medical_project()
        elif project_type == "ğŸ­ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬":
            self._render_quality_control_project()
        elif project_type == "ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´":
            self._render_style_transfer_project()
        else:
            self._render_product_search_project()

    def _render_medical_project(self):
        """ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ¥ X-ray ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹œìŠ¤í…œ")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **í”„ë¡œì íŠ¸ ëª©í‘œ**:
            - í‰ë¶€ X-rayì—ì„œ íë ´ ê²€ì¶œ
            - Transfer Learningìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
            - ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
            """)

            uploaded_xray = st.file_uploader(
                "X-ray ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="xray_upload"
            )

        with col2:
            if uploaded_xray:
                st.image(uploaded_xray, caption="ì—…ë¡œë“œëœ X-ray")

                if st.button("ğŸ” ì§„ë‹¨ ì‹œì‘", key="diagnose"):
                    with st.spinner("AI ë¶„ì„ ì¤‘..."):
                        # ì§„ë‹¨ ì‹œë®¬ë ˆì´ì…˜
                        st.success("ë¶„ì„ ì™„ë£Œ!")
                        st.metric("ì •ìƒ í™•ë¥ ", "15%")
                        st.metric("íë ´ í™•ë¥ ", "85%", delta="ì£¼ì˜ í•„ìš”")

    def _render_quality_control_project(self):
        """ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ­ ì œí’ˆ ë¶ˆëŸ‰ ê²€ì¶œ ì‹œìŠ¤í…œ")

        st.markdown("""
        **ì‹œìŠ¤í…œ íŠ¹ì§•**:
        - ì‹¤ì‹œê°„ ë¶ˆëŸ‰í’ˆ ê²€ì¶œ
        - ë‹¤ì–‘í•œ ë¶ˆëŸ‰ ìœ í˜• ë¶„ë¥˜
        - Transfer Learningìœ¼ë¡œ ë¹ ë¥¸ ë°°í¬
        """)

        # ë¶ˆëŸ‰ ìœ í˜• ì„¤ì •
        defect_types = st.multiselect(
            "ê²€ì¶œí•  ë¶ˆëŸ‰ ìœ í˜•",
            ["ìŠ¤í¬ë˜ì¹˜", "ì°Œê·¸ëŸ¬ì§", "ë³€ìƒ‰", "í¬ë™", "ì´ë¬¼ì§ˆ"],
            default=["ìŠ¤í¬ë˜ì¹˜", "í¬ë™"],
            key="defect_types"
        )

        if st.button("ì‹œìŠ¤í…œ ì‹œì‘", key="start_qc"):
            st.info("í’ˆì§ˆ ê²€ì‚¬ ì‹œìŠ¤í…œì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤...")

    def _render_style_transfer_project(self):
        """ìŠ¤íƒ€ì¼ ì „ì´ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ¨ Neural Style Transfer")

        col1, col2 = st.columns(2)

        with col1:
            content_image = st.file_uploader(
                "ì½˜í…ì¸  ì´ë¯¸ì§€",
                type=['png', 'jpg', 'jpeg'],
                key="content_img"
            )
            if content_image:
                st.image(content_image, caption="ì½˜í…ì¸ ")

        with col2:
            style_image = st.file_uploader(
                "ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€",
                type=['png', 'jpg', 'jpeg'],
                key="style_img"
            )
            if style_image:
                st.image(style_image, caption="ìŠ¤íƒ€ì¼")

        if content_image and style_image:
            style_weight = st.slider("ìŠ¤íƒ€ì¼ ê°•ë„", 0.0, 1.0, 0.5, key="style_weight")

            if st.button("ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´ ì‹œì‘", key="transfer_style"):
                with st.spinner("ìŠ¤íƒ€ì¼ì„ ì „ì´í•˜ëŠ” ì¤‘..."):
                    st.info("Neural Style Transfer ì²˜ë¦¬ ì¤‘...")
                    st.success("ìŠ¤íƒ€ì¼ ì „ì´ ì™„ë£Œ!")

    def _render_product_search_project(self):
        """ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ” ì‹œê°ì  ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ")

        search_method = st.radio(
            "ê²€ìƒ‰ ë°©ë²•",
            ["í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰", "ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"],
            key="search_method"
        )

        if search_method == "í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰":
            query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥", placeholder="ë¹¨ê°„ ìš´ë™í™”", key="text_query")
        elif search_method == "ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰":
            query_img = st.file_uploader("ì°¸ì¡° ì´ë¯¸ì§€", type=['png', 'jpg', 'jpeg'], key="img_query")
        else:
            col1, col2 = st.columns(2)
            with col1:
                text_q = st.text_input("í…ìŠ¤íŠ¸", placeholder="í¸ì•ˆí•œ", key="hybrid_text")
            with col2:
                img_q = st.file_uploader("ì´ë¯¸ì§€", type=['png', 'jpg', 'jpeg'], key="hybrid_img")

        if st.button("ğŸ” ê²€ìƒ‰", key="search_products"):
            st.success("ìœ ì‚¬í•œ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ

    def _show_model_info(self, model_name):
        """ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
        model_info = {
            "ResNet50": {
                "parameters": "25.6M",
                "layers": "50",
                "year": "2015",
                "accuracy": "92.1%"
            },
            "VGG16": {
                "parameters": "138M",
                "layers": "16",
                "year": "2014",
                "accuracy": "90.1%"
            },
            "EfficientNet": {
                "parameters": "5.3M",
                "layers": "Variable",
                "year": "2019",
                "accuracy": "91.7%"
            },
            "MobileNet": {
                "parameters": "4.2M",
                "layers": "28",
                "year": "2017",
                "accuracy": "89.5%"
            },
            "DenseNet": {
                "parameters": "25.6M",
                "layers": "121",
                "year": "2016",
                "accuracy": "91.8%"
            }
        }

        if model_name in model_info:
            info = model_info[model_name]
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Parameters", info["parameters"])
            col2.metric("Layers", info["layers"])
            col3.metric("Year", info["year"])
            col4.metric("ImageNet Top-5", info["accuracy"])