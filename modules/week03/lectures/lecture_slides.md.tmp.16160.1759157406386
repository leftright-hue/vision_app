# Week 3: ì „ì´í•™ìŠµ + ë©€í‹°ëª¨ë‹¬ API í™œìš©

## ê°•ì˜ ìŠ¬ë¼ì´ë“œ

---

# ğŸ“š 3ì£¼ì°¨ í•™ìŠµ ëª©í‘œ

## ì˜¤ëŠ˜ ë°°ìš¸ ë‚´ìš©

1. **Transfer Learningì˜ ì›ë¦¬ì™€ ì‹¤ì „**
2. **Vision-Language Models ì´í•´**
3. **ë©€í‹°ëª¨ë‹¬ API í™œìš©ë²•**
4. **ìì—°ì–´ ê¸°ë°˜ ì‚¬ì§„ì²© ê²€ìƒ‰ ì•± êµ¬ì¶•**

---

# Part 1: Transfer Learning

## ğŸ¯ ì „ì´í•™ìŠµì´ë€?

### ì •ì˜
> "ì´ë¯¸ í•™ìŠµëœ ì§€ì‹ì„ ìƒˆë¡œìš´ ë¬¸ì œì— ì ìš©í•˜ëŠ” ê¸°ë²•"

### ì™œ í•„ìš”í•œê°€?
- **ë°ì´í„° ë¶€ì¡±**: ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥
- **ì‹œê°„ ì ˆì•½**: ì²˜ìŒë¶€í„° í•™ìŠµ ë¶ˆí•„ìš”
- **ë¹„ìš© ì ˆê°**: GPU ì‚¬ìš© ìµœì†Œí™”
- **ì„±ëŠ¥ í–¥ìƒ**: ë” ë‚˜ì€ ì´ˆê¸° ê°€ì¤‘ì¹˜

---

## ğŸ”„ Transfer Learning ê³¼ì •

```
[ImageNet 1000 Classes]
         â†“
    [Pretrained Model]
         â†“
    [Remove Last Layer]
         â†“
    [Add Custom Layers]
         â†“
    [Your Task: 10 Classes]
```

### í•µì‹¬ ì•„ì´ë””ì–´
- í•˜ìœ„ ë ˆì´ì–´: ì¼ë°˜ì  íŠ¹ì§• (edges, textures)
- ìƒìœ„ ë ˆì´ì–´: íƒœìŠ¤í¬ íŠ¹í™” íŠ¹ì§•

---

## ğŸ“Š Feature Extraction vs Fine-tuning

### Feature Extraction
```python
# ëª¨ë“  ë ˆì´ì–´ ë™ê²°
for param in model.parameters():
    param.requires_grad = False

# ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ êµì²´
model.fc = nn.Linear(features, num_classes)
```

**ì¥ì **: ë¹ ë¦„, ì ì€ ë°ì´í„°
**ë‹¨ì **: ì„±ëŠ¥ í•œê³„

### Fine-tuning
```python
# ì¼ë¶€/ì „ì²´ ë ˆì´ì–´ í•™ìŠµ
for param in model.parameters():
    param.requires_grad = True
```

**ì¥ì **: ìµœê³  ì„±ëŠ¥
**ë‹¨ì **: ì˜¤ë²„í”¼íŒ… ìœ„í—˜

---

## ğŸ’¡ ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí• ê¹Œ?

### Decision Tree

```
ë°ì´í„° ì–‘?
â”œâ”€ ì ìŒ (<1000)
â”‚  â””â”€ Feature Extraction
â””â”€ ë§ìŒ (>10000)
   â””â”€ ìœ ì‚¬ë„?
      â”œâ”€ ë†’ìŒ
      â”‚  â””â”€ Feature Extraction
      â””â”€ ë‚®ìŒ
         â””â”€ Fine-tuning
```

---

## ğŸš€ ì‹¤ì „ ì½”ë“œ: Transfer Learning

```python
import torch
import torchvision.models as models

# 1. ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ë¡œë“œ
model = models.resnet50(pretrained=True)

# 2. Feature Extraction ì„¤ì •
for param in model.parameters():
    param.requires_grad = False

# 3. ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ì¶”ê°€
num_features = model.fc.in_features
model.fc = nn.Sequential(
    nn.Linear(num_features, 256),
    nn.ReLU(),
    nn.Dropout(0.5),
    nn.Linear(256, num_classes)
)

# 4. í•™ìŠµ
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
```

---

# Part 2: Vision-Language Models

## ğŸŒŸ CLIPì˜ í˜ì‹ 

### Contrastive Language-Image Pre-training

```
   Text Encoder          Image Encoder
        â†“                      â†“
   Text Embedding       Image Embedding
        â†“                      â†“
        â””â”€â”€â”€â”€ Similarity â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ íŠ¹ì§•
- **4ì–µ ê°œ** ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ í•™ìŠµ
- **Zero-shot** ì´ë¯¸ì§€ ë¶„ë¥˜ ê°€ëŠ¥
- **ìì—°ì–´**ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰

---

## ğŸ” CLIP ì‘ë™ ì›ë¦¬

### Contrastive Learning

```python
# ìœ ì‚¬ë„ í–‰ë ¬
similarities = image_embeddings @ text_embeddings.T

# ëŒ€ê°ì„ : ë§¤ì¹­ ìŒ (positive)
# ë‚˜ë¨¸ì§€: ë¹„ë§¤ì¹­ ìŒ (negative)

loss = cross_entropy(similarities, labels)
```

### ëª©í‘œ
- Positive pairs: ìœ ì‚¬ë„ â†‘
- Negative pairs: ìœ ì‚¬ë„ â†“

---

## ğŸ“ ì„ë² ë”© ê³µê°„

### Shared Embedding Space

```
    "a photo of a cat"  â†’  [0.2, 0.8, ...]
              â†“
         Similarity
              â†‘
    [Cat Image] â†’  [0.21, 0.79, ...]
```

### ì‘ìš©
- ì´ë¯¸ì§€ ê²€ìƒ‰
- Zero-shot ë¶„ë¥˜
- ì´ë¯¸ì§€ ìƒì„± ê°€ì´ë“œ

---

## ğŸ¨ CLIP í™œìš© ì˜ˆì œ

```python
from transformers import CLIPProcessor, CLIPModel

# ëª¨ë¸ ë¡œë“œ
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­
image = Image.open("photo.jpg")
texts = ["a cat", "a dog", "a bird"]

inputs = processor(text=texts, images=image, 
                  return_tensors="pt", padding=True)
outputs = model(**inputs)

# ìœ ì‚¬ë„ ê³„ì‚°
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)
```

---

# Part 3: ë©€í‹°ëª¨ë‹¬ API í™œìš©

## ğŸ“… 2025ë…„ 9ì›” ê¸°ì¤€ API ì ‘ê·¼ ë°©ë²•

### 1. ğŸ”— OpenAI CLIP
- **ì ‘ê·¼ ë°©ì‹**: ì˜¤í”ˆì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (API ì„œë¹„ìŠ¤ ì•„ë‹˜)
- **ì„¤ì¹˜ ë°©ë²•**:
  ```bash
  # GitHub ì§ì ‘ ì„¤ì¹˜
  pip install git+https://github.com/openai/CLIP.git
  # ë˜ëŠ” Hugging Face ë²„ì „
  pip install transformers
  ```
- **íŠ¹ì§•**: API í‚¤ ë¶ˆí•„ìš”, ë¡œì»¬ ì‹¤í–‰, ì™„ì „ ë¬´ë£Œ
- **ì‘ë‹µ ì†ë„**: <100ms (GPU ì‚¬ìš© ì‹œ)

### 2. ğŸ¤– Google Gemini API (ê¶Œì¥)
- **2025ë…„ í˜„ì¬**: Vision APIë¥¼ ëŒ€ì²´í•˜ëŠ” ì¶”ì„¸
- **ê°•ì **:
  - ë³µì¡í•œ ì´ë¯¸ì§€ ì´í•´ ë° ì¶”ë¡ 
  - ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤)
  - PDF ì§ì ‘ ì²˜ë¦¬ (OCR ë¶ˆí•„ìš”)
  - 90ë¶„ê¹Œì§€ ë¹„ë””ì˜¤ ì§€ì›
- **ì ‘ê·¼ ë°©ë²•**:
  1. Google Cloud Console ì ‘ì†
  2. í”„ë¡œì íŠ¸ ìƒì„± â†’ Gemini API í™œì„±í™”
  3. API í‚¤ ìƒì„±
- **ë¬´ë£Œ í• ë‹¹ëŸ‰**: ì¶©ë¶„í•œ ë¬´ë£Œ í‹°ì–´ ì œê³µ

### 3. ğŸ¤— Hugging Face API
- **í† í° ìƒì„± ë°©ë²•**:
  1. HuggingFace.co ê³„ì • ìƒì„±
  2. Settings â†’ Access Tokens
  3. "New Token" í´ë¦­
  4. ê¶Œí•œ ì„¤ì • (read/write)
  5. í† í° ìƒì„± (í˜•ì‹: `hf_xxxxx`)
- **2025ë…„ ê¶Œì¥ì‚¬í•­**:
  - í”„ë¡œë•ì…˜ì—ëŠ” fine-grained í† í° ì‚¬ìš©
  - ì•±ë³„ë¡œ ë³„ë„ í† í° ìƒì„±

---

## ğŸ”§ Gemini API ì‹¤ìŠµ

```python
import google.generativeai as genai

# API ì„¤ì •
genai.configure(api_key="YOUR_API_KEY")
model = genai.GenerativeModel('gemini-1.5-flash')

# ì´ë¯¸ì§€ ë¶„ì„
image = Image.open("photo.jpg")
response = model.generate_content([
    "Describe this image in detail",
    image
])

print(response.text)
```

### í™œìš© ì˜ˆì‹œ
- ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±
- Visual Q&A
- OCR ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ

---

## ğŸ¤– Together AI í™œìš©

```python
import together
import base64

# API ì„¤ì •
together.api_key = "YOUR_API_KEY"

# ì´ë¯¸ì§€ ì¸ì½”ë”©
with open("photo.jpg", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode()

# ë¶„ì„ ìš”ì²­
response = together.Complete.create(
    model="meta-llama/Llama-3.2-11B-Vision",
    prompt=f"<image>{image_base64}</image>\nDescribe this image",
    max_tokens=256
)

print(response['output']['choices'][0]['text'])
```

---

## ğŸ“Š API ì„±ëŠ¥ ë¹„êµ

### ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

| API | ì‘ë‹µ ì‹œê°„ | ì •í™•ë„ | ë¹„ìš© |
|-----|---------|-------|------|
| Gemini | 1.2s | 95% | Free tier |
| Llama Vision | 2.1s | 92% | Free credits |
| CLIP | 0.1s | 88% | 100% Free |

### ì„ íƒ ê°€ì´ë“œ
- **ì†ë„ ì¤‘ìš”**: CLIP
- **ì •í™•ë„ ì¤‘ìš”**: Gemini
- **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: Llama Vision

---

# Part 4: í†µí•© í”„ë¡œì íŠ¸

## ğŸ¯ ìì—°ì–´ ê¸°ë°˜ ì‚¬ì§„ì²© ê²€ìƒ‰ ì•±

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
User Query â†’ CLIP Encoder â†’ Similarity Search
                â†“
           Image Results
                â†“
          Gemini API â†’ Captions
                â†“
           Final Display
```

### í•µì‹¬ ê¸°ëŠ¥
1. í…ìŠ¤íŠ¸ë¡œ ì‚¬ì§„ ê²€ìƒ‰
2. ìœ ì‚¬ ì´ë¯¸ì§€ ì°¾ê¸°
3. ìë™ ìº¡ì…˜ ìƒì„±
4. ê³ ê¸‰ í•„í„°ë§

---

## ğŸ’» í†µí•© êµ¬í˜„

```python
class SmartPhotoAlbum:
    def __init__(self):
        # Transfer Learning ëª¨ë¸
        self.classifier = load_transfer_model()
        
        # CLIP ëª¨ë¸
        self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Gemini API
        self.gemini = genai.GenerativeModel('gemini-1.5-flash')
    
    def search(self, query):
        # CLIPìœ¼ë¡œ ê²€ìƒ‰
        results = self.clip_search(query)
        
        # Geminië¡œ ìº¡ì…˜ ì¶”ê°€
        for result in results:
            result['caption'] = self.generate_caption(result['image'])
        
        return results
```

---

## ğŸš€ Gradio ì¸í„°í˜ì´ìŠ¤

```python
import gradio as gr

def create_app():
    with gr.Blocks() as app:
        gr.Markdown("# ğŸ–¼ï¸ Smart Photo Album")
        
        with gr.Tab("Search"):
            query = gr.Textbox(label="Search photos")
            results = gr.Gallery(label="Results")
            
            query.submit(search_photos, inputs=[query], 
                        outputs=[results])
        
        with gr.Tab("Upload"):
            upload = gr.File(label="Upload photos")
            status = gr.Textbox(label="Status")
            
            upload.change(process_upload, inputs=[upload], 
                         outputs=[status])
    
    return app

app = create_app()
app.launch()
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. ì„ë² ë”© ìºì‹±
```python
# ì‚¬ì „ ê³„ì‚° ë° ì €ì¥
embeddings = compute_embeddings(images)
np.save('embeddings.npy', embeddings)

# ë¹ ë¥¸ ë¡œë“œ
embeddings = np.load('embeddings.npy')
```

### 2. ë°°ì¹˜ ì²˜ë¦¬
```python
# í•œ ë²ˆì— ì—¬ëŸ¬ ì´ë¯¸ì§€ ì²˜ë¦¬
batch_size = 32
for i in range(0, len(images), batch_size):
    batch = images[i:i+batch_size]
    process_batch(batch)
```

### 3. ë¹„ë™ê¸° ì²˜ë¦¬
```python
import asyncio

async def process_async(images):
    tasks = [process_image(img) for img in images]
    results = await asyncio.gather(*tasks)
    return results
```

---

# ì‹¤ìŠµ ì‹œê°„

## ğŸ§ª Lab 3: í†µí•© ì‹¤ìŠµ

### Step 1: Transfer Learning
1. ResNet50ìœ¼ë¡œ ë¶„ë¥˜ê¸° ë§Œë“¤ê¸°
2. Feature Extraction vs Fine-tuning ë¹„êµ

### Step 2: CLIP ê²€ìƒ‰
1. ì´ë¯¸ì§€ ì¸ë±ì‹±
2. í…ìŠ¤íŠ¸ ê²€ìƒ‰ êµ¬í˜„

### Step 3: API í†µí•©
1. Geminië¡œ ìº¡ì…˜ ìƒì„±
2. ì„±ëŠ¥ ë¹„êµ

### Step 4: ì•± ë°°í¬
1. Gradio ì¸í„°í˜ì´ìŠ¤
2. Hugging Face Space ë°°í¬

---

## ğŸ¯ í•µì‹¬ ì •ë¦¬

### ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©
âœ… Transfer Learningìœ¼ë¡œ íš¨ìœ¨ì ì¸ ëª¨ë¸ êµ¬ì¶•
âœ… CLIPìœ¼ë¡œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê²€ìƒ‰
âœ… ë©€í‹°ëª¨ë‹¬ API í™œìš©ë²•
âœ… í†µí•© ì‹œìŠ¤í…œ êµ¬í˜„

### ë‹¤ìŒ ì£¼ ì˜ˆê³ 
- Vision Transformer (ViT)
- Self-Attention ë©”ì»¤ë‹ˆì¦˜
- DINOì™€ ìê¸°ì§€ë„í•™ìŠµ

---

## ğŸ“ ê³¼ì œ ì•ˆë‚´

### Assignment 3: ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì‹œìŠ¤í…œ

**ìš”êµ¬ì‚¬í•­**:
1. ìµœì†Œ 3ê°€ì§€ ê²€ìƒ‰ ëª¨ë“œ
2. 2ê°œ ì´ìƒ API í†µí•©
3. ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸
4. HF Space ë°°í¬

**í‰ê°€ ê¸°ì¤€**:
- ê¸°ëŠ¥ì„± 40%
- ì„±ëŠ¥ 20%
- UI/UX 20%
- ì½”ë“œ í’ˆì§ˆ 10%
- ì°½ì˜ì„± 10%

**ì œì¶œ ê¸°í•œ**: ë‹¤ìŒ ì£¼ ìˆ˜ì—… ì „

---

## ğŸ’¬ Q&A

### ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

**Q1: Transfer Learning vs Scratch?**
- ê±°ì˜ í•­ìƒ Transfer Learningì´ ìœ ë¦¬
- íŠ¹ìˆ˜í•œ ë„ë©”ì¸ë§Œ ì˜ˆì™¸

**Q2: CLIP vs ì¼ë°˜ ë¶„ë¥˜ê¸°?**
- CLIP: ìœ ì—°ì„±, Zero-shot
- ë¶„ë¥˜ê¸°: íŠ¹ì • íƒœìŠ¤í¬ ìµœì í™”

**Q3: API ë¹„ìš©ì€?**
- ìˆ˜ì—…ìš©ìœ¼ë¡œëŠ” ëª¨ë‘ ë¬´ë£Œ
- ìƒì—…ìš© ì‚¬ìš© ì‹œ ê³¼ê¸ˆ

---

## ğŸ”— ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- [CLIP Paper](https://arxiv.org/abs/2103.00020)
- [Vision Transformers](https://arxiv.org/abs/2010.11929)

### íŠœí† ë¦¬ì–¼
- [PyTorch Transfer Learning](https://pytorch.org/tutorials/)
- [Hugging Face CLIP Guide](https://huggingface.co/docs/transformers/model_doc/clip)

### ì½”ë“œ
- [Week 3 GitHub](https://github.com/course/week03)
- [ì˜ˆì œ ë…¸íŠ¸ë¶](https://colab.research.google.com/)

---

# Thank You! ğŸ™

## ë‹¤ìŒ ì£¼ì— ë§Œë‚˜ìš”!

### ì—°ë½ì²˜
- ì´ë©”ì¼: newmind68@hs.ac.kr
- ì˜¤í”¼ìŠ¤ ì•„ì›Œ: ìˆ˜ìš”ì¼ 14:00-16:00

### ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤
- ê°•ì˜ ìë£Œ: [Course Website]
- ì§ˆë¬¸: [Course Forum]

---