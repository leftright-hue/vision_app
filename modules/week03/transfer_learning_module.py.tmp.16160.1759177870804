"""
Week 3: Transfer Learning & Multi-modal API ëª¨ë“ˆ
Transfer Learningê³¼ Multi-modal API ê´€ë ¨ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import streamlit as st
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from typing import Dict, List, Tuple, Optional
import cv2
import matplotlib.pyplot as plt
import os
import sys

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from core.base_processor import BaseImageProcessor
from core.ai_models import AIModelManager
from .transfer_helpers import TransferLearningHelper
from .multimodal_helpers import MultiModalHelper


class TransferLearningModule(BaseImageProcessor):
    """Transfer Learning ë° Multi-modal API í•™ìŠµ ëª¨ë“ˆ"""

    def __init__(self):
        super().__init__()
        self.ai_manager = AIModelManager()
        self.transfer_helper = TransferLearningHelper()
        self.multimodal_helper = MultiModalHelper()

    def render(self):
        """Week 3 ëª¨ë“ˆ UI ë Œë”ë§ - Week 2ì™€ ë™ì¼í•œ ë©”ì„œë“œëª…"""
        self.render_ui()

    def render_ui(self):
        """Week 3 ëª¨ë“ˆ UI ë Œë”ë§"""
        st.title("ğŸ”„ Week 3: Transfer Learning & Multi-modal API")
        st.markdown("---")

        # íƒ­ ìƒì„±
        tabs = st.tabs([
            "ğŸ“š ì´ë¡ ",
            "ğŸ”„ Transfer Learning",
            "ğŸ–¼ï¸ CLIP ê²€ìƒ‰",
            "ğŸ¨ íŠ¹ì§• ì¶”ì¶œ",
            "ğŸ“Š í†µí•© ë¶„ì„",
            "ğŸš€ ì‹¤ì „ í”„ë¡œì íŠ¸",
            "ğŸ” API ë¹„êµ"
        ])

        with tabs[0]:
            self._render_theory_tab()

        with tabs[1]:
            self._render_transfer_learning_tab()

        with tabs[2]:
            self._render_clip_search_tab()

        with tabs[3]:
            self._render_feature_extraction_tab()

        with tabs[4]:
            self._render_integrated_analysis_tab()

        with tabs[5]:
            self._render_project_tab()

        with tabs[6]:
            self._render_api_comparison_tab()

    def _render_theory_tab(self):
        """ì´ë¡  íƒ­"""
        st.header("ğŸ“– Transfer Learning & Multi-modal ì´ë¡ ")

        # ì„œë¸Œ íƒ­ ìƒì„±
        theory_tabs = st.tabs([
            "ğŸ“š Transfer Learning ì´ë¡ ",
            "ğŸ¤– CLIP ì´ë¡ ",
            "ğŸ”¬ ìˆ˜í•™ì  ê¸°ì´ˆ",
            "ğŸ’¡ ì‹¤ì „ ê°€ì´ë“œ"
        ])

        with theory_tabs[0]:
            self._render_transfer_learning_theory()

        with theory_tabs[1]:
            self._render_clip_theory()

        with theory_tabs[2]:
            self._render_mathematical_foundation()

        with theory_tabs[3]:
            self._render_practical_guide()

    def _render_transfer_learning_theory(self):
        """Transfer Learning ìƒì„¸ ì´ë¡ """
        st.markdown("## ğŸ“š Transfer Learning ì´ë¡ ê³¼ ì‹¤ìŠµ")

        # ê°œë…ê³¼ ë°°ê²½
        with st.expander("### 1. Transfer Learningì˜ íƒ„ìƒ ë°°ê²½ê³¼ í•µì‹¬ ê°œë…", expanded=True):
            st.markdown("""
            #### ğŸŒ± íƒ„ìƒ ë°°ê²½
            - **ë¬¸ì œ**: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì—ëŠ” ë§‰ëŒ€í•œ ë°ì´í„°ì™€ ì»´í“¨íŒ… ìì› í•„ìš”
            - **í•´ê²°ì±…**: ì´ë¯¸ í•™ìŠµëœ ì§€ì‹ì„ ì¬í™œìš©í•˜ì!
            - **ì˜ê°**: ì¸ê°„ì˜ í•™ìŠµ ë°©ì‹ (ìì „ê±° â†’ ì˜¤í† ë°”ì´ ìš´ì „)

            #### ğŸ¯ í•µì‹¬ ê°œë…
            **Transfer Learning = ì§€ì‹ ì „ì´ í•™ìŠµ**
            ```
            Source Domain (ì›ì²œ ë„ë©”ì¸) â†’ Target Domain (ëª©í‘œ ë„ë©”ì¸)
            ImageNet 1000ê°œ í´ë˜ìŠ¤    â†’    ê°œ/ê³ ì–‘ì´ 2ê°œ í´ë˜ìŠ¤
            ```

            #### ğŸ“Š ì‘ë™ ì›ë¦¬
            1. **Low-level Features** (í•˜ìœ„ ë ˆì´ì–´)
               - Edge, Corner, Texture ë“± ì¼ë°˜ì  íŠ¹ì§•
               - ëŒ€ë¶€ë¶„ì˜ ì´ë¯¸ì§€ ì‘ì—…ì— ê³µí†µì ìœ¼ë¡œ ìœ ìš©
               - ë³´í†µ ë™ê²°(Freeze)í•˜ì—¬ ì¬ì‚¬ìš©

            2. **High-level Features** (ìƒìœ„ ë ˆì´ì–´)
               - í´ë˜ìŠ¤ë³„ íŠ¹í™”ëœ íŠ¹ì§•
               - Task-specificí•˜ë¯€ë¡œ ì¬í•™ìŠµ í•„ìš”
               - Fine-tuningì˜ ì£¼ìš” ëŒ€ìƒ

            #### ğŸš€ ì™œ íš¨ê³¼ì ì¸ê°€?
            - **Feature Hierarchy**: CNNì€ ê³„ì¸µì  íŠ¹ì§• í•™ìŠµ
            - **Universal Features**: í•˜ìœ„ ë ˆì´ì–´ëŠ” ë²”ìš©ì 
            - **Data Efficiency**: ì ì€ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥
            - **Convergence Speed**: ë¹ ë¥¸ ìˆ˜ë ´
            """)

        # Transfer Learning ë°©ë²•ë¡ 
        with st.expander("### 2. Transfer Learning ë°©ë²•ë¡  ìƒì„¸"):
            st.markdown("""
            #### ğŸ”§ ë°©ë²• 1: Feature Extraction (íŠ¹ì§• ì¶”ì¶œ)
            ```python
            # ëª¨ë“  ë ˆì´ì–´ ë™ê²°
            for param in model.parameters():
                param.requires_grad = False

            # ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ êµì²´
            model.fc = nn.Linear(2048, num_classes)
            ```
            - **ì¥ì **: ë¹ ë¦„, ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ
            - **ë‹¨ì **: ì„±ëŠ¥ í–¥ìƒ ì œí•œì 
            - **ì ìš©**: ë°ì´í„° ë§¤ìš° ì ì„ ë•Œ (< 1000ê°œ)

            #### ğŸ¨ ë°©ë²• 2: Fine-tuning (ë¯¸ì„¸ ì¡°ì •)
            ```python
            # ì´ˆê¸° ë ˆì´ì–´ë§Œ ë™ê²°
            for layer in model.layers[:-5]:
                layer.requires_grad = False

            # ìƒìœ„ ë ˆì´ì–´ëŠ” í•™ìŠµ ê°€ëŠ¥
            for layer in model.layers[-5:]:
                layer.requires_grad = True
            ```
            - **ì¥ì **: ë†’ì€ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥
            - **ë‹¨ì **: ê³¼ì í•© ìœ„í—˜, í•™ìŠµ ì‹œê°„ ì¦ê°€
            - **ì ìš©**: ì¶©ë¶„í•œ ë°ì´í„° (> 5000ê°œ)

            #### ğŸ”„ ë°©ë²• 3: Progressive Fine-tuning
            ```python
            # Step 1: ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ
            train_last_layer(epochs=10)

            # Step 2: ì ì§„ì ìœ¼ë¡œ ë” ë§ì€ ë ˆì´ì–´
            unfreeze_layers(n=2)
            train_model(epochs=5, lr=lr/10)

            # Step 3: ì „ì²´ ë¯¸ì„¸ì¡°ì •
            unfreeze_all()
            train_model(epochs=3, lr=lr/100)
            ```
            - **ì¥ì **: ì•ˆì •ì  í•™ìŠµ, ìµœê³  ì„±ëŠ¥
            - **ë‹¨ì **: ë³µì¡í•œ êµ¬í˜„, ì‹œê°„ ì†Œìš”
            - **ì ìš©**: ì¤‘ìš”í•œ í”„ë¡œì íŠ¸

            #### ğŸ“Š ë°©ë²• ì„ íƒ ê°€ì´ë“œ
            | ë°ì´í„° ì–‘ | ìœ ì‚¬ë„ | ì¶”ì²œ ë°©ë²• |
            |----------|--------|----------|
            | ì ìŒ + ë†’ìŒ | Feature Extraction |
            | ì ìŒ + ë‚®ìŒ | Fine-tuning (ìƒìœ„ ë ˆì´ì–´) |
            | ë§ìŒ + ë†’ìŒ | Fine-tuning (ì „ì²´) |
            | ë§ìŒ + ë‚®ìŒ | ì²˜ìŒë¶€í„° í•™ìŠµ or Progressive |
            """)

        # ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ
        with st.expander("### 3. ì‹¤ì œ êµ¬í˜„ ì½”ë“œ ì˜ˆì œ"):
            st.markdown("""
            #### ğŸ• ì˜ˆì œ: ê°œ í’ˆì¢… ë¶„ë¥˜ê¸° ë§Œë“¤ê¸°

            ```python
            import torch
            import torch.nn as nn
            import torchvision.models as models
            from torch.optim import Adam
            from torch.optim.lr_scheduler import StepLR

            class DogBreedClassifier:
                def __init__(self, num_breeds=120):
                    # 1. ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ
                    self.model = models.resnet50(pretrained=True)

                    # 2. Feature Extraction ì„¤ì •
                    for param in self.model.parameters():
                        param.requires_grad = False

                    # 3. ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ì¶”ê°€
                    num_features = self.model.fc.in_features
                    self.model.fc = nn.Sequential(
                        nn.Linear(num_features, 512),
                        nn.ReLU(),
                        nn.Dropout(0.3),
                        nn.Linear(512, num_breeds)
                    )

                def progressive_unfreeze(self, stage):
                    '''ì ì§„ì  ì–¸í”„ë¦¬ì§•'''
                    if stage == 1:  # ë§ˆì§€ë§‰ ë¸”ë¡ë§Œ
                        for param in self.model.layer4.parameters():
                            param.requires_grad = True
                    elif stage == 2:  # ë§ˆì§€ë§‰ 2ê°œ ë¸”ë¡
                        for param in self.model.layer3.parameters():
                            param.requires_grad = True
                    elif stage == 3:  # ì „ì²´
                        for param in self.model.parameters():
                            param.requires_grad = True

                def train_stage(self, dataloader, stage, epochs):
                    self.progressive_unfreeze(stage)

                    # í•™ìŠµë¥  ì¡°ì • (ì–¸í”„ë¦¬ì§•í• ìˆ˜ë¡ ë‚®ê²Œ)
                    lr = 1e-3 * (0.1 ** (stage - 1))
                    optimizer = Adam(
                        filter(lambda p: p.requires_grad,
                               self.model.parameters()),
                        lr=lr
                    )

                    for epoch in range(epochs):
                        # í•™ìŠµ ì½”ë“œ
                        pass

            # ì‚¬ìš© ì˜ˆì‹œ
            classifier = DogBreedClassifier(num_breeds=120)

            # Stage 1: Feature Extraction (10 epochs)
            classifier.train_stage(dataloader, stage=0, epochs=10)

            # Stage 2: Fine-tune ë§ˆì§€ë§‰ ë¸”ë¡ (5 epochs)
            classifier.train_stage(dataloader, stage=1, epochs=5)

            # Stage 3: Fine-tune ë” ë§ì€ ë ˆì´ì–´ (3 epochs)
            classifier.train_stage(dataloader, stage=2, epochs=3)
            ```
            """)

        # ê³ ê¸‰ ê¸°ë²•
        with st.expander("### 4. Transfer Learning ê³ ê¸‰ ê¸°ë²•"):
            st.markdown("""
            #### ğŸ­ Domain Adaptation (ë„ë©”ì¸ ì ì‘)
            - **ë¬¸ì œ**: Sourceì™€ Target ë„ë©”ì¸ì´ ë„ˆë¬´ ë‹¤ë¦„
            - **í•´ê²°**: Adversarial Training, MMD ë“± í™œìš©
            ```python
            # Domain Adversarial Neural Network (DANN)
            class DANN(nn.Module):
                def __init__(self):
                    self.feature_extractor = ResNet50()
                    self.label_classifier = LabelClassifier()
                    self.domain_classifier = DomainClassifier()
                    self.gradient_reversal = GradientReversal()
            ```

            #### ğŸ¯ Few-shot Learning (í“¨ìƒ· ëŸ¬ë‹)
            - **Prototypical Networks**: í´ë˜ìŠ¤ë³„ í”„ë¡œí† íƒ€ì… í•™ìŠµ
            - **Siamese Networks**: ìœ ì‚¬ë„ í•™ìŠµ
            - **MAML**: ë¹ ë¥¸ ì ì‘ì„ ìœ„í•œ ë©”íƒ€ í•™ìŠµ

            #### ğŸ”„ Knowledge Distillation (ì§€ì‹ ì¦ë¥˜)
            ```python
            # Teacher-Student ëª¨ë¸
            def distillation_loss(student_output, teacher_output,
                                 true_labels, T=3, alpha=0.7):
                # Soft targets from teacher
                soft_loss = KL_div(
                    F.log_softmax(student_output/T),
                    F.softmax(teacher_output/T)
                ) * T * T

                # Hard targets
                hard_loss = F.cross_entropy(student_output, true_labels)

                return alpha * soft_loss + (1-alpha) * hard_loss
            ```

            #### ğŸ“Š Multi-task Learning (ë©€í‹°íƒœìŠ¤í¬ ëŸ¬ë‹)
            - **Hard Parameter Sharing**: ë ˆì´ì–´ ê³µìœ 
            - **Soft Parameter Sharing**: ì •ê·œí™”ë¡œ ìœ ì‚¬ì„± ìœ ë„
            - **Cross-stitch Networks**: íƒœìŠ¤í¬ ê°„ ì •ë³´ êµí™˜
            """)

    def _render_clip_theory(self):
        """CLIP ìƒì„¸ ì´ë¡ """
        st.markdown("## ğŸ¤– CLIP (Contrastive Language-Image Pre-training) ì´ë¡ ê³¼ ì‘ìš©")

        with st.expander("### 1. CLIPì˜ ê°œë°œ ë°°ê²½ê³¼ í•µì‹¬ ì•„ì´ë””ì–´", expanded=True):
            st.markdown("""
            #### ğŸŒŸ CLIPì˜ íŠ¹ì§•
            - **2021ë…„ OpenAI ë°œí‘œ**: ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜
            - **í•µì‹¬**: 4ì–µ ê°œì˜ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒìœ¼ë¡œ í•™ìŠµ
            - **ê²°ê³¼**: Zero-shotìœ¼ë¡œ ImageNet ì •í™•ë„ 76.2% ë‹¬ì„±

            #### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´: Contrastive Learning
            ```
            ëª©í‘œ: ë§¤ì¹­ë˜ëŠ” (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒì€ ê°€ê¹ê²Œ
                 ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” ìŒì€ ë©€ê²Œ

            [ê³ ì–‘ì´ ì´ë¯¸ì§€] â†â†’ "ê·€ì—¬ìš´ ê³ ì–‘ì´" âœ… (ê°€ê¹ê²Œ)
            [ê³ ì–‘ì´ ì´ë¯¸ì§€] â†â†’ "ë¹¨ê°„ ìë™ì°¨" âŒ (ë©€ê²Œ)
            ```

            #### ğŸ—ï¸ CLIP ì•„í‚¤í…ì²˜
            ```
            ì´ë¯¸ì§€ â†’ Image Encoder â†’ ì´ë¯¸ì§€ ì„ë² ë”© (512ì°¨ì›)
                                          â†“
                                    ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                                          â†‘
            í…ìŠ¤íŠ¸ â†’ Text Encoder â†’ í…ìŠ¤íŠ¸ ì„ë² ë”© (512ì°¨ì›)
            ```

            #### ğŸ“Š í•™ìŠµ ê³¼ì •
            1. **ë°°ì¹˜ êµ¬ì„±**: Nê°œì˜ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ
            2. **ì¸ì½”ë”©**: ê°ê°ì„ 512ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
            3. **ìœ ì‚¬ë„ ê³„ì‚°**: NÃ—N ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤
            4. **ëŒ€ì¡° í•™ìŠµ**: ëŒ€ê°ì„ ì€ 1, ë‚˜ë¨¸ì§€ëŠ” 0ì´ ë˜ë„ë¡
            """)

        with st.expander("### 2. CLIP ì†ì‹¤ í•¨ìˆ˜ì™€ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜"):
            st.markdown("""
            #### ğŸ“ InfoNCE Loss (Contrastive Loss)
            ```python
            def clip_loss(image_embeddings, text_embeddings, temperature=0.07):
                # ì •ê·œí™”
                image_embeddings = F.normalize(image_embeddings)
                text_embeddings = F.normalize(text_embeddings)

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                logits = image_embeddings @ text_embeddings.T / temperature

                # ëŒ€ê°ì„ ì´ ì •ë‹µ (positive pairs)
                labels = torch.arange(len(logits))

                # ì–‘ë°©í–¥ ì†ì‹¤
                loss_i2t = F.cross_entropy(logits, labels)
                loss_t2i = F.cross_entropy(logits.T, labels)

                return (loss_i2t + loss_t2i) / 2
            ```

            #### ğŸŒ¡ï¸ Temperature Parameter
            - **ì—­í• **: ìœ ì‚¬ë„ ë¶„í¬ì˜ sharpness ì¡°ì ˆ
            - **ë‚®ì€ ì˜¨ë„ (0.01)**: ë” í™•ì‹¤í•œ êµ¬ë¶„
            - **ë†’ì€ ì˜¨ë„ (0.1)**: ë¶€ë“œëŸ¬ìš´ êµ¬ë¶„
            - **CLIP ê¸°ë³¸ê°’**: 0.07

            #### ğŸ“Š í•™ìŠµ ì „ëµ
            1. **Large Batch Size**: 32,768 (ë§¤ìš° í¼)
               - ë” ë§ì€ negative samples
               - ì•ˆì •ì ì¸ ëŒ€ì¡° í•™ìŠµ

            2. **Mixed Precision Training**
               ```python
               with torch.cuda.amp.autocast():
                   image_features = image_encoder(images)
                   text_features = text_encoder(texts)
                   loss = clip_loss(image_features, text_features)
               ```

            3. **Gradient Accumulation**
               - ë©”ëª¨ë¦¬ ì œì•½ ê·¹ë³µ
               - íš¨ê³¼ì ì¸ large batch ì‹œë®¬ë ˆì´ì…˜
            """)

        with st.expander("### 3. CLIPì˜ Zero-shot ëŠ¥ë ¥"):
            st.markdown("""
            #### ğŸ¯ Zero-shot Classification
            ```python
            def zero_shot_classifier(image, class_names, model):
                # 1. í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
                text_prompts = [f"a photo of a {name}" for name in class_names]

                # 2. ì¸ì½”ë”©
                image_features = model.encode_image(image)
                text_features = model.encode_text(text_prompts)

                # 3. ìœ ì‚¬ë„ ê³„ì‚°
                similarities = (image_features @ text_features.T)

                # 4. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
                probs = similarities.softmax(dim=-1)

                return class_names[probs.argmax()]
            ```

            #### ğŸ”„ Prompt Engineering for CLIP
            ```python
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            "a photo of a {class}"

            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë“¤
            templates = [
                "a photo of a {class}",
                "a bad photo of a {class}",
                "a origami {class}",
                "a photo of the large {class}",
                "a {class} in a video game",
                "art of a {class}",
                "a photo of the small {class}"
            ]

            # ì•™ìƒë¸”ë¡œ ì„±ëŠ¥ í–¥ìƒ
            def ensemble_classify(image, class_name, templates):
                scores = []
                for template in templates:
                    text = template.format(class=class_name)
                    score = compute_similarity(image, text)
                    scores.append(score)
                return np.mean(scores)
            ```

            #### ğŸ“Š Zero-shot vs Fine-tuned ì„±ëŠ¥
            | Dataset | Zero-shot CLIP | Fine-tuned ResNet50 |
            |---------|---------------|-------------------|
            | ImageNet | 76.2% | 76.3% |
            | CIFAR-100 | 65.1% | 71.5% |
            | Food101 | 88.9% | 72.3% |
            | Flowers102 | 68.7% | 91.3% |
            """)

        with st.expander("### 4. CLIP ì‘ìš©ê³¼ í™•ì¥"):
            st.markdown("""
            #### ğŸ¨ CLIP ê¸°ë°˜ ì‘ìš©
            1. **DALL-E 2**: CLIP ì„ë² ë”© â†’ ì´ë¯¸ì§€ ìƒì„±
            2. **CLIP-Seg**: ì´ë¯¸ì§€ ë¶„í• 
            3. **CLIP4Clip**: ë¹„ë””ì˜¤ ê²€ìƒ‰
            4. **AudioCLIP**: ì˜¤ë””ì˜¤-ë¹„ì „-ì–¸ì–´ í†µí•©

            #### ğŸ”§ CLIP Fine-tuning ì „ëµ
            ```python
            class CLIPFineTuner:
                def __init__(self, clip_model):
                    self.clip = clip_model
                    # LoRA: Low-Rank Adaptation
                    self.lora_image = LoRAAdapter(self.clip.visual)
                    self.lora_text = LoRAAdapter(self.clip.transformer)

                def forward(self, images, texts):
                    # ì›ë³¸ + LoRA ì–´ëŒ‘í„°
                    image_features = self.clip.visual(images)
                    image_features += self.lora_image(images)

                    text_features = self.clip.transformer(texts)
                    text_features += self.lora_text(texts)

                    return image_features, text_features
            ```

            #### ğŸŒ ë‹¤êµ­ì–´ CLIP
            - **mCLIP**: ë‹¤êµ­ì–´ ì§€ì›
            - **XLM-R**: 100ê°œ ì–¸ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë”
            - **Korean CLIP**: í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
            """)

    def _render_mathematical_foundation(self):
        """ìˆ˜í•™ì  ê¸°ì´ˆ"""
        st.markdown("## ğŸ”¬ ìˆ˜í•™ì  ê¸°ì´ˆì™€ ì´ë¡ ")

        with st.expander("### 1. Gradient ê¸°ë°˜ ìµœì í™”"):
            st.markdown("""
            #### ğŸ“ Transfer Learningì˜ ìˆ˜í•™

            **ëª©ì  í•¨ìˆ˜**:
            $$L_{total} = L_{task} + \\lambda L_{regularization}$$

            **Fine-tuning ê·¸ë˜ë””ì–¸íŠ¸**:
            $$\\theta_{new} = \\theta_{pretrained} - \\eta \\nabla L_{task}$$

            ì—¬ê¸°ì„œ:
            - $\\theta_{pretrained}$: ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜
            - $\\eta$: í•™ìŠµë¥  (ë³´í†µ ë§¤ìš° ì‘ì€ ê°’)
            - $L_{task}$: ìƒˆë¡œìš´ íƒœìŠ¤í¬ ì†ì‹¤
            """)

        with st.expander("### 2. Contrastive Learning ìˆ˜í•™"):
            st.markdown("""
            #### ğŸ“ CLIPì˜ InfoNCE Loss

            $$L = -\\log\\frac{\\exp(sim(x_i, y_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(sim(x_i, y_j)/\\tau)}$$

            ì—¬ê¸°ì„œ:
            - $sim(x, y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}$ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            - $\\tau$: temperature parameter
            - $N$: ë°°ì¹˜ í¬ê¸°

            #### ğŸ¯ ìµœì í™” ëª©í‘œ
            - Positive pairs: $sim(x_i, y_i) \\rightarrow 1$
            - Negative pairs: $sim(x_i, y_j) \\rightarrow 0$ (i â‰  j)
            """)

    def _render_practical_guide(self):
        """ì‹¤ì „ ê°€ì´ë“œ"""
        st.markdown("## ğŸ’¡ ì‹¤ì „ ê°€ì´ë“œì™€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤")

        with st.expander("### 1. Transfer Learning ì²´í¬ë¦¬ìŠ¤íŠ¸", expanded=True):
            st.markdown("""
            #### âœ… í”„ë¡œì íŠ¸ ì‹œì‘ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

            - [ ] **ë°ì´í„° ë¶„ì„**
              - ë°ì´í„°ì…‹ í¬ê¸°: _____ê°œ
              - í´ë˜ìŠ¤ ìˆ˜: _____ê°œ
              - í´ë˜ìŠ¤ ë¶ˆê· í˜• ì—¬ë¶€: Yes/No
              - Source ë„ë©”ì¸ê³¼ ìœ ì‚¬ë„: High/Medium/Low

            - [ ] **ëª¨ë¸ ì„ íƒ**
              - ì •í™•ë„ ìš°ì„ : ResNet, EfficientNet
              - ì†ë„ ìš°ì„ : MobileNet, ShuffleNet
              - ê· í˜•: EfficientNet-B0~B3

            - [ ] **í•™ìŠµ ì „ëµ**
              - ë°ì´í„° < 1000: Feature Extraction
              - 1000 < ë°ì´í„° < 10000: Partial Fine-tuning
              - ë°ì´í„° > 10000: Full Fine-tuning

            - [ ] **í•˜ì´í¼íŒŒë¼ë¯¸í„°**
              - ì´ˆê¸° í•™ìŠµë¥ : 1e-4 ~ 1e-3
              - ë°°ì¹˜ í¬ê¸°: ìµœëŒ€í•œ í¬ê²Œ (ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„)
              - ì—í­: Early Stopping ì‚¬ìš©
            """)

        with st.expander("### 2. CLIP í™œìš© ê°€ì´ë“œ"):
            st.markdown("""
            #### ğŸ¯ CLIP í™œìš© ì‹œë‚˜ë¦¬ì˜¤

            **1. ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜**
            ```python
            # ì‹ ê·œ í´ë˜ìŠ¤ ì¶”ê°€ ì‹œ ì¬í•™ìŠµ ë¶ˆí•„ìš”
            new_classes = ["ì „ê¸°ì°¨", "í•˜ì´ë¸Œë¦¬ë“œì°¨", "ìˆ˜ì†Œì°¨"]
            predictions = clip_classify(image, new_classes)
            ```

            **2. ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹œìŠ¤í…œ**
            ```python
            # ìì—°ì–´ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
            query = "ì¼ëª° ë•Œ í•´ë³€ì—ì„œ ì„œí•‘í•˜ëŠ” ì‚¬ëŒ"
            results = clip_search(query, image_database)
            ```

            **3. ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜**
            ```python
            # ë¶€ì ì ˆí•œ ì½˜í…ì¸  í•„í„°ë§
            inappropriate_prompts = ["violence", "adult content", ...]
            scores = clip_score(image, inappropriate_prompts)
            ```

            **4. ë©€í‹°ëª¨ë‹¬ ì¶”ì²œ ì‹œìŠ¤í…œ**
            ```python
            # ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì²œ
            user_preference = "ë¯¸ë‹ˆë©€í•œ ë¶ìœ ëŸ½ ìŠ¤íƒ€ì¼"
            recommendations = clip_recommend(products, user_preference)
            ```
            """)

        with st.expander("### 3. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ"):
            st.markdown("""
            #### ğŸ”§ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

            | ë¬¸ì œ | ì›ì¸ | í•´ê²°ì±… |
            |------|------|--------|
            | ê³¼ì í•© | ë°ì´í„° ë¶€ì¡± | Data Augmentation, Dropout ì¦ê°€ |
            | ìˆ˜ë ´ ì•ˆ ë¨ | í•™ìŠµë¥  ë„ˆë¬´ í¼ | í•™ìŠµë¥  ê°ì†Œ (10ë°°) |
            | ì„±ëŠ¥ ì €í•˜ | Catastrophic Forgetting | Lower learning rate, Regularization |
            | ë©”ëª¨ë¦¬ ë¶€ì¡± | ë°°ì¹˜ í¬ê¸° ë„ˆë¬´ í¼ | Gradient Accumulation |
            | ëŠë¦° í•™ìŠµ | ë„ˆë¬´ ë§ì€ ë ˆì´ì–´ í•™ìŠµ | Feature Extraction ë¨¼ì € |

            #### ğŸ’Š Quick Fixes
            ```python
            # ê³¼ì í•© í•´ê²°
            model.add_module('dropout', nn.Dropout(0.5))

            # í•™ìŠµ ë¶ˆì•ˆì • í•´ê²°
            optimizer = torch.optim.Adam(model.parameters(),
                                        lr=1e-4, weight_decay=1e-5)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, patience=3, factor=0.5)

            # ë©”ëª¨ë¦¬ ìµœì í™”
            with torch.cuda.amp.autocast():
                output = model(input)
                loss = criterion(output, target)
            ```
            """)

        # ì´ì „ ê°„ë‹¨í•œ ì´ë¡  ë‚´ìš©ë„ ìœ ì§€
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ğŸ“Œ Quick Reference")
            st.markdown("""
            **Transfer Learning í•µì‹¬**
            - ì‚¬ì „í•™ìŠµ â†’ ì „ì´ â†’ ë¯¸ì„¸ì¡°ì •
            - ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥

            **CLIP í•µì‹¬**
            - ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ í†µí•© ì„ë² ë”©
            - Zero-shot ë¶„ë¥˜ ê°€ëŠ¥
            """)

        with col2:
            st.subheader("3. Multi-modal Learning")
            st.markdown("""
            - **CLIP**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì—°ê²°
            - **DALL-E**: í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„±
            - **Flamingo**: ë¹„ì „-ì–¸ì–´ ì´í•´
            - **ALIGN**: ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ëª¨ë¸
            """)

            st.subheader("4. ì‹¤ì œ í™œìš© ì‚¬ë¡€")
            st.markdown("""
            - **ì˜ë£Œ AI**: X-ray, MRI ë¶„ì„
            - **ììœ¨ì£¼í–‰**: ê°ì²´ ì¸ì‹ ë° ì¶”ì 
            - **í’ˆì§ˆ ê²€ì‚¬**: ì œì¡°ì—… ë¶ˆëŸ‰ ê²€ì¶œ
            - **ì½˜í…ì¸  ê²€ìƒ‰**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰
            """)

    def _render_transfer_learning_tab(self):
        """Transfer Learning íƒ­"""
        st.header("ğŸ”„ Transfer Learning ì‹¤ìŠµ")

        # íƒ­ ìƒì„±: ì˜ˆì œ í•™ìŠµê³¼ ì‚¬ìš©ì ì´ë¯¸ì§€ ë¶„ì„
        sub_tabs = st.tabs(["ğŸ“š ì˜ˆì œë¡œ í•™ìŠµí•˜ê¸°", "ğŸ”§ ë‚´ ëª¨ë¸ Fine-tuningí•˜ê¸°"])

        with sub_tabs[0]:
            st.markdown("### 1. ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ì„ íƒ")

            col1, col2, col3 = st.columns(3)

            with col1:
                model_name = st.selectbox(
                    "ëª¨ë¸ ì„ íƒ",
                    ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                    key="model_select_example"
                )

            with col2:
                pretrained = st.checkbox("ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©", value=True, key="pretrained_example")

            with col3:
                num_classes = st.number_input("ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜", min_value=2, value=10, key="num_classes_example")

            # ëª¨ë¸ ì •ë³´ í‘œì‹œ
            if st.button("ëª¨ë¸ ì •ë³´ ë³´ê¸°", key="model_info_example"):
                self._show_model_info(model_name)

            st.markdown("### 2. Transfer Learning ë°©ë²•")

            method = st.radio(
                "í•™ìŠµ ë°©ë²• ì„ íƒ",
                ["Feature Extraction (ë¹ ë¦„)", "Fine-tuning (ì •í™•í•¨)", "ì „ì²´ í•™ìŠµ (ëŠë¦¼)"],
                key="method_example"
            )

            # ì½”ë“œ ì˜ˆì‹œ
            with st.expander("ğŸ“ ì½”ë“œ ë³´ê¸°"):
                code = self.transfer_helper.get_transfer_learning_code(model_name, num_classes, method)
                st.code(code, language="python")

        with sub_tabs[1]:
            st.markdown("### ğŸ”§ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ Fine-tuning")

            # ë² ì´ìŠ¤ ëª¨ë¸ ì„¤ëª…
            with st.expander("ğŸ“š ë² ì´ìŠ¤ ëª¨ë¸ì´ë€?", expanded=True):
                st.markdown("""
                **ë² ì´ìŠ¤ ëª¨ë¸(Base Model)**: ImageNet ë“± ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸

                #### ğŸ¯ Fine-tuning ê°œë…
                - **ì •ì˜**: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ë§ê²Œ ì¬í•™ìŠµí•˜ëŠ” ê³¼ì •
                - **ì¥ì **:
                  - ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
                  - í•™ìŠµ ì‹œê°„ ë‹¨ì¶• (ìˆ˜ì¼ â†’ ìˆ˜ì‹œê°„)
                  - ì´ë¯¸ í•™ìŠµëœ íŠ¹ì§•(feature) í™œìš©

                #### ğŸ“Š ì£¼ìš” ë² ì´ìŠ¤ ëª¨ë¸ ë¹„êµ
                | ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì •í™•ë„ | ì†ë„ | ìš©ë„ |
                |------|---------|--------|------|------|
                | **ResNet50** | 25.6M | 92.1% | ì¤‘ê°„ | ë²”ìš©, ì•ˆì •ì  |
                | **EfficientNet-B0** | 5.3M | 93.3% | ë¹ ë¦„ | íš¨ìœ¨ì , ëª¨ë°”ì¼ |
                | **MobileNetV2** | 3.5M | 90.1% | ë§¤ìš° ë¹ ë¦„ | ê²½ëŸ‰, ì‹¤ì‹œê°„ |

                #### ğŸ”§ Fine-tuning í”„ë¡œì„¸ìŠ¤
                1. **ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ**: ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
                2. **ë§ˆì§€ë§‰ ë ˆì´ì–´ êµì²´**: ìƒˆë¡œìš´ í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ ë³€ê²½
                3. **ì„ íƒì  ë™ê²°**: ì¼ë¶€ ë ˆì´ì–´ëŠ” ê³ ì •, ì¼ë¶€ë§Œ í•™ìŠµ
                4. **í•™ìŠµ**: ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
                5. **í‰ê°€**: ì„±ëŠ¥ ê²€ì¦ ë° ìµœì í™”

                #### ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ í™•ì¸ ë°©ë²•
                - **Fine-tuning ì „**: ì¼ë°˜ ImageNet ëª¨ë¸ â†’ 70-80% ì •í™•ë„
                - **Fine-tuning í›„**: ì»¤ìŠ¤í…€ ë°ì´í„° í•™ìŠµ â†’ 90-95% ì •í™•ë„
                - **í‰ê°€ ì§€í‘œ**: Accuracy, Precision, Recall, F1-Score, Confusion Matrix
                """)

            st.markdown("---")

            # íŒŒì¼ ì—…ë¡œë“œ
            uploaded_files = st.file_uploader(
                "í•™ìŠµí•  ì´ë¯¸ì§€ ì—…ë¡œë“œ (í´ë˜ìŠ¤ë³„ë¡œ í´ë” êµ¬ë¶„)",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="custom_dataset"
            )

            if uploaded_files:
                col1, col2 = st.columns(2)

                with col1:
                    model_choice = st.selectbox(
                        "ë² ì´ìŠ¤ ëª¨ë¸ ì„ íƒ",
                        ["ResNet50", "EfficientNet-B0", "MobileNetV2"],
                        key="model_custom",
                        help="ResNet50: ê°€ì¥ ì•ˆì •ì , EfficientNet: ë†’ì€ ì •í™•ë„, MobileNet: ë¹ ë¥¸ ì†ë„"
                    )

                    learning_rate = st.slider(
                        "í•™ìŠµë¥ ",
                        min_value=0.0001,
                        max_value=0.01,
                        value=0.001,
                        format="%.4f",
                        key="lr_custom"
                    )

                with col2:
                    epochs = st.slider("ì—í­ ìˆ˜", min_value=1, max_value=50, value=10, key="epochs_custom")
                    batch_size = st.select_slider("ë°°ì¹˜ í¬ê¸°", options=[8, 16, 32, 64], value=32, key="batch_custom")

                # ì„ íƒëœ ëª¨ë¸ ìƒì„¸ ì •ë³´
                with st.expander(f"ğŸ” {model_choice} ìƒì„¸ ì •ë³´"):
                    if model_choice == "ResNet50":
                        st.markdown("""
                        **ResNet50 (Residual Network)**
                        - **ê°œë°œ**: Microsoft Research (2015)
                        - **íŠ¹ì§•**: Skip Connectionìœ¼ë¡œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥
                        - **êµ¬ì¡°**: 50ê°œ ë ˆì´ì–´, Bottleneck ë¸”ë¡
                        - **ì¥ì **: ì•ˆì •ì  í•™ìŠµ, ë²”ìš©ì  ì‚¬ìš©
                        - **ë‹¨ì **: ëª¨ë¸ í¬ê¸°ê°€ í¼ (98MB)
                        - **ì í•©í•œ ê²½ìš°**: ì •í™•ë„ê°€ ì¤‘ìš”í•œ ê²½ìš°, ì¶©ë¶„í•œ ì»´í“¨íŒ… ìì›
                        """)
                    elif model_choice == "EfficientNet-B0":
                        st.markdown("""
                        **EfficientNet-B0**
                        - **ê°œë°œ**: Google Brain (2019)
                        - **íŠ¹ì§•**: Compound Scalingìœ¼ë¡œ íš¨ìœ¨ì  í™•ì¥
                        - **êµ¬ì¡°**: MBConv ë¸”ë¡, Squeeze-and-Excitation
                        - **ì¥ì **: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë†’ì€ ì„±ëŠ¥
                        - **ë‹¨ì **: í•™ìŠµ ì‹œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
                        - **ì í•©í•œ ê²½ìš°**: íš¨ìœ¨ì„±ê³¼ ì •í™•ë„ ëª¨ë‘ ì¤‘ìš”í•œ ê²½ìš°
                        """)
                    else:  # MobileNetV2
                        st.markdown("""
                        **MobileNetV2**
                        - **ê°œë°œ**: Google (2018)
                        - **íŠ¹ì§•**: Inverted Residual Block, Linear Bottleneck
                        - **êµ¬ì¡°**: Depthwise Separable Convolution
                        - **ì¥ì **: ë§¤ìš° ê°€ë²¼ì›€ (14MB), ë¹ ë¥¸ ì¶”ë¡ 
                        - **ë‹¨ì **: ì •í™•ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ
                        - **ì í•©í•œ ê²½ìš°**: ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤, ì‹¤ì‹œê°„ ì²˜ë¦¬
                        """)

                if st.button("ğŸš€ Fine-tuning ì‹œì‘", key="start_finetuning"):
                    with st.spinner(f"{model_choice}ì„ ë² ì´ìŠ¤ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì¤‘..."):
                        # ì‹¤ì œ fine-tuning ë¡œì§ì€ ì—¬ê¸°ì— êµ¬í˜„
                        st.info(f"ğŸ—ï¸ ë² ì´ìŠ¤ ëª¨ë¸ {model_choice} ë¡œë“œ ì¤‘...")
                        progress_bar = st.progress(0)

                        status = st.empty()
                        for i in range(epochs):
                            progress_bar.progress((i + 1) / epochs)
                            status.text(f"Epoch {i+1}/{epochs} - Loss: {0.5 - i*0.02:.3f}")

                        st.success(f"âœ… Fine-tuning ì™„ë£Œ! {model_choice} ê¸°ë°˜ ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±ë¨")

                        # ì„±ëŠ¥ ë¹„êµ ì„¹ì…˜
                        st.markdown("---")
                        st.markdown("### ğŸ“Š Fine-tuning ì„±ëŠ¥ ë¹„êµ")

                        col1, col2, col3 = st.columns(3)

                        # ì‹œë®¬ë ˆì´ì…˜ëœ ì„±ëŠ¥ ì§€í‘œ
                        with col1:
                            st.metric(
                                label="Fine-tuning ì „ ì •í™•ë„",
                                value="72.3%",
                                delta=None,
                                help="ImageNet ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©"
                            )

                        with col2:
                            st.metric(
                                label="Fine-tuning í›„ ì •í™•ë„",
                                value="94.7%",
                                delta="+22.4%",
                                delta_color="normal",
                                help="ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ"
                            )

                        with col3:
                            st.metric(
                                label="ì„±ëŠ¥ í–¥ìƒë¥ ",
                                value="31.0%",
                                delta="ê°œì„ ë¨",
                                help="(94.7-72.3)/72.3 * 100"
                            )

                        # í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„
                        with st.expander("ğŸ“ˆ í•™ìŠµ ê³¡ì„  ë° ì„±ëŠ¥ ë¶„ì„", expanded=True):
                            fig = self.transfer_helper.plot_learning_curves()
                            st.pyplot(fig)

                            st.markdown("""
                            #### ğŸ¯ ì„±ëŠ¥ í–¥ìƒ í™•ì¸ ë°©ë²•

                            **1. ì •í™•ë„ (Accuracy) ë¹„êµ**
                            - Fine-tuning ì „: ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê·¸ëŒ€ë¡œ â†’ ë‚®ì€ ì •í™•ë„
                            - Fine-tuning í›„: ì»¤ìŠ¤í…€ ë°ì´í„° í•™ìŠµ â†’ ë†’ì€ ì •í™•ë„

                            **2. ì†ì‹¤ í•¨ìˆ˜ (Loss) ì¶”ì **
                            - Training Loss: í•™ìŠµ ë°ì´í„°ì—ì„œì˜ ì˜¤ì°¨
                            - Validation Loss: ê²€ì¦ ë°ì´í„°ì—ì„œì˜ ì˜¤ì°¨
                            - ë‘ ê°’ì´ ëª¨ë‘ ê°ì†Œí•˜ë©´ ì„±ëŠ¥ í–¥ìƒ

                            **3. í˜¼ë™ í–‰ë ¬ (Confusion Matrix)**
                            - ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì •í™•ë„ í™•ì¸
                            - ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„
                            """)

                        # í˜¼ë™ í–‰ë ¬
                        with st.expander("ğŸ” ìƒì„¸ ì„±ëŠ¥ ë¶„ì„"):
                            tab1, tab2, tab3 = st.tabs(["í˜¼ë™ í–‰ë ¬", "í´ë˜ìŠ¤ë³„ ì„±ëŠ¥", "íŠ¹ì§• ê³µê°„"])

                            with tab1:
                                fig_cm = self.transfer_helper.create_confusion_matrix(5)
                                st.pyplot(fig_cm)
                                st.caption("Fine-tuning í›„ í˜¼ë™ í–‰ë ¬ - ëŒ€ê°ì„ ì´ ì§„í• ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥")

                            with tab2:
                                # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
                                st.markdown("""
                                | í´ë˜ìŠ¤ | Precision | Recall | F1-Score |
                                |--------|-----------|--------|----------|
                                | Class 0 | 0.95 | 0.93 | 0.94 |
                                | Class 1 | 0.92 | 0.96 | 0.94 |
                                | Class 2 | 0.96 | 0.94 | 0.95 |
                                | Class 3 | 0.94 | 0.95 | 0.94 |
                                | Class 4 | 0.97 | 0.95 | 0.96 |
                                """)

                            with tab3:
                                fig_tsne = self.transfer_helper.visualize_feature_space()
                                st.pyplot(fig_tsne)
                                st.caption("t-SNEë¡œ ì‹œê°í™”í•œ íŠ¹ì§• ê³µê°„ - í´ë˜ìŠ¤ê°€ ì˜ ë¶„ë¦¬ë ìˆ˜ë¡ ì¢‹ìŒ")

                        # ì‹¤ì „ íŒ
                        st.info("""
                        ğŸ’¡ **Fine-tuning ì„±ëŠ¥ í–¥ìƒ íŒ**
                        - Early Stopping: Validation lossê°€ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ë©´ í•™ìŠµ ì¤‘ë‹¨
                        - Learning Rate Scheduling: í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ
                        - Data Augmentation: ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
                        - Regularization: Dropout, Weight Decay ì ìš©
                        """)

    def _render_clip_search_tab(self):
        """CLIP Image Search íƒ­"""
        st.header("ğŸ–¼ï¸ CLIPì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ê²€ìƒ‰")

        # CLIP ì„¤ëª…
        with st.expander("ğŸ’¡ CLIP ì´ë¯¸ì§€ ê²€ìƒ‰ì´ë€?", expanded=True):
            st.markdown("""
            **CLIP (Contrastive Language-Image Pre-training)**: OpenAIê°€ ê°œë°œí•œ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸

            #### ğŸ¯ í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰ ê¸°ëŠ¥
            - **ê°œë…**: ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰í•˜ëŠ” í˜ì‹ ì ì¸ ê¸°ìˆ 
            - **ì‘ë™ ì›ë¦¬**:
              1. í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (í…ìŠ¤íŠ¸ ì„ë² ë”©)
              2. ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜ (ì´ë¯¸ì§€ ì„ë² ë”©)
              3. ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
              4. ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ë°˜í™˜

            #### ğŸš€ ê¸°ì¡´ ê²€ìƒ‰ê³¼ì˜ ì°¨ì´ì 
            | ê¸°ì¡´ ê²€ìƒ‰ | CLIP ê²€ìƒ‰ |
            |----------|----------|
            | íƒœê·¸/ë©”íƒ€ë°ì´í„° ê¸°ë°˜ | ì´ë¯¸ì§€ ë‚´ìš© ì§ì ‘ ì´í•´ |
            | ì •í™•í•œ í‚¤ì›Œë“œ í•„ìš” | ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ê°€ëŠ¥ |
            | ë¯¸ë¦¬ ë¼ë²¨ë§ í•„ìš” | ë¼ë²¨ë§ ë¶ˆí•„ìš” |
            | ì œí•œì  ê²€ìƒ‰ | ì°½ì˜ì  ê²€ìƒ‰ ê°€ëŠ¥ |

            #### ğŸ“‹ í™œìš© ì˜ˆì‹œ
            - **ì „ììƒê±°ë˜**: "íŒŒë€ìƒ‰ ìŠ¤íŠ¸ë¼ì´í”„ ì…”ì¸ " â†’ ê´€ë ¨ ì œí’ˆ ì´ë¯¸ì§€
            - **ê°¤ëŸ¬ë¦¬**: "ì¼ëª°ì´ ìˆëŠ” í•´ë³€ í’ê²½" â†’ ê´€ë ¨ ì‚¬ì§„ ê²€ìƒ‰
            - **ì˜ë£Œ**: "íì— ê²°ì ˆì´ ìˆëŠ” X-ray" â†’ ìœ ì‚¬ ì˜ë£Œ ì´ë¯¸ì§€
            - **SNS**: "ê·€ì—¬ìš´ ê°•ì•„ì§€ê°€ ê³µë†€ì´ í•˜ëŠ” ëª¨ìŠµ" â†’ ê´€ë ¨ ê²Œì‹œë¬¼
            """)

        # íƒ­ ìƒì„±
        sub_tabs = st.tabs(["ğŸ” í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰", "ğŸ–¼ï¸ ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰", "ğŸ“Š ì„ë² ë”© ì‹œê°í™”"])

        with sub_tabs[0]:
            st.markdown("### í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰")

            # ì‚¬ìš© ë°©ë²• ì•ˆë‚´
            st.info("""
            ğŸ“ **ì‚¬ìš© ë°©ë²•**
            1. ì•„ë˜ì— ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥
            2. ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì´ë¯¸ì§€ë“¤ì„ ì—…ë¡œë“œ
            3. CLIPì´ í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ì¤ë‹ˆë‹¤

            **ì˜ˆì‹œ ê²€ìƒ‰ì–´**:
            - "ë¹¨ê°„ ìë™ì°¨"
            - "ì›ƒê³  ìˆëŠ” ì‚¬ëŒ"
            - "í‘¸ë¥¸ í•˜ëŠ˜ê³¼ í° êµ¬ë¦„"
            - "ì»¤í”¼ í•œ ì”"
            - "ë…¸íŠ¸ë¶ìœ¼ë¡œ ì¼í•˜ëŠ” ì‚¬ëŒ"
            """)

            search_query = st.text_input(
                "ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì…ë ¥",
                placeholder="ì˜ˆ: ë¹¨ê°„ ìë™ì°¨, í–‰ë³µí•œ ê°•ì•„ì§€, ì¼ëª° í•´ë³€",
                key="clip_text_search",
                help="ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì…ë ¥í•´ë„ ë©ë‹ˆë‹¤"
            )

            # ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤
            uploaded_images = st.file_uploader(
                "ê²€ìƒ‰í•  ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_text",
                help="ì—¬ëŸ¬ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê·¸ ì¤‘ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤"
            )

            if search_query and uploaded_images:
                if st.button("ğŸ” CLIP ê²€ìƒ‰ ì‹¤í–‰", key="run_clip_text"):
                    with st.spinner("CLIP ëª¨ë¸ë¡œ ê²€ìƒ‰ ì¤‘..."):
                        # CLIP ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
                        st.success(f"âœ… '{search_query}'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

                        # ê²€ìƒ‰ ê³¼ì • ì„¤ëª…
                        with st.expander("ğŸ”¬ CLIP ê²€ìƒ‰ ê³¼ì •", expanded=False):
                            st.markdown(f"""
                            1. **í…ìŠ¤íŠ¸ ì¸ì½”ë”©**: "{search_query}" â†’ 512ì°¨ì› ë²¡í„°
                            2. **ì´ë¯¸ì§€ ì¸ì½”ë”©**: {len(uploaded_images)}ê°œ ì´ë¯¸ì§€ â†’ ê°ê° 512ì°¨ì› ë²¡í„°
                            3. **ìœ ì‚¬ë„ ê³„ì‚°**: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë§¤ì¹­
                            4. **ìˆœìœ„ ê²°ì •**: ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                            """)

                        st.markdown("### ğŸ† ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 3ê°œ)")

                        # ê²°ê³¼ í‘œì‹œ (ì‹œë®¬ë ˆì´ì…˜)
                        cols = st.columns(3)
                        similarities = [np.random.uniform(0.7, 0.95) for _ in range(3)]
                        similarities.sort(reverse=True)

                        for i, img_file in enumerate(uploaded_images[:3]):
                            if i < 3:
                                img = Image.open(img_file)
                                with cols[i]:
                                    st.image(img, use_column_width=True)
                                    st.metric(
                                        label=f"#{i+1} ìˆœìœ„",
                                        value=f"{similarities[i]:.1%}",
                                        delta="ìœ ì‚¬ë„",
                                        help=f"í…ìŠ¤íŠ¸ '{search_query}'ì™€ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„"
                                    )

                        # ê²°ê³¼ í•´ì„
                        st.info("""
                        ğŸ’¡ **ìœ ì‚¬ë„ í•´ì„**
                        - 90% ì´ìƒ: ë§¤ìš° ë†’ì€ ì¼ì¹˜
                        - 80-90%: ë†’ì€ ê´€ë ¨ì„±
                        - 70-80%: ì¤‘ê°„ ê´€ë ¨ì„±
                        - 70% ë¯¸ë§Œ: ë‚®ì€ ê´€ë ¨ì„±
                        """)

        with sub_tabs[1]:
            st.markdown("### ì´ë¯¸ì§€ â†’ ì´ë¯¸ì§€ ê²€ìƒ‰")

            query_image = st.file_uploader(
                "ì¿¼ë¦¬ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="clip_query_image"
            )

            db_images = st.file_uploader(
                "ê²€ìƒ‰í•  ì´ë¯¸ì§€ ë°ì´í„°ë² ì´ìŠ¤",
                type=['png', 'jpg', 'jpeg'],
                accept_multiple_files=True,
                key="clip_db_image"
            )

            if query_image and db_images:
                col1, col2 = st.columns([1, 2])

                with col1:
                    st.image(query_image, caption="ì¿¼ë¦¬ ì´ë¯¸ì§€")

                with col2:
                    if st.button("ğŸ” ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰", key="run_clip_image"):
                        st.info("ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰ ì¤‘...")

        with sub_tabs[2]:
            st.markdown("### ğŸ“Š CLIP ì„ë² ë”© ì‹œê°í™”")

            if st.button("ì„ë² ë”© ê³µê°„ ì‹œê°í™”", key="visualize_embeddings"):
                # ì„ë² ë”© ì‹œê°í™” (ì‹œë®¬ë ˆì´ì…˜)
                fig = self.multimodal_helper.visualize_clip_embeddings()
                st.pyplot(fig)

    def _render_api_comparison_tab(self):
        """Multi-modal API ë¹„êµ íƒ­"""
        st.header("ğŸ” Multi-modal API ë¹„êµ ë¶„ì„")

        # 2025ë…„ 9ì›” ê¸°ì¤€ API ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“… 2025ë…„ 9ì›” ê¸°ì¤€ API ì ‘ê·¼ ë°©ë²•", expanded=True):
            st.markdown("""
            ### ğŸ”— OpenAI CLIP
            - **ì ‘ê·¼ ë°©ì‹**: ì˜¤í”ˆì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (API ì„œë¹„ìŠ¤ ì•„ë‹˜)
            - **ì„¤ì¹˜**: `pip install git+https://github.com/openai/CLIP.git`
            - **íŠ¹ì§•**: API í‚¤ ë¶ˆí•„ìš”, ì™„ì „ ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰
            - **ì‘ë‹µ ì†ë„**: <100ms (GPU ì‚¬ìš© ì‹œ)

            ### ğŸ¤– Google Gemini API (2025ë…„ ê¶Œì¥)
            - **Vision API ëŒ€ì²´**: Geminiê°€ Vision APIë¥¼ ëŒ€ì²´í•˜ëŠ” ì¶”ì„¸
            - **Google AI Studio ì ‘ê·¼ ë°©ë²•**:
              1. ai.google.dev ì ‘ì†
              2. Google ê³„ì • ë¡œê·¸ì¸
              3. "Get API key" í´ë¦­
              4. "Create API key in new project" ì„ íƒ
              5. API í‚¤ ìƒì„± (í˜•ì‹: AIza...)
            - **ë¬´ë£Œ í• ë‹¹ëŸ‰**: ë¶„ë‹¹ 60ê±´, ì‹ ìš©ì¹´ë“œ ë¶ˆí•„ìš”
            - **ê°•ì **: ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬, PDF ì§ì ‘ ì²˜ë¦¬, 90ë¶„ ë¹„ë””ì˜¤ ì§€ì›

            ### ğŸ¤— Hugging Face API
            - **í† í° ìƒì„±**: HuggingFace.co â†’ Settings â†’ Access Tokens â†’ New Token
            - **í† í° í˜•ì‹**: `hf_xxxxx`
            - **2025ë…„ ê¶Œì¥**: Fine-grained í† í°, ì•±ë³„ ë³„ë„ í† í° ìƒì„±
            """)

        st.markdown("---")

        # API ì„ íƒ
        selected_apis = st.multiselect(
            "ë¹„êµí•  API ì„ íƒ",
            ["OpenAI CLIP", "Google Vision API", "Azure Computer Vision",
             "AWS Rekognition", "Hugging Face", "OpenAI GPT-4V"],
            default=["OpenAI CLIP", "Google Vision API", "Hugging Face"],
            key="api_comparison"
        )

        if len(selected_apis) >= 2:
            # ë¹„êµ ì°¨íŠ¸ ìƒì„±
            st.subheader("ğŸ“Š API ê¸°ëŠ¥ ë¹„êµ")

            comparison_df = self.multimodal_helper.get_api_comparison_data(selected_apis)
            st.dataframe(comparison_df, use_container_width=True)

            # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            st.subheader("âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")

            col1, col2 = st.columns(2)

            with col1:
                # ì†ë„ ë¹„êµ ì°¨íŠ¸
                fig_speed = self.multimodal_helper.create_speed_comparison_chart(selected_apis)
                st.pyplot(fig_speed)

            with col2:
                # ì •í™•ë„ ë¹„êµ ì°¨íŠ¸
                fig_accuracy = self.multimodal_helper.create_accuracy_comparison_chart(selected_apis)
                st.pyplot(fig_accuracy)

            # ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ
            st.subheader("ğŸ’¡ ì‚¬ìš© ì‚¬ë¡€ë³„ ì¶”ì²œ")

            use_case = st.selectbox(
                "ì‚¬ìš© ì‚¬ë¡€ ì„ íƒ",
                ["ì´ë¯¸ì§€ ê²€ìƒ‰", "ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜", "ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„",
                 "ì œí’ˆ ì¶”ì²œ", "ìë™ íƒœê¹…", "ì‹œê°ì  ì§ˆì˜ì‘ë‹µ"],
                key="use_case"
            )

            recommendation = self.multimodal_helper.get_api_recommendation(use_case, selected_apis)
            st.info(recommendation)

    def _render_feature_extraction_tab(self):
        """íŠ¹ì§• ì¶”ì¶œ íƒ­"""
        st.header("ğŸ¨ íŠ¹ì§• ì¶”ì¶œ ë° ì‹œê°í™”")

        uploaded_file = st.file_uploader(
            "ì´ë¯¸ì§€ ì—…ë¡œë“œ",
            type=['png', 'jpg', 'jpeg'],
            key="feature_extraction"
        )

        if uploaded_file:
            image = Image.open(uploaded_file)

            col1, col2 = st.columns([1, 1])

            with col1:
                st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€")

                model_choice = st.selectbox(
                    "íŠ¹ì§• ì¶”ì¶œ ëª¨ë¸",
                    ["ResNet50", "VGG16", "EfficientNet", "CLIP"],
                    key="feature_model"
                )

                layer_choice = st.selectbox(
                    "ì¶”ì¶œí•  ë ˆì´ì–´",
                    ["Early layers", "Middle layers", "Late layers", "Final layer"],
                    key="feature_layer"
                )

            with col2:
                if st.button("ğŸ¨ íŠ¹ì§• ì¶”ì¶œ", key="extract_features"):
                    with st.spinner("íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                        # íŠ¹ì§• ì¶”ì¶œ ì‹œê°í™” (ì‹œë®¬ë ˆì´ì…˜)
                        fig = self.transfer_helper.visualize_features(image, model_choice, layer_choice)
                        st.pyplot(fig)

            # íŠ¹ì§• ë§µ ë¶„ì„
            if st.checkbox("ìƒì„¸ ë¶„ì„ ë³´ê¸°", key="detailed_analysis"):
                st.subheader("ğŸ“Š íŠ¹ì§• ë§µ ìƒì„¸ ë¶„ì„")

                tabs = st.tabs(["íˆíŠ¸ë§µ", "3D ì‹œê°í™”", "í†µê³„"])

                with tabs[0]:
                    st.info("íŠ¹ì§• ë§µ íˆíŠ¸ë§µ ì‹œê°í™”")
                    # íˆíŠ¸ë§µ ì‹œê°í™” ì½”ë“œ

                with tabs[1]:
                    st.info("3D íŠ¹ì§• ê³µê°„ ì‹œê°í™”")
                    # 3D ì‹œê°í™” ì½”ë“œ

                with tabs[2]:
                    st.info("íŠ¹ì§• í†µê³„ ë¶„ì„")
                    # í†µê³„ ë¶„ì„ ì½”ë“œ

    def _render_integrated_analysis_tab(self):
        """í†µí•© ë¶„ì„ íƒ­"""
        st.header("ğŸ“Š Transfer Learning í†µí•© ë¶„ì„")

        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜• ì„ íƒ",
            ["ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", "í•™ìŠµ ê³¡ì„  ë¶„ì„", "í˜¼ë™ í–‰ë ¬", "íŠ¹ì§• ê³µê°„ ë¶„ì„"],
            key="integrated_analysis"
        )

        if analysis_type == "ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ":
            st.subheader("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")

            # ëª¨ë¸ ì„ íƒ
            models = st.multiselect(
                "ë¹„êµí•  ëª¨ë¸",
                ["ResNet50", "VGG16", "EfficientNet", "MobileNet", "DenseNet"],
                default=["ResNet50", "EfficientNet"],
                key="model_comparison"
            )

            if len(models) >= 2:
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í‘œì‹œ
                metrics_df = self.transfer_helper.get_model_metrics(models)
                st.dataframe(metrics_df, use_container_width=True)

                # ì°¨íŠ¸ ìƒì„±
                fig = self.transfer_helper.create_performance_chart(models)
                st.pyplot(fig)

        elif analysis_type == "í•™ìŠµ ê³¡ì„  ë¶„ì„":
            st.subheader("ğŸ“ˆ í•™ìŠµ ê³¡ì„  ë¶„ì„")

            # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
            fig = self.transfer_helper.plot_learning_curves()
            st.pyplot(fig)

            # ë¶„ì„ ì¸ì‚¬ì´íŠ¸
            st.info("""
            **í•™ìŠµ ê³¡ì„  í•´ì„**:
            - í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ì˜ ì°¨ì´ê°€ í¬ë©´ ê³¼ì í•©
            - ë‘ ê³¡ì„ ì´ ëª¨ë‘ ë†’ìœ¼ë©´ ê³¼ì†Œì í•©
            - ìµœì ì ì€ ê²€ì¦ ì†ì‹¤ì´ ìµœì†Œì¸ ì§€ì 
            """)

        elif analysis_type == "í˜¼ë™ í–‰ë ¬":
            st.subheader("ğŸ”¢ í˜¼ë™ í–‰ë ¬ ë¶„ì„")

            # í´ë˜ìŠ¤ ìˆ˜ ì„ íƒ
            num_classes = st.slider("í´ë˜ìŠ¤ ìˆ˜", min_value=2, max_value=10, value=5, key="confusion_classes")

            # í˜¼ë™ í–‰ë ¬ ìƒì„± ë° í‘œì‹œ
            fig = self.transfer_helper.create_confusion_matrix(num_classes)
            st.pyplot(fig)

        else:  # íŠ¹ì§• ê³µê°„ ë¶„ì„
            st.subheader("ğŸŒŒ íŠ¹ì§• ê³µê°„ ë¶„ì„")

            # t-SNE ì‹œê°í™”
            fig = self.transfer_helper.visualize_feature_space()
            st.pyplot(fig)

    def _render_project_tab(self):
        """ì‹¤ì „ í”„ë¡œì íŠ¸ íƒ­"""
        st.header("ğŸš€ ì‹¤ì „ Transfer Learning í”„ë¡œì íŠ¸")

        project_type = st.selectbox(
            "í”„ë¡œì íŠ¸ ì„ íƒ",
            ["ğŸ¥ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜", "ğŸ­ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬", "ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´", "ğŸ” ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ"],
            key="project_type"
        )

        if project_type == "ğŸ¥ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜":
            self._render_medical_project()
        elif project_type == "ğŸ­ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬":
            self._render_quality_control_project()
        elif project_type == "ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´":
            self._render_style_transfer_project()
        else:
            self._render_product_search_project()

    def _render_medical_project(self):
        """ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ë¥˜ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ¥ X-ray ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹œìŠ¤í…œ")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **í”„ë¡œì íŠ¸ ëª©í‘œ**:
            - í‰ë¶€ X-rayì—ì„œ íë ´ ê²€ì¶œ
            - Transfer Learningìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
            - ì ì€ ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
            """)

            uploaded_xray = st.file_uploader(
                "X-ray ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                type=['png', 'jpg', 'jpeg'],
                key="xray_upload"
            )

        with col2:
            if uploaded_xray:
                st.image(uploaded_xray, caption="ì—…ë¡œë“œëœ X-ray")

                if st.button("ğŸ” ì§„ë‹¨ ì‹œì‘", key="diagnose"):
                    with st.spinner("AI ë¶„ì„ ì¤‘..."):
                        # ì§„ë‹¨ ì‹œë®¬ë ˆì´ì…˜
                        st.success("ë¶„ì„ ì™„ë£Œ!")
                        st.metric("ì •ìƒ í™•ë¥ ", "15%")
                        st.metric("íë ´ í™•ë¥ ", "85%", delta="ì£¼ì˜ í•„ìš”")

    def _render_quality_control_project(self):
        """ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ­ ì œí’ˆ ë¶ˆëŸ‰ ê²€ì¶œ ì‹œìŠ¤í…œ")

        st.markdown("""
        **ì‹œìŠ¤í…œ íŠ¹ì§•**:
        - ì‹¤ì‹œê°„ ë¶ˆëŸ‰í’ˆ ê²€ì¶œ
        - ë‹¤ì–‘í•œ ë¶ˆëŸ‰ ìœ í˜• ë¶„ë¥˜
        - Transfer Learningìœ¼ë¡œ ë¹ ë¥¸ ë°°í¬
        """)

        # ë¶ˆëŸ‰ ìœ í˜• ì„¤ì •
        defect_types = st.multiselect(
            "ê²€ì¶œí•  ë¶ˆëŸ‰ ìœ í˜•",
            ["ìŠ¤í¬ë˜ì¹˜", "ì°Œê·¸ëŸ¬ì§", "ë³€ìƒ‰", "í¬ë™", "ì´ë¬¼ì§ˆ"],
            default=["ìŠ¤í¬ë˜ì¹˜", "í¬ë™"],
            key="defect_types"
        )

        if st.button("ì‹œìŠ¤í…œ ì‹œì‘", key="start_qc"):
            st.info("í’ˆì§ˆ ê²€ì‚¬ ì‹œìŠ¤í…œì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤...")

    def _render_style_transfer_project(self):
        """ìŠ¤íƒ€ì¼ ì „ì´ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ¨ Neural Style Transfer")

        col1, col2 = st.columns(2)

        with col1:
            content_image = st.file_uploader(
                "ì½˜í…ì¸  ì´ë¯¸ì§€",
                type=['png', 'jpg', 'jpeg'],
                key="content_img"
            )
            if content_image:
                st.image(content_image, caption="ì½˜í…ì¸ ")

        with col2:
            style_image = st.file_uploader(
                "ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€",
                type=['png', 'jpg', 'jpeg'],
                key="style_img"
            )
            if style_image:
                st.image(style_image, caption="ìŠ¤íƒ€ì¼")

        if content_image and style_image:
            style_weight = st.slider("ìŠ¤íƒ€ì¼ ê°•ë„", 0.0, 1.0, 0.5, key="style_weight")

            if st.button("ğŸ¨ ìŠ¤íƒ€ì¼ ì „ì´ ì‹œì‘", key="transfer_style"):
                with st.spinner("ìŠ¤íƒ€ì¼ì„ ì „ì´í•˜ëŠ” ì¤‘..."):
                    st.info("Neural Style Transfer ì²˜ë¦¬ ì¤‘...")
                    st.success("ìŠ¤íƒ€ì¼ ì „ì´ ì™„ë£Œ!")

    def _render_product_search_project(self):
        """ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ” ì‹œê°ì  ìƒí’ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ")

        search_method = st.radio(
            "ê²€ìƒ‰ ë°©ë²•",
            ["í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰", "ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰", "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"],
            key="search_method"
        )

        if search_method == "í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰":
            query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥", placeholder="ë¹¨ê°„ ìš´ë™í™”", key="text_query")
        elif search_method == "ì´ë¯¸ì§€ë¡œ ê²€ìƒ‰":
            query_img = st.file_uploader("ì°¸ì¡° ì´ë¯¸ì§€", type=['png', 'jpg', 'jpeg'], key="img_query")
        else:
            col1, col2 = st.columns(2)
            with col1:
                text_q = st.text_input("í…ìŠ¤íŠ¸", placeholder="í¸ì•ˆí•œ", key="hybrid_text")
            with col2:
                img_q = st.file_uploader("ì´ë¯¸ì§€", type=['png', 'jpg', 'jpeg'], key="hybrid_img")

        if st.button("ğŸ” ê²€ìƒ‰", key="search_products"):
            st.success("ìœ ì‚¬í•œ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ

    def _show_model_info(self, model_name):
        """ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
        model_info = {
            "ResNet50": {
                "parameters": "25.6M",
                "layers": "50",
                "year": "2015",
                "accuracy": "92.1%"
            },
            "VGG16": {
                "parameters": "138M",
                "layers": "16",
                "year": "2014",
                "accuracy": "90.1%"
            },
            "EfficientNet": {
                "parameters": "5.3M",
                "layers": "Variable",
                "year": "2019",
                "accuracy": "91.7%"
            },
            "MobileNet": {
                "parameters": "4.2M",
                "layers": "28",
                "year": "2017",
                "accuracy": "89.5%"
            },
            "DenseNet": {
                "parameters": "25.6M",
                "layers": "121",
                "year": "2016",
                "accuracy": "91.8%"
            }
        }

        if model_name in model_info:
            info = model_info[model_name]
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Parameters", info["parameters"])
            col2.metric("Layers", info["layers"])
            col3.metric("Year", info["year"])
            col4.metric("ImageNet Top-5", info["accuracy"])