#!/usr/bin/env python3
"""
Week 5 Lab: ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ê°ì²´ íƒì§€
ì›¹ìº ê³¼ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ êµì‹¤ ë¬¼ê±´ íƒì§€

ì´ ì‹¤ìŠµì—ì„œëŠ”:
1. ì›¹ìº ì„ í†µí•œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€
2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ë° ì €ì¥
3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
4. ë‹¤ì–‘í•œ ì‹œê°í™” ì˜µì…˜
"""

import cv2
import numpy as np
import time
import argparse
from pathlib import Path
from collections import deque, Counter
import json
from datetime import datetime
import threading
import queue
import warnings
warnings.filterwarnings('ignore')

# YOLOv8 import
try:
    from ultralytics import YOLO
    print("âœ… Ultralytics YOLOv8 íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    print("âŒ Ultralytics íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install ultralytics")
    exit(1)

class RealTimeDetector:
    """
    ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ í´ë˜ìŠ¤
    """
    
    def __init__(self, model_path='yolov8n.pt', conf_threshold=0.25, iou_threshold=0.7):
        """
        ì‹¤ì‹œê°„ íƒì§€ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: YOLO ëª¨ë¸ ê²½ë¡œ
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            iou_threshold: IoU ì„ê³„ê°’
        """
        self.model_path = model_path
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = None
        self.load_model()
        
        # êµì‹¤ ë¬¼ê±´ í´ë˜ìŠ¤ (ì»¤ìŠ¤í…€ ëª¨ë¸ìš©)
        self.classroom_classes = {
            0: 'book',
            1: 'laptop', 
            2: 'chair',
            3: 'whiteboard',
            4: 'bag'
        }
        
        # COCO í´ë˜ìŠ¤ì—ì„œ êµì‹¤ ê´€ë ¨ í´ë˜ìŠ¤ë“¤
        self.coco_classroom_filter = [
            'book', 'laptop', 'chair', 'backpack', 'handbag', 'suitcase',
            'bottle', 'cup', 'cell phone', 'clock', 'mouse', 'keyboard', 'remote'
        ]
        
        # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ
        self.class_colors = {
            'book': (0, 0, 255),        # ë¹¨ê°• (BGR)
            'laptop': (0, 255, 0),      # ì´ˆë¡
            'chair': (255, 0, 0),       # íŒŒë‘
            'whiteboard': (0, 255, 255), # ë…¸ë‘
            'bag': (255, 0, 255),       # ë§ˆì  íƒ€
            'backpack': (255, 0, 255),
            'handbag': (0, 165, 255),   # ì£¼í™©
            'suitcase': (0, 255, 128),
            'bottle': (255, 255, 0),    # ì‹œì•ˆ
            'cup': (203, 192, 255),     # í•‘í¬
            'cell phone': (128, 0, 128),
            'clock': (0, 165, 255),
            'mouse': (255, 128, 0),
            'keyboard': (128, 128, 128),
            'remote': (208, 224, 64)
        }
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.fps_queue = deque(maxlen=30)
        self.detection_history = deque(maxlen=100)
        self.frame_count = 0
        self.total_inference_time = 0
        
        # í†µê³„
        self.session_stats = {
            'start_time': datetime.now(),
            'total_frames': 0,
            'total_detections': 0,
            'class_counts': Counter(),
            'avg_fps': 0,
            'avg_inference_time': 0
        }
        
        # ì‹œê°í™” ì˜µì…˜
        self.show_fps = True
        self.show_confidence = True
        self.show_class_count = True
        self.show_inference_time = True
        
        # ë…¹í™” ì„¤ì •
        self.is_recording = False
        self.video_writer = None
        self.output_path = None
    
    def load_model(self):
        """YOLO ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ”„ ëª¨ë¸ ë¡œë”©: {self.model_path}")
            self.model = YOLO(self.model_path)
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            if hasattr(self.model, 'info'):
                self.model.info()
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = None
    
    def detect_objects(self, frame):
        """
        í”„ë ˆì„ì—ì„œ ê°ì²´ íƒì§€
        
        Args:
            frame: ì…ë ¥ í”„ë ˆì„
        
        Returns:
            results: íƒì§€ ê²°ê³¼
            inference_time: ì¶”ë¡  ì‹œê°„ (ms)
        """
        if self.model is None:
            return None, 0
        
        start_time = time.time()
        
        try:
            results = self.model.predict(
                frame,
                conf=self.conf_threshold,
                iou=self.iou_threshold,
                verbose=False,
                stream=False
            )
            
            end_time = time.time()
            inference_time = (end_time - start_time) * 1000
            
            return results[0] if results else None, inference_time
            
        except Exception as e:
            print(f"âŒ íƒì§€ ì‹¤íŒ¨: {e}")
            return None, 0
    
    def filter_classroom_objects(self, results):
        """êµì‹¤ ê´€ë ¨ ê°ì²´ë§Œ í•„í„°ë§"""
        if results is None or results.boxes is None:
            return results
        
        # ì»¤ìŠ¤í…€ ëª¨ë¸ì¸ì§€ í™•ì¸ (í´ë˜ìŠ¤ ìˆ˜ë¡œ íŒë‹¨)
        num_classes = len(results.names)
        is_custom_model = num_classes <= 10  # ì¼ë°˜ì ìœ¼ë¡œ ì»¤ìŠ¤í…€ ëª¨ë¸ì€ í´ë˜ìŠ¤ê°€ ì ìŒ
        
        if is_custom_model:
            return results  # ì»¤ìŠ¤í…€ ëª¨ë¸ì€ ì´ë¯¸ êµì‹¤ ê°ì²´ë§Œ íƒì§€
        
        # COCO ëª¨ë¸ì—ì„œ êµì‹¤ ê´€ë ¨ ê°ì²´ë§Œ í•„í„°ë§
        boxes = results.boxes.xyxy.cpu().numpy()
        confidences = results.boxes.conf.cpu().numpy()
        class_ids = results.boxes.cls.cpu().numpy().astype(int)
        
        filtered_indices = []
        for i, class_id in enumerate(class_ids):
            class_name = results.names[class_id]
            if class_name in self.coco_classroom_filter:
                filtered_indices.append(i)
        
        if filtered_indices:
            # í•„í„°ë§ëœ ê²°ê³¼ ìƒì„±
            import torch
            results.boxes.xyxy = torch.tensor(boxes[filtered_indices])
            results.boxes.conf = torch.tensor(confidences[filtered_indices])
            results.boxes.cls = torch.tensor(class_ids[filtered_indices])
        else:
            results.boxes = None
        
        return results
    
    def draw_detections(self, frame, results, inference_time):
        """
        íƒì§€ ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°
        
        Args:
            frame: ì›ë³¸ í”„ë ˆì„
            results: íƒì§€ ê²°ê³¼
            inference_time: ì¶”ë¡  ì‹œê°„
        
        Returns:
            annotated_frame: ì–´ë…¸í…Œì´ì…˜ëœ í”„ë ˆì„
            detection_count: íƒì§€ëœ ê°ì²´ ìˆ˜
        """
        annotated_frame = frame.copy()
        detection_count = 0
        class_counts = Counter()
        
        if results is not None and results.boxes is not None:
            boxes = results.boxes.xyxy.cpu().numpy()
            confidences = results.boxes.conf.cpu().numpy()
            class_ids = results.boxes.cls.cpu().numpy().astype(int)
            
            detection_count = len(boxes)
            
            # ê° íƒì§€ ê²°ê³¼ ê·¸ë¦¬ê¸°
            for box, conf, class_id in zip(boxes, confidences, class_ids):
                x1, y1, x2, y2 = box.astype(int)
                
                # í´ë˜ìŠ¤ ì´ë¦„ ê²°ì •
                class_name = results.names[class_id]
                class_counts[class_name] += 1
                
                # ìƒ‰ìƒ ì„ íƒ
                color = self.class_colors.get(class_name, (128, 128, 128))
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                if self.show_confidence:
                    label = f"{class_name}: {conf:.2f}"
                else:
                    label = class_name
                
                # ë¼ë²¨ ë°°ê²½
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), 
                            (x1 + label_size[0], y1), color, -1)
                
                # ë¼ë²¨ í…ìŠ¤íŠ¸
                cv2.putText(annotated_frame, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.update_statistics(detection_count, class_counts, inference_time)
        
        # ì •ë³´ íŒ¨ë„ ê·¸ë¦¬ê¸°
        self.draw_info_panel(annotated_frame, detection_count, class_counts, inference_time)
        
        return annotated_frame, detection_count
    
    def update_statistics(self, detection_count, class_counts, inference_time):
        """í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.session_stats['total_frames'] += 1
        self.session_stats['total_detections'] += detection_count
        self.session_stats['class_counts'].update(class_counts)
        
        # í‰ê·  ì¶”ë¡  ì‹œê°„ ì—…ë°ì´íŠ¸
        self.total_inference_time += inference_time
        self.session_stats['avg_inference_time'] = self.total_inference_time / self.session_stats['total_frames']
        
        # íƒì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.detection_history.append({
            'timestamp': time.time(),
            'detections': detection_count,
            'inference_time': inference_time,
            'classes': dict(class_counts)
        })
    
    def draw_info_panel(self, frame, detection_count, class_counts, inference_time):
        """ì •ë³´ íŒ¨ë„ ê·¸ë¦¬ê¸°"""
        h, w = frame.shape[:2]
        
        # ë°°ê²½ íŒ¨ë„
        panel_height = 120
        cv2.rectangle(frame, (10, 10), (400, panel_height), (0, 0, 0), -1)
        cv2.rectangle(frame, (10, 10), (400, panel_height), (255, 255, 255), 2)
        
        y_offset = 30
        
        # FPS í‘œì‹œ
        if self.show_fps and len(self.fps_queue) > 0:
            avg_fps = sum(self.fps_queue) / len(self.fps_queue)
            cv2.putText(frame, f"FPS: {avg_fps:.1f}", (20, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            y_offset += 20
        
        # ì¶”ë¡  ì‹œê°„ í‘œì‹œ
        if self.show_inference_time:
            cv2.putText(frame, f"Inference: {inference_time:.1f}ms", (20, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            y_offset += 20
        
        # íƒì§€ ê°ì²´ ìˆ˜ í‘œì‹œ
        cv2.putText(frame, f"Objects: {detection_count}", (20, y_offset), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
        y_offset += 20
        
        # í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸ í‘œì‹œ (ìƒìœ„ 3ê°œ)
        if self.show_class_count and class_counts:
            most_common = class_counts.most_common(3)
            class_text = ", ".join([f"{cls}:{cnt}" for cls, cnt in most_common])
            cv2.putText(frame, f"Classes: {class_text}", (20, y_offset), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 192, 203), 2)
        
        # ë…¹í™” ìƒíƒœ í‘œì‹œ
        if self.is_recording:
            cv2.circle(frame, (w - 30, 30), 10, (0, 0, 255), -1)
            cv2.putText(frame, "REC", (w - 60, 40), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
    
    def start_recording(self, output_path, fps=30):
        """ë¹„ë””ì˜¤ ë…¹í™” ì‹œì‘"""
        self.output_path = output_path
        self.is_recording = True
        
        # VideoWriter ì„¤ì •ì€ ì²« í”„ë ˆì„ì—ì„œ ìˆ˜í–‰
        self.video_writer = None
        print(f"ğŸ”´ ë…¹í™” ì‹œì‘: {output_path}")
    
    def stop_recording(self):
        """ë¹„ë””ì˜¤ ë…¹í™” ì¤‘ì§€"""
        if self.is_recording and self.video_writer is not None:
            self.video_writer.release()
            self.video_writer = None
            self.is_recording = False
            print(f"â¹ï¸ ë…¹í™” ì™„ë£Œ: {self.output_path}")
    
    def process_webcam(self, camera_id=0, window_name="êµì‹¤ ë¬¼ê±´ íƒì§€ê¸°"):
        """
        ì›¹ìº ì„ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ íƒì§€
        
        Args:
            camera_id: ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)
            window_name: ìœˆë„ìš° ì´ë¦„
        """
        print(f"ğŸ“¹ ì›¹ìº  ì‹œì‘ (ì¹´ë©”ë¼ ID: {camera_id})")
        
        # ì›¹ìº  ì´ˆê¸°í™”
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ {camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì¹´ë©”ë¼ ì„¤ì •
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = cap.get(cv2.CAP_PROP_FPS)
        
        print(f"âœ… ì¹´ë©”ë¼ ì„¤ì •: {actual_width}x{actual_height} @ {actual_fps}fps")
        print("\nğŸ® ì¡°ì‘ë²•:")
        print("  ESC: ì¢…ë£Œ")
        print("  SPACE: ë…¹í™” ì‹œì‘/ì¤‘ì§€")
        print("  S: ìŠ¤í¬ë¦°ìƒ· ì €ì¥")
        print("  F: FPS í‘œì‹œ í† ê¸€")
        print("  C: ì‹ ë¢°ë„ í‘œì‹œ í† ê¸€")
        print("  I: ì¶”ë¡  ì‹œê°„ í‘œì‹œ í† ê¸€")
        
        frame_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # FPS ê³„ì‚°
                current_time = time.time()
                fps = 1 / (current_time - frame_time) if (current_time - frame_time) > 0 else 0
                frame_time = current_time
                self.fps_queue.append(fps)
                
                # ê°ì²´ íƒì§€
                results, inference_time = self.detect_objects(frame)
                results = self.filter_classroom_objects(results)
                
                # ê²°ê³¼ ì‹œê°í™”
                annotated_frame, detection_count = self.draw_detections(frame, results, inference_time)
                
                # ë…¹í™”
                if self.is_recording:
                    if self.video_writer is None:
                        # ì²« í”„ë ˆì„ì—ì„œ VideoWriter ì´ˆê¸°í™”
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        self.video_writer = cv2.VideoWriter(
                            self.output_path, fourcc, 30.0, 
                            (annotated_frame.shape[1], annotated_frame.shape[0])
                        )
                    
                    if self.video_writer is not None:
                        self.video_writer.write(annotated_frame)
                
                # í™”ë©´ ì¶œë ¥
                cv2.imshow(window_name, annotated_frame)
                
                # í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                
                if key == 27:  # ESC
                    break
                elif key == ord(' '):  # SPACE - ë…¹í™” í† ê¸€
                    if self.is_recording:
                        self.stop_recording()
                    else:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        output_path = f"classroom_detection_{timestamp}.mp4"
                        self.start_recording(output_path)
                elif key == ord('s'):  # S - ìŠ¤í¬ë¦°ìƒ·
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    screenshot_path = f"screenshot_{timestamp}.jpg"
                    cv2.imwrite(screenshot_path, annotated_frame)
                    print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
                elif key == ord('f'):  # F - FPS í† ê¸€
                    self.show_fps = not self.show_fps
                    print(f"FPS í‘œì‹œ: {'ON' if self.show_fps else 'OFF'}")
                elif key == ord('c'):  # C - ì‹ ë¢°ë„ í† ê¸€
                    self.show_confidence = not self.show_confidence
                    print(f"ì‹ ë¢°ë„ í‘œì‹œ: {'ON' if self.show_confidence else 'OFF'}")
                elif key == ord('i'):  # I - ì¶”ë¡  ì‹œê°„ í† ê¸€
                    self.show_inference_time = not self.show_inference_time
                    print(f"ì¶”ë¡  ì‹œê°„ í‘œì‹œ: {'ON' if self.show_inference_time else 'OFF'}")
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        
        finally:
            # ì •ë¦¬
            if self.is_recording:
                self.stop_recording()
            
            cap.release()
            cv2.destroyAllWindows()
            
            # ì„¸ì…˜ í†µê³„ ì¶œë ¥
            self.print_session_statistics()
    
    def process_video_file(self, input_path, output_path=None):
        """
        ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            input_path: ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_path: ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        input_path = Path(input_path)
        
        if not input_path.exists():
            print(f"âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}")
            return
        
        print(f"ğŸ¬ ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬: {input_path}")
        
        # ë¹„ë””ì˜¤ ìº¡ì²˜ ì´ˆê¸°í™”
        cap = cv2.VideoCapture(str(input_path))
        
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
            return
        
        # ë¹„ë””ì˜¤ ì •ë³´
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps:.1f}fps, {total_frames}í”„ë ˆì„")
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        video_writer = None
        if output_path:
            output_path = Path(output_path)
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
            print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {output_path}")
        
        frame_count = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # ê°ì²´ íƒì§€
                results, inference_time = self.detect_objects(frame)
                results = self.filter_classroom_objects(results)
                
                # ê²°ê³¼ ì‹œê°í™”
                annotated_frame, detection_count = self.draw_detections(frame, results, inference_time)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress = (frame_count / total_frames) * 100
                
                # ì§„í–‰ë¥ ì„ í”„ë ˆì„ì— í‘œì‹œ
                cv2.putText(annotated_frame, f"Progress: {progress:.1f}% ({frame_count}/{total_frames})", 
                           (10, height - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # ì¶œë ¥ ë¹„ë””ì˜¤ì— ì €ì¥
                if video_writer is not None:
                    video_writer.write(annotated_frame)
                
                # í™”ë©´ ì¶œë ¥ (ì„ íƒì‚¬í•­)
                cv2.imshow('ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘...', annotated_frame)
                
                # ESCë¡œ ì¤‘ë‹¨ ê°€ëŠ¥
                if cv2.waitKey(1) & 0xFF == 27:
                    print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
                    break
                
                # ì§„í–‰ë¥  ì¶œë ¥ (ë§¤ 100í”„ë ˆì„ë§ˆë‹¤)
                if frame_count % 100 == 0:
                    elapsed_time = time.time() - start_time
                    estimated_total = elapsed_time * total_frames / frame_count
                    remaining_time = estimated_total - elapsed_time
                    
                    print(f"ì§„í–‰ë¥ : {progress:.1f}% | "
                          f"ê²½ê³¼: {elapsed_time:.1f}s | "
                          f"ë‚¨ì€ ì‹œê°„: {remaining_time:.1f}s")
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        
        finally:
            # ì •ë¦¬
            cap.release()
            if video_writer is not None:
                video_writer.release()
            cv2.destroyAllWindows()
            
            # ì²˜ë¦¬ ì™„ë£Œ ë©”ì‹œì§€
            total_time = time.time() - start_time
            print(f"\nâœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"   ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_count}/{total_frames}")
            print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ")
            print(f"   í‰ê·  FPS: {frame_count/total_time:.1f}")
            
            # ì„¸ì…˜ í†µê³„ ì¶œë ¥
            self.print_session_statistics()
    
    def print_session_statistics(self):
        """ì„¸ì…˜ í†µê³„ ì¶œë ¥"""
        duration = datetime.now() - self.session_stats['start_time']
        
        print("\nğŸ“Š ì„¸ì…˜ í†µê³„")
        print("=" * 50)
        print(f"ì„¸ì…˜ ì‹œê°„: {duration}")
        print(f"ì´ í”„ë ˆì„: {self.session_stats['total_frames']}")
        print(f"ì´ íƒì§€ ê°ì²´: {self.session_stats['total_detections']}")
        print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {self.session_stats['avg_inference_time']:.1f}ms")
        
        if len(self.fps_queue) > 0:
            print(f"í‰ê·  FPS: {sum(self.fps_queue)/len(self.fps_queue):.1f}")
        
        if self.session_stats['total_frames'] > 0:
            print(f"í”„ë ˆì„ë‹¹ í‰ê·  ê°ì²´: {self.session_stats['total_detections']/self.session_stats['total_frames']:.1f}")
        
        print("\ní´ë˜ìŠ¤ë³„ íƒì§€ ìˆ˜:")
        for class_name, count in self.session_stats['class_counts'].most_common():
            percentage = (count / self.session_stats['total_detections']) * 100 if self.session_stats['total_detections'] > 0 else 0
            print(f"  {class_name}: {count}ê°œ ({percentage:.1f}%)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì‹¤ì‹œê°„ êµì‹¤ ë¬¼ê±´ íƒì§€")
    parser.add_argument('--model', type=str, default='yolov8n.pt', 
                       help='YOLO ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: yolov8n.pt)')
    parser.add_argument('--source', type=str, default='webcam', 
                       help='ì…ë ¥ ì†ŒìŠ¤: webcam, ë¹„ë””ì˜¤íŒŒì¼ê²½ë¡œ')
    parser.add_argument('--output', type=str, default=None, 
                       help='ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--conf', type=float, default=0.25, 
                       help='ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.25)')
    parser.add_argument('--iou', type=float, default=0.7, 
                       help='IoU ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.7)')
    parser.add_argument('--camera', type=int, default=0, 
                       help='ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)')
    
    args = parser.parse_args()
    
    print("ğŸ¯ ì‹¤ì‹œê°„ êµì‹¤ ë¬¼ê±´ íƒì§€ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print(f"ëª¨ë¸: {args.model}")
    print(f"ì‹ ë¢°ë„ ì„ê³„ê°’: {args.conf}")
    print(f"IoU ì„ê³„ê°’: {args.iou}")
    
    # íƒì§€ê¸° ì´ˆê¸°í™”
    detector = RealTimeDetector(
        model_path=args.model,
        conf_threshold=args.conf,
        iou_threshold=args.iou
    )
    
    if detector.model is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    try:
        if args.source == 'webcam':
            # ì›¹ìº  ëª¨ë“œ
            detector.process_webcam(camera_id=args.camera)
        else:
            # ë¹„ë””ì˜¤ íŒŒì¼ ëª¨ë“œ
            if not Path(args.source).exists():
                print(f"âŒ ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.source}")
                return
            
            output_path = args.output
            if output_path is None:
                # ìë™ ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
                input_path = Path(args.source)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = f"{input_path.stem}_detected_{timestamp}.mp4"
            
            detector.process_video_file(args.source, output_path)
    
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print("\nğŸ‰ ì‹¤ì‹œê°„ íƒì§€ ì„¸ì…˜ ì™„ë£Œ!")

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ì ì—†ì´ ì‹¤í–‰ ì‹œ ëŒ€í™”í˜• ëª¨ë“œ
    import sys
    
    if len(sys.argv) == 1:
        print("ğŸ¯ ì‹¤ì‹œê°„ êµì‹¤ ë¬¼ê±´ íƒì§€ ì‹œìŠ¤í…œ")
        print("=" * 60)
        
        print("\nğŸ“‹ ì‹¤í–‰ ëª¨ë“œ ì„ íƒ:")
        print("1. ì›¹ìº  ì‹¤ì‹œê°„ íƒì§€")
        print("2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬")
        print("3. ëª…ë ¹í–‰ ë„ì›€ë§")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
        
        if choice == '1':
            # ì›¹ìº  ëª¨ë“œ
            detector = RealTimeDetector()
            if detector.model is not None:
                detector.process_webcam()
        
        elif choice == '2':
            # ë¹„ë””ì˜¤ íŒŒì¼ ëª¨ë“œ
            video_path = input("ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if video_path and Path(video_path).exists():
                detector = RealTimeDetector()
                if detector.model is not None:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    output_path = f"output_detected_{timestamp}.mp4"
                    detector.process_video_file(video_path, output_path)
            else:
                print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì¼ ê²½ë¡œì…ë‹ˆë‹¤.")
        
        elif choice == '3':
            # ë„ì›€ë§
            print("\nğŸ“– ëª…ë ¹í–‰ ì‚¬ìš©ë²•:")
            print("python realtime_video_detection.py --help")
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    else:
        # ëª…ë ¹í–‰ ì¸ìê°€ ìˆëŠ” ê²½ìš°
        main()
