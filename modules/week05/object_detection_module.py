"""
Week 5: ê°ì²´ íƒì§€ì™€ YOLO ëª¨ë“ˆ
ê°ì²´ íƒì§€ ì´ë¡ , R-CNN ê³„ì—´, YOLO ì•„í‚¤í…ì²˜, ì‹¤ì „ í”„ë¡œì íŠ¸
"""

import streamlit as st
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import io
import os
import google.generativeai as genai

class ObjectDetectionModule:
    def _check_environment(self):
        """í™˜ê²½ ì²´í¬ ë° ìë™ ì„¤ì •"""
        import sys
        import subprocess

        issues = []

        # Python ë²„ì „ ì²´í¬
        python_version = sys.version_info
        if python_version.major == 3 and python_version.minor >= 12:
            # Python 3.12+ ê°ì§€
            try:
                import mediapipe
                import streamlit_webrtc
            except ImportError as e:
                issues.append(f"âŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ ëˆ„ë½: {str(e)}")

                st.warning("""
                **í™˜ê²½ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.**

                Python 3.12 ì´ìƒì—ì„œëŠ” ì¼ë¶€ íŒ¨í‚¤ì§€ì˜ í˜¸í™˜ì„± ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """)

                if st.button("ğŸ”§ ìë™ìœ¼ë¡œ í™˜ê²½ ì„¤ì •í•˜ê¸°", key="auto_setup_env"):
                    with st.spinner("í™˜ê²½ì„ ì„¤ì •í•˜ëŠ” ì¤‘..."):
                        try:
                            # requirements.txt ì—…ë°ì´íŠ¸
                            st.info("requirements.txtë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì¤‘...")

                            # í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
                            packages_to_install = [
                                "mediapipe>=0.10.0,<0.11",
                                "streamlit-webrtc>=0.63.0",
                                "numpy>=1.26.0,<2.0"
                            ]

                            for package in packages_to_install:
                                st.info(f"ì„¤ì¹˜ ì¤‘: {package}")
                                result = subprocess.run(
                                    [sys.executable, "-m", "pip", "install", package],
                                    capture_output=True,
                                    text=True
                                )
                                if result.returncode == 0:
                                    st.success(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
                                else:
                                    st.error(f"âŒ {package} ì„¤ì¹˜ ì‹¤íŒ¨: {result.stderr}")

                            st.success("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ! í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
                            st.balloons()

                        except Exception as e:
                            st.error(f"í™˜ê²½ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            st.code("""
# ìˆ˜ë™ ì„¤ì¹˜ ë°©ë²•:
python -m pip install mediapipe>=0.10.0,<0.11
python -m pip install streamlit-webrtc>=0.63.0
python -m pip install numpy>=1.26.0,<2.0
                            """, language="bash")

                return False

        elif python_version.major == 3 and python_version.minor == 13:
            st.error("""
            âš ï¸ **Python ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ**

            í˜„ì¬ Python 3.13ì„ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.
            mediapipeëŠ” Python 3.12 ì´í•˜ì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.

            **í•´ê²° ë°©ë²•:**
            1. Python 3.12 ì„¤ì¹˜
            2. ìƒˆ ê°€ìƒí™˜ê²½ ìƒì„±:
            ```bash
            py -3.12 -m venv venv
            venv\\Scripts\\activate
            pip install -r requirements.txt
            ```
            """)
            return False

        return True

    def _ensure_yolo_model(self, model_path="yolov8n.pt"):
        import os
        import requests
        import streamlit as st

        url = f"https://huggingface.co/ultralytics/yolov8/resolve/main/{model_path}"
        if not os.path.exists(model_path):
            with st.spinner(f"'{model_path}' ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤... (ì•½ 6MB)"):
                try:
                    response = requests.get(url, stream=True)
                    response.raise_for_status()
                    with open(model_path, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    st.success(f"'{model_path}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    return False
        return True

    def __init__(self):
        self.name = "Week 5: Object Detection & YOLO"

    def render(self):
        st.title("ğŸ¯ Week 5: ê°ì²´ íƒì§€ì™€ YOLO")
        st.markdown("**ê°ì²´ íƒì§€ì˜ ì´ë¡ ë¶€í„° YOLO ì‹¤ì „ êµ¬í˜„ê¹Œì§€**")

        # í™˜ê²½ ì²´í¬
        if not self._check_environment():
            st.warning("âš ï¸ í™˜ê²½ ì„¤ì • í›„ ê³„ì† ì§„í–‰í•˜ì„¸ìš”.")
            return

        tabs = st.tabs([
            "ğŸ“– ì´ë¡ ",
            "ğŸ” IoU & mAP",
            "ğŸ—ï¸ R-CNN ê³„ì—´",
            "âš¡ YOLO ë°œì „ì‚¬",
            "ğŸ¨ NMS",
            "ğŸ’» ì‹¤ì „ í”„ë¡œì íŠ¸"
        ])

        with tabs[0]:
            self.render_theory()

        with tabs[1]:
            self.render_iou_map()

        with tabs[2]:
            self.render_rcnn()

        with tabs[3]:
            self.render_yolo()

        with tabs[4]:
            self.render_nms()

        with tabs[5]:
            self.render_projects()

    def render_theory(self):
        """ê°ì²´ íƒì§€ ê¸°ì´ˆ ì´ë¡ """
        st.header("ğŸ“– ê°ì²´ íƒì§€ ê¸°ì´ˆ ì´ë¡ ")

        theory_tabs = st.tabs(["ê°œìš”", "êµ¬ì„± ìš”ì†Œ", "í‰ê°€ ì§€í‘œ"])

        with theory_tabs[0]:
            st.subheader("1. ê°ì²´ íƒì§€ë€?")

            st.markdown("""
            ### ì •ì˜
            **ê°ì²´ íƒì§€(Object Detection)**: ì´ë¯¸ì§€ì—ì„œ ê´€ì‹¬ ìˆëŠ” ê°ì²´ë“¤ì„ ì°¾ì•„ë‚´ê³ ,
            ê° ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ í‘œì‹œí•˜ë©°, ê°ì²´ì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…

            ### ì´ë¯¸ì§€ ë¶„ë¥˜ vs ê°ì²´ íƒì§€
            """)

            col1, col2 = st.columns(2)

            with col1:
                st.info("""
                **ì´ë¯¸ì§€ ë¶„ë¥˜ (Classification)**
                - ì…ë ¥: ì´ë¯¸ì§€
                - ì¶œë ¥: í´ë˜ìŠ¤ ë¼ë²¨ (ì˜ˆ: "ê³ ì–‘ì´")
                - ëª©ì : "ë¬´ì—‡ì¸ê°€?"
                """)

            with col2:
                st.success("""
                **ê°ì²´ íƒì§€ (Detection)**
                - ì…ë ¥: ì´ë¯¸ì§€
                - ì¶œë ¥: í´ë˜ìŠ¤ + ìœ„ì¹˜ + ì‹ ë¢°ë„
                - ëª©ì : "ë¬´ì—‡ì´ ì–´ë””ì—?"
                """)

            st.markdown("### ê°ì²´ íƒì§€ì˜ ë„ì „ê³¼ì œ")

            challenges = {
                "ë‹¤ì¤‘ ê°ì²´": "í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ê°ì²´ê°€ ì¡´ì¬",
                "ë‹¤ì–‘í•œ í¬ê¸°": "ê°™ì€ í´ë˜ìŠ¤ë¼ë„ í¬ê¸°ê°€ ë‹¤ì–‘í•¨",
                "ê°€ë ¤ì§(Occlusion)": "ê°ì²´ë“¤ì´ ì„œë¡œ ê²¹ì³ìˆìŒ",
                "ë°°ê²½ ë³µì¡ì„±": "ë³µì¡í•œ ë°°ê²½ì—ì„œ ê°ì²´ êµ¬ë¶„",
                "ì‹¤ì‹œê°„ ì²˜ë¦¬": "ë¹ ë¥¸ ì¶”ë¡  ì†ë„ ìš”êµ¬"
            }

            for challenge, description in challenges.items():
                st.markdown(f"**{challenge}**: {description}")

        with theory_tabs[1]:
            st.subheader("2. í•µì‹¬ êµ¬ì„± ìš”ì†Œ")

            st.markdown("### 1) ë°”ìš´ë”© ë°•ìŠ¤ (Bounding Box)")

            st.code("""
# ë°”ìš´ë”© ë°•ìŠ¤ í‘œí˜„ ë°©ì‹ë“¤
bbox_formats = {
    "xyxy": [x_min, y_min, x_max, y_max],           # ì¢Œìƒë‹¨, ìš°í•˜ë‹¨
    "xywh": [x_center, y_center, width, height],    # ì¤‘ì‹¬ì ê³¼ í¬ê¸°
    "cxcywh": [cx, cy, w, h],                       # ì •ê·œí™”ëœ ì¤‘ì‹¬ì 
}
            """, language="python")

            st.markdown("### 2) ì‹ ë¢°ë„ ì ìˆ˜ (Confidence Score)")
            st.latex(r"\text{Confidence} = P(\text{object}) \times \text{IoU}(\text{pred}, \text{true})")

            st.markdown("""
            - **P(object)**: í•´ë‹¹ ìœ„ì¹˜ì— ê°ì²´ê°€ ìˆì„ í™•ë¥ 
            - **IoU**: ì˜ˆì¸¡ ë°•ìŠ¤ì™€ ì‹¤ì œ ë°•ìŠ¤ì˜ ê²¹ì¹¨ ì •ë„
            """)

            st.markdown("### 3) í´ë˜ìŠ¤ í™•ë¥  (Class Probability)")
            st.code("""
# ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬
class_probs = softmax([logit_cat, logit_dog, logit_car, ...])
# ì˜ˆ: [0.7, 0.2, 0.05, 0.03, 0.02]
            """, language="python")

        with theory_tabs[2]:
            st.subheader("3. í‰ê°€ ì§€í‘œ")

            st.markdown("### IoU (Intersection over Union)")

            st.latex(r"\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}")

            st.markdown("""
            **IoU í•´ì„:**
            - IoU > 0.5: "ì¢‹ì€" íƒì§€
            - IoU > 0.7: "ë§¤ìš° ì¢‹ì€" íƒì§€
            - IoU > 0.9: "ê±°ì˜ ì™„ë²½í•œ" íƒì§€
            """)

            # IoU ì‹œë®¬ë ˆì´ì…˜
            st.markdown("#### IoU ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜")

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("**Ground Truth Box**")
                gt_x1 = st.slider("GT X1", 0, 100, 20, key="gt_x1")
                gt_y1 = st.slider("GT Y1", 0, 100, 20, key="gt_y1")
                gt_x2 = st.slider("GT X2", 0, 100, 60, key="gt_x2")
                gt_y2 = st.slider("GT Y2", 0, 100, 60, key="gt_y2")

            with col2:
                st.markdown("**Predicted Box**")
                pred_x1 = st.slider("Pred X1", 0, 100, 25, key="pred_x1")
                pred_y1 = st.slider("Pred Y1", 0, 100, 25, key="pred_y1")
                pred_x2 = st.slider("Pred X2", 0, 100, 65, key="pred_x2")
                pred_y2 = st.slider("Pred Y2", 0, 100, 65, key="pred_y2")

            # IoU ê³„ì‚°
            iou = self.calculate_iou(
                [gt_x1, gt_y1, gt_x2, gt_y2],
                [pred_x1, pred_y1, pred_x2, pred_y2]
            )

            st.metric("IoU", f"{iou:.3f}")

            # ì‹œê°í™”
            fig, ax = plt.subplots(figsize=(6, 6))
            ax.set_xlim(0, 100)
            ax.set_ylim(0, 100)
            ax.set_aspect('equal')

            # Ground Truth (íŒŒë€ìƒ‰)
            gt_rect = plt.Rectangle((gt_x1, gt_y1), gt_x2-gt_x1, gt_y2-gt_y1,
                                    linewidth=2, edgecolor='blue', facecolor='none',
                                    label='Ground Truth')
            ax.add_patch(gt_rect)

            # Prediction (ë¹¨ê°„ìƒ‰)
            pred_rect = plt.Rectangle((pred_x1, pred_y1), pred_x2-pred_x1, pred_y2-pred_y1,
                                      linewidth=2, edgecolor='red', facecolor='none',
                                      label='Prediction')
            ax.add_patch(pred_rect)

            ax.legend()
            ax.set_title(f'IoU = {iou:.3f}')
            ax.grid(True, alpha=0.3)

            st.pyplot(fig)
            plt.close()

            st.markdown("### mAP (mean Average Precision)")
            st.markdown("""
            **mAP ë³€í˜•ë“¤:**
            - **mAP@0.5**: IoU ì„ê³„ê°’ 0.5ì—ì„œì˜ mAP
            - **mAP@0.5:0.95**: IoU 0.5ë¶€í„° 0.95ê¹Œì§€ 0.05 ê°„ê²©ìœ¼ë¡œ í‰ê· 
            - **mAP@small/medium/large**: ê°ì²´ í¬ê¸°ë³„ mAP
            """)

    def render_iou_map(self):
        """IoUì™€ mAP ìƒì„¸ ì„¤ëª…"""
        st.header("ğŸ” IoU & mAP ì‹¬í™”")

        iou_tabs = st.tabs(["IoU ê³„ì‚°", "Precision-Recall", "mAP"])

        with iou_tabs[0]:
            st.subheader("IoU ê³„ì‚° ì‹¤ìŠµ")

            st.code("""
def calculate_iou(box1, box2):
    # êµì§‘í•© ì˜ì—­ ê³„ì‚°
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    intersection = max(0, x2 - x1) * max(0, y2 - y1)

    # ê° ë°•ìŠ¤ì˜ ë©´ì 
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # í•©ì§‘í•© ë©´ì 
    union = area1 + area2 - intersection

    return intersection / union if union > 0 else 0
            """, language="python")

        with iou_tabs[1]:
            st.subheader("Precision-Recall ê³¡ì„ ")

            st.markdown("""
            ### Precisionê³¼ Recall
            """)

            col1, col2 = st.columns(2)

            with col1:
                st.latex(r"\text{Precision} = \frac{TP}{TP + FP}")
                st.info("ì •ë°€ë„: ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œë¡œ ë§ì€ ë¹„ìœ¨")

            with col2:
                st.latex(r"\text{Recall} = \frac{TP}{TP + FN}")
                st.info("ì¬í˜„ìœ¨: ì‹¤ì œ ê°ì²´ ì¤‘ ì°¾ì•„ë‚¸ ë¹„ìœ¨")

            # PR ê³¡ì„  ì‹œë®¬ë ˆì´ì…˜
            st.markdown("#### PR ê³¡ì„  ì˜ˆì‹œ")

            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            recall = np.linspace(0, 1, 100)
            precision = 1 - recall * 0.3 + np.random.normal(0, 0.05, 100)
            precision = np.clip(precision, 0, 1)

            fig, ax = plt.subplots(figsize=(8, 6))
            ax.plot(recall, precision, 'b-', linewidth=2)
            ax.fill_between(recall, 0, precision, alpha=0.3)
            ax.set_xlabel('Recall', fontsize=12)
            ax.set_ylabel('Precision', fontsize=12)
            ax.set_title('Precision-Recall Curve', fontsize=14)
            ax.grid(True, alpha=0.3)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)

            # AP ê³„ì‚° (ê³¡ì„  ì•„ë˜ ë©´ì )
            ap = np.trapz(precision, recall)
            ax.text(0.6, 0.9, f'AP = {ap:.3f}', fontsize=14,
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

            st.pyplot(fig)
            plt.close()

        with iou_tabs[2]:
            st.subheader("mAP ê³„ì‚°")

            st.markdown("""
            ### Average Precision (AP)

            APëŠ” Precision-Recall ê³¡ì„  ì•„ë˜ì˜ ë©´ì ì…ë‹ˆë‹¤.
            """)

            st.code("""
def calculate_ap(precisions, recalls):
    # 11-point interpolation
    ap = 0
    for t in np.arange(0, 1.1, 0.1):  # 0.0, 0.1, ..., 1.0
        if np.sum(recalls >= t) == 0:
            p = 0
        else:
            p = np.max(precisions[recalls >= t])
        ap += p / 11
    return ap

def calculate_map(all_aps):
    # ëª¨ë“  í´ë˜ìŠ¤ì˜ AP í‰ê· 
    return np.mean(all_aps)
            """, language="python")

            st.markdown("### mAP@0.5:0.95")
            st.markdown("""
            COCO ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” ì§€í‘œ:
            - IoU ì„ê³„ê°’ì„ 0.5ë¶€í„° 0.95ê¹Œì§€ 0.05 ê°„ê²©ìœ¼ë¡œ ë³€ê²½
            - ê° ì„ê³„ê°’ì—ì„œ AP ê³„ì‚°
            - ëª¨ë“  APì˜ í‰ê·  ê³„ì‚°
            """)

    def render_rcnn(self):
        """R-CNN ê³„ì—´ ì„¤ëª…"""
        st.header("ğŸ—ï¸ R-CNN ê³„ì—´ì˜ ë°œì „")

        rcnn_tabs = st.tabs(["R-CNN", "Fast R-CNN", "Faster R-CNN", "ë¹„êµ"])

        with rcnn_tabs[0]:
            st.subheader("R-CNN (2014)")

            st.markdown("""
            ### í•µì‹¬ ì•„ì´ë””ì–´
            1. **Region Proposal**: Selective Searchë¡œ ê°ì²´ê°€ ìˆì„ ë§Œí•œ ì˜ì—­ ì œì•ˆ
            2. **CNN Feature Extraction**: ê° ì˜ì—­ì—ì„œ CNNìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
            3. **Classification**: SVMìœ¼ë¡œ ê°ì²´ ë¶„ë¥˜
            """)

            st.code("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     R-CNN êµ¬ì¡° (2014)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì…ë ¥ ì´ë¯¸ì§€
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Selective Search  â”‚  â† 2000ê°œ Region Proposal ìƒì„±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Region Warping   â”‚  â† ê° ì˜ì—­ì„ 227Ã—227ë¡œ í¬ê¸° ì¡°ì •
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CNN (AlexNet)   â”‚  â† ê° ì˜ì—­ë§ˆë‹¤ 4096ì°¨ì› íŠ¹ì§• ì¶”ì¶œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SVM #1 â”‚      â”‚ SVM #2 â”‚ ...  â”‚ Bbox Regress â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
            ìµœì¢… íƒì§€ ê²°ê³¼
""", language="text")

            st.warning("""
            **R-CNNì˜ í•œê³„:**
            - â±ï¸ ì†ë„: ì´ë¯¸ì§€ë‹¹ 47ì´ˆ (GPU ê¸°ì¤€)
            - ğŸ’¾ ë©”ëª¨ë¦¬: ê° ì˜ì—­ë§ˆë‹¤ CNN ì—°ì‚° í•„ìš”
            - ğŸ”§ ë³µì¡ì„±: 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸
            """)

        with rcnn_tabs[1]:
            st.subheader("Fast R-CNN (2015)")

            st.markdown("""
            ### ì£¼ìš” ê°œì„ ì‚¬í•­
            1. **ì „ì²´ ì´ë¯¸ì§€ CNN**: ì´ë¯¸ì§€ ì „ì²´ì— í•œ ë²ˆë§Œ CNN ì ìš©
            2. **RoI Pooling**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì˜ì—­ì„ ê³ ì • í¬ê¸°ë¡œ ë³€í™˜
            3. **Multi-task Loss**: ë¶„ë¥˜ì™€ ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€ë¥¼ ë™ì‹œì— í•™ìŠµ
            """)

            st.code("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Fast R-CNN êµ¬ì¡° (2015)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì…ë ¥ ì´ë¯¸ì§€
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                  â”‚
    â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CNN   â”‚     â”‚   Selective  â”‚
â”‚(VGG16) â”‚     â”‚    Search    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                  â”‚
    â”‚      Feature Map â”‚
    â”‚                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RoI Pooling    â”‚  â† ê° Regionì„ 7Ã—7 ê³ ì • í¬ê¸°ë¡œ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   FC Layers     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Softmax (ë¶„ë¥˜)â”‚  â”‚ Bbox Regressâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚              â”‚
             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
            ìµœì¢… íƒì§€ ê²°ê³¼
""", language="text")

            st.success("""
            **ì„±ëŠ¥ ê°œì„ :**
            - ì†ë„: ì´ë¯¸ì§€ë‹¹ 2.3ì´ˆ (9ë°° ë¹ ë¦„)
            - ì •í™•ë„: mAP 66% (R-CNN ëŒ€ë¹„ í–¥ìƒ)
            """)

            st.code("""
# RoI Pooling í•µì‹¬ ê°œë…
def roi_pooling(feature_map, roi, output_size=(7, 7)):
    x1, y1, x2, y2 = roi
    roi_feature = feature_map[:, :, y1:y2, x1:x2]
    pooled = adaptive_max_pool2d(roi_feature, output_size)
    return pooled
            """, language="python")

        with rcnn_tabs[2]:
            st.subheader("Faster R-CNN (2015)")

            st.markdown("""
            ### í˜ì‹ ì  ì•„ì´ë””ì–´: RPN (Region Proposal Network)

            Selective Searchë¥¼ ì‹ ê²½ë§ìœ¼ë¡œ ëŒ€ì²´!
            """)

            st.code("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Faster R-CNN êµ¬ì¡° (2015)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì…ë ¥ ì´ë¯¸ì§€
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CNN (VGG16)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ Feature Map
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                    â–¼                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚    RPN     â”‚    â”‚  RoI Pooling â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚(Region     â”‚    â”‚              â”‚
â”‚ Proposal   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Network)   â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â–¼
    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚   FC Layers     â”‚
    â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                     â”‚
    â”‚                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â–¼              â–¼
    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Softmax (ë¶„ë¥˜)â”‚  â”‚ Bbox Regressâ”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚              â”‚
                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                        ìµœì¢… íƒì§€ ê²°ê³¼

ã€RPN ìƒì„¸ êµ¬ì¡°ã€‘
Feature Map â†’ 3Ã—3 Conv â†’ 1Ã—1 Conv â”€â”¬â”€ 2k scores (obj/not obj)
                                   â””â”€ 4k coords (bbox regression)
                                      â†“
                                   9ê°œ Anchors per position
""", language="text")

            st.markdown("""
            #### ì•µì»¤ (Anchor) ê°œë…
            - íŠ¹ì§• ë§µì˜ ê° ìœ„ì¹˜ì— ë¯¸ë¦¬ ì •ì˜ëœ ë°•ìŠ¤ë“¤ì„ ë°°ì¹˜
            - 3ê°œ ìŠ¤ì¼€ì¼ Ã— 3ê°œ ë¹„ìœ¨ = 9ê°œ ì•µì»¤ per position
            """)

            col1, col2, col3 = st.columns(3)
            with col1:
                st.info("**ìŠ¤ì¼€ì¼**\n8, 16, 32")
            with col2:
                st.info("**ë¹„ìœ¨**\n0.5, 1.0, 2.0")
            with col3:
                st.info("**ì•µì»¤ ìˆ˜**\n9ê°œ/ìœ„ì¹˜")

            st.success("""
            **ì„±ëŠ¥ ê°œì„ :**
            - ì†ë„: ì´ë¯¸ì§€ë‹¹ 0.2ì´ˆ (ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥!)
            - ì •í™•ë„: mAP 73.2%
            - End-to-End: ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ í•œ ë²ˆì— í•™ìŠµ
            """)

        with rcnn_tabs[3]:
            st.subheader("R-CNN ê³„ì—´ ë¹„êµ")

            comparison_data = {
                "ëª¨ë¸": ["R-CNN", "Fast R-CNN", "Faster R-CNN"],
                "ì†ë„ (ì´ˆ/ì´ë¯¸ì§€)": [47, 2.3, 0.2],
                "mAP (%)": [62, 66, 73.2],
                "Region Proposal": ["Selective Search", "Selective Search", "RPN"],
                "End-to-End": ["âŒ", "ë¶€ë¶„", "âœ…"]
            }

            st.table(comparison_data)

            st.markdown("### Two-stage Detectorì˜ íŠ¹ì§•")

            col1, col2 = st.columns(2)

            with col1:
                st.success("""
                **ì¥ì **
                - ë†’ì€ ì •í™•ë„
                - ì•ˆì •ì  ì„±ëŠ¥
                - ì‘ì€ ê°ì²´ íƒì§€
                """)

            with col2:
                st.warning("""
                **ë‹¨ì **
                - ëŠë¦° ì†ë„
                - ë³µì¡í•œ êµ¬ì¡°
                - ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                """)

    def render_yolo(self):
        """YOLO ì•„í‚¤í…ì²˜ ì„¤ëª…"""
        st.header("âš¡ YOLO (You Only Look Once)")

        yolo_tabs = st.tabs(["YOLOv1", "YOLOv2/v3", "YOLOv4/v5", "YOLOv8"])

        with yolo_tabs[0]:
            st.subheader("YOLOv1 (2016): í˜ì‹ ì˜ ì‹œì‘")

            st.markdown("""
            ### í•µì‹¬ ê°œë…
            > "ê°ì²´ íƒì§€ë¥¼ íšŒê·€ ë¬¸ì œë¡œ!"

            - ì´ë¯¸ì§€ë¥¼ SÃ—S ê·¸ë¦¬ë“œë¡œ ë¶„í•  (S=7)
            - ê° ê·¸ë¦¬ë“œ ì…€ì´ Bê°œì˜ ë°”ìš´ë”© ë°•ìŠ¤ ì˜ˆì¸¡ (B=2)
            - í•œ ë²ˆì˜ forward passë¡œ ëª¨ë“  ê°ì²´ íƒì§€
            """)

            # YOLO ê·¸ë¦¬ë“œ ì‹œê°í™”
            st.markdown("#### ê·¸ë¦¬ë“œ ë¶„í•  ì‹œê°í™”")

            grid_size = st.slider("ê·¸ë¦¬ë“œ í¬ê¸° (SÃ—S)", 3, 13, 7)

            fig, ax = plt.subplots(figsize=(8, 8))

            # ê·¸ë¦¬ë“œ ê·¸ë¦¬ê¸°
            for i in range(grid_size + 1):
                ax.axhline(i, color='gray', linewidth=0.5)
                ax.axvline(i, color='gray', linewidth=0.5)

            ax.set_xlim(0, grid_size)
            ax.set_ylim(0, grid_size)
            ax.set_aspect('equal')
            ax.set_title(f'{grid_size}Ã—{grid_size} Grid', fontsize=14)
            ax.invert_yaxis()

            st.pyplot(fig)
            plt.close()

            st.markdown("""
            ### YOLO ì¶œë ¥ í…ì„œ
            - í¬ê¸°: S Ã— S Ã— (B Ã— 5 + C)
            - 7 Ã— 7 Ã— 30 (S=7, B=2, C=20)
            - ê° ë°•ìŠ¤: [x, y, w, h, confidence]
            - ê° ì…€: Cê°œì˜ í´ë˜ìŠ¤ í™•ë¥ 
            """)

        with yolo_tabs[1]:
            st.subheader("YOLOv2/v3: ì„±ëŠ¥ ê°œì„ ")

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("### YOLOv2 (2017)")
                st.info("""
                **ì£¼ìš” ê°œì„ :**
                - Batch Normalization
                - High Resolution (448Ã—448)
                - Anchor Boxes ë„ì…
                - K-means ì•µì»¤ í´ëŸ¬ìŠ¤í„°ë§
                """)

            with col2:
                st.markdown("### YOLOv3 (2018)")
                st.success("""
                **ì£¼ìš” ê°œì„ :**
                - ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ê²€ì¶œ (3ê°œ)
                - Darknet-53 ë°±ë³¸
                - Feature Pyramid Network
                - 9ê°œ ì•µì»¤ ë°•ìŠ¤
                """)

            st.markdown("#### ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ê²€ì¶œ")

            scales = {
                "13Ã—13": "í° ê°ì²´",
                "26Ã—26": "ì¤‘ê°„ ê°ì²´",
                "52Ã—52": "ì‘ì€ ê°ì²´"
            }

            cols = st.columns(3)
            for i, (scale, description) in enumerate(scales.items()):
                with cols[i]:
                    st.metric(scale, description)

        with yolo_tabs[2]:
            st.subheader("YOLOv4/v5: ìµœì í™”")

            st.markdown("### YOLOv4 (2020)")
            st.markdown("""
            **ì£¼ìš” ê¸°ìˆ :**
            1. **CSPDarknet53**: Cross Stage Partial ì—°ê²°
            2. **PANet**: Path Aggregation Network
            3. **Mosaic Augmentation**: 4ê°œ ì´ë¯¸ì§€ ì¡°í•©
            4. **CIoU Loss**: Complete IoU ì†ì‹¤
            """)

            st.markdown("### YOLOv5 (2020)")
            st.markdown("""
            **ì‹¤ìš©ì„± ê°•í™”:**
            - PyTorch êµ¬í˜„
            - AutoAnchor (ìë™ ì•µì»¤ ìµœì í™”)
            - Model Scaling (n, s, m, l, x)
            - ì‰¬ìš´ ì‚¬ìš©ì„±
            """)

            # YOLOv5 ëª¨ë¸ í¬ê¸° ë¹„êµ
            st.markdown("#### YOLOv5 ëª¨ë¸ ìŠ¤ì¼€ì¼")

            model_sizes = {
                "ëª¨ë¸": ["YOLOv5n", "YOLOv5s", "YOLOv5m", "YOLOv5l", "YOLOv5x"],
                "íŒŒë¼ë¯¸í„° (M)": [1.9, 7.2, 21.2, 46.5, 86.7],
                "FLOPs (G)": [4.5, 16.5, 49.0, 109.1, 205.7],
                "ì†ë„ (ms)": [6.3, 6.4, 8.2, 10.1, 12.1]
            }

            st.table(model_sizes)

        with yolo_tabs[3]:
            st.subheader("YOLOv8 (2023): ìµœì‹  ê¸°ìˆ ")

            st.markdown("""
            ### í˜ì‹ ì  ê°œì„ 
            1. **Anchor-Free**: ì•µì»¤ ë°•ìŠ¤ ì—†ì´ ì§ì ‘ ì˜ˆì¸¡
            2. **Decoupled Head**: ë¶„ë¥˜ì™€ íšŒê·€ í—¤ë“œ ë¶„ë¦¬
            3. **C2f ëª¨ë“ˆ**: ìƒˆë¡œìš´ ë°±ë³¸ êµ¬ì¡°
            4. **Advanced Augmentation**: MixUp, CutMix
            """)

            st.success("""
            **ì£¼ìš” íŠ¹ì§•:**
            - ë” ë¹ ë¥¸ ì†ë„
            - ë” ë†’ì€ ì •í™•ë„
            - ì‰¬ìš´ í•™ìŠµ ë° ë°°í¬
            - ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì§€ì› (Detection, Segmentation, Classification, Pose)
            """)

            st.code("""
from ultralytics import YOLO

# ëª¨ë¸ ë¡œë“œ
model = YOLO('yolov8n.pt')

# í•™ìŠµ
model.train(data='dataset.yaml', epochs=100)

# ì¶”ë¡ 
results = model.predict('image.jpg')
            """, language="python")

    def render_nms(self):
        """NMS ì„¤ëª… ë° ì‹œë®¬ë ˆì´ì…˜"""
        st.header("ğŸ¨ NMS (Non-Maximum Suppression)")

        st.markdown("""
        ### ğŸ¤” ë¬¸ì œ ìƒí™©

        **ì˜ˆì‹œ**: ì‚¬ì§„ ì† ê°•ì•„ì§€ë¥¼ íƒì§€í•  ë•Œ
        - AIê°€ ê°™ì€ ê°•ì•„ì§€ì—ê²Œ ë°•ìŠ¤ë¥¼ **10ê°œë‚˜ ê·¸ë¦¼** ğŸ“¦ğŸ“¦ğŸ“¦ğŸ“¦ğŸ“¦
        - ê²¹ì¹˜ëŠ” ë°•ìŠ¤ë“¤ì´ ë„ˆë¬´ ë§ì•„ì„œ ì§€ì €ë¶„í•¨

        ### ğŸ’¡ NMSê°€ í•˜ëŠ” ì¼

        **"ì¤‘ë³µëœ ë°•ìŠ¤ë¥¼ ì •ë¦¬í•´ì„œ ê°€ì¥ ì¢‹ì€ ë°•ìŠ¤ 1ê°œë§Œ ë‚¨ê¸´ë‹¤!"**

        **ë¹„ìœ **:
        - ì‹œí—˜ ë‹µì•ˆì§€ì— ê°™ì€ ë‹µì„ 10ë²ˆ ì¼ëŠ”ë°, ê°€ì¥ ì˜ ì“´ 1ê°œë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì§€ìš°ëŠ” ê²ƒ
        - ì¹œêµ¬ ì‚¬ì§„ì„ 10ì¥ ì°ì—ˆëŠ”ë°, ê°€ì¥ ì˜ ë‚˜ì˜¨ 1ì¥ë§Œ ê³¨ë¼ì„œ ì¸ìŠ¤íƒ€ì— ì˜¬ë¦¬ëŠ” ê²ƒ
        """)

        st.info("""
        ### ğŸ”¥ 2025ë…„ í˜„ì¬ë„ ì‚¬ìš©í•˜ë‚˜ìš”?

        **ë„¤, ì—¬ì „íˆ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤!** âœ…

        **ì™œ ê·œì¹™ ê¸°ë°˜ì¸ë°ë„ ì‚¬ìš©í•˜ëŠ”ê°€?**

        1ï¸âƒ£ **ë”¥ëŸ¬ë‹ì˜ í•œê³„**
        - AI ëª¨ë¸ì€ "ì˜ˆì¸¡"ë§Œ ë‹´ë‹¹ (ì—¬ëŸ¬ ë°•ìŠ¤ ì¶œë ¥)
        - **ì •ë¦¬ëŠ” ê·œì¹™**ì´ ë” ë¹ ë¥´ê³  ì •í™•í•¨

        2ï¸âƒ£ **ì‹¤ì „ì—ì„œ ì—¬ì „íˆ ì‚¬ìš© ì¤‘**
        - âœ… YOLOv8, YOLOv9, YOLOv10 (2024)
        - âœ… DETR ê³„ì—´ (Facebook AI)
        - âœ… Detectron2 (Meta)
        - âœ… MMDetection (OpenMMLab)

        3ï¸âƒ£ **ì†ë„ê°€ ì¤‘ìš”**
        - NMS: 0.001ì´ˆ (ì´ˆê³ ì†) âš¡
        - ë”¥ëŸ¬ë‹ í›„ì²˜ë¦¬: 0.1ì´ˆ (100ë°° ëŠë¦¼) ğŸ¢

        **ë¹„ìœ **: ê³„ì‚°ê¸° vs AI
        - ë§ì…ˆ/ëº„ì…ˆ: ê³„ì‚°ê¸°(ê·œì¹™)ê°€ ë” ë¹ ë¥´ê³  ì •í™•
        - ì–¼êµ´ ì¸ì‹: AIê°€ í•„ìš”
        â†’ **ì ì¬ì ì†Œ!**
        """)

        st.markdown("""
        ### ğŸ“ 3ë‹¨ê³„ ì•Œê³ ë¦¬ì¦˜

        1ï¸âƒ£ **ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬**: ì‹ ë¢°ë„ê°€ ë†’ì€ ë°•ìŠ¤ë¶€í„° ë‚˜ì—´
        2ï¸âƒ£ **1ë“± ë°•ìŠ¤ ì„ íƒ**: ê°€ì¥ í™•ì‹ í•˜ëŠ” ë°•ìŠ¤ë¥¼ ë¨¼ì € ì„ íƒ
        3ï¸âƒ£ **ë¹„ìŠ·í•œ ë°•ìŠ¤ ì œê±°**: 1ë“±ê³¼ ë„ˆë¬´ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ë“¤ì€ ì‚­ì œ
        4ï¸âƒ£ **ë°˜ë³µ**: ë‚¨ì€ ë°•ìŠ¤ë“¤ ì¤‘ì—ì„œ ë‹¤ì‹œ 1~3 ë°˜ë³µ
        """)

        nms_tabs = st.tabs(["ê¸°ë³¸ NMS", "Soft NMS", "DIoU NMS"])

        with nms_tabs[0]:
            st.subheader("ê¸°ë³¸ NMS ì•Œê³ ë¦¬ì¦˜")

            st.info("""
            **ğŸ’¡ ì‹¤ìƒí™œ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°**

            **ìƒí™©**: 10ëª…ì´ ê°™ì€ ë¬¸ì œì˜ ë‹µì„ ì œì¶œí–ˆì–´ìš”
            - ì ìˆ˜: 95ì , 92ì , 90ì , 88ì , 85ì ... (ëª¨ë‘ ë¹„ìŠ·í•œ ë‹µ)

            **NMS ê³¼ì •**:
            1. ê°€ì¥ ë†’ì€ ì ìˆ˜(95ì ) ì„ íƒ âœ…
            2. 95ì  ë‹µê³¼ ë„ˆë¬´ ë¹„ìŠ·í•œ ë‹µë“¤(92, 90, 88ì ) ëª¨ë‘ ì‚­ì œ âŒ
            3. ë‚¨ì€ ë‹µ ì¤‘ì—ì„œ ë‹¤ì‹œ 1ë²ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°

            **ê²°ê³¼**: ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ì˜ ê°ì²´ë§Œ ë‚¨ìŒ!
            """)

            st.code("""
def non_max_suppression(detections, iou_threshold=0.5):
    # 1ï¸âƒ£ ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (1ë“±ë¶€í„° ê¼´ë“±ê¹Œì§€)
    detections.sort(key=lambda x: x['confidence'], reverse=True)

    keep = []  # ìµœì¢… ê²°ê³¼ ì €ì¥

    while detections:  # ë°•ìŠ¤ê°€ ë‚¨ì•„ìˆëŠ” ë™ì•ˆ ë°˜ë³µ
        # 2ï¸âƒ£ 1ë“± ë°•ìŠ¤ ì„ íƒ
        best = detections.pop(0)  # ê°€ì¥ ì•ì— ìˆëŠ” = ì ìˆ˜ 1ë“±
        keep.append(best)         # ìµœì¢… ê²°ê³¼ì— ì €ì¥

        # 3ï¸âƒ£ 1ë“±ê³¼ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ë“¤ ì œê±°
        remaining = []
        for det in detections:
            iou = calculate_iou(best['bbox'], det['bbox'])

            # IoUê°€ ë‚®ìœ¼ë©´ = ë‹¤ë¥¸ ê°ì²´ â†’ ë‚¨ê¹€
            if iou <= iou_threshold:
                remaining.append(det)
            # IoUê°€ ë†’ìœ¼ë©´ = ê°™ì€ ê°ì²´ â†’ ë²„ë¦¼

        detections = remaining  # ë‚¨ì€ ë°•ìŠ¤ë¡œ ì—…ë°ì´íŠ¸

    return keep  # ì¤‘ë³µ ì œê±°ëœ ìµœì¢… ë°•ìŠ¤ë“¤
            """, language="python")

            st.markdown("""
            **ğŸ” IoU ì„ê³„ê°’ì´ë€?**
            - **0.5**: ë°•ìŠ¤ê°€ 50% ì´ìƒ ê²¹ì¹˜ë©´ "ê°™ì€ ê°ì²´"ë¡œ íŒë‹¨ â†’ ì‚­ì œ
            - **ë†’ì„ìˆ˜ë¡** (0.7, 0.9): ë” ë§ì´ ê²¹ì³ì•¼ ì‚­ì œ â†’ ë°•ìŠ¤ê°€ ë” ë§ì´ ë‚¨ìŒ
            - **ë‚®ì„ìˆ˜ë¡** (0.3): ì¡°ê¸ˆë§Œ ê²¹ì³ë„ ì‚­ì œ â†’ ë°•ìŠ¤ê°€ ì ê²Œ ë‚¨ìŒ
            """)

            st.markdown("#### NMS ì‹œë®¬ë ˆì´ì…˜")

            iou_threshold = st.slider("IoU ì„ê³„ê°’", 0.0, 1.0, 0.5, 0.05)

            # ìƒ˜í”Œ ê²€ì¶œ ê²°ê³¼ ìƒì„±
            detections = [
                {"bbox": [100, 100, 200, 200], "confidence": 0.9},
                {"bbox": [105, 95, 205, 195], "confidence": 0.85},
                {"bbox": [98, 102, 198, 202], "confidence": 0.8},
                {"bbox": [300, 150, 400, 250], "confidence": 0.95},
            ]

            st.write(f"ì›ë³¸ ê²€ì¶œ ê°œìˆ˜: {len(detections)}")

            # NMS ì ìš©
            filtered = self.apply_nms(detections, iou_threshold)
            st.write(f"NMS í›„ ê²€ì¶œ ê°œìˆ˜: {len(filtered)}")

            # ì‹œê°í™”
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

            # Before NMS
            ax1.set_xlim(0, 500)
            ax1.set_ylim(0, 300)
            ax1.set_title('Before NMS')
            for det in detections:
                x1, y1, x2, y2 = det['bbox']
                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,
                                    linewidth=2, edgecolor='red', facecolor='none',
                                    label=f"conf={det['confidence']:.2f}")
                ax1.add_patch(rect)
            ax1.invert_yaxis()

            # After NMS
            ax2.set_xlim(0, 500)
            ax2.set_ylim(0, 300)
            ax2.set_title('After NMS')
            for det in filtered:
                x1, y1, x2, y2 = det['bbox']
                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,
                                    linewidth=2, edgecolor='green', facecolor='none',
                                    label=f"conf={det['confidence']:.2f}")
                ax2.add_patch(rect)
            ax2.invert_yaxis()

            st.pyplot(fig)
            plt.close()

        with nms_tabs[1]:
            st.subheader("Soft NMS")

            st.markdown("""
            ### ğŸ˜± ê¸°ë³¸ NMSì˜ ë¬¸ì œì 

            **ìƒí™©**: ì‚¬ëŒë“¤ì´ ì˜¹ê¸°ì¢…ê¸° ëª¨ì—¬ìˆëŠ” ì‚¬ì§„
            - ì‚¬ëŒ Aì™€ ì‚¬ëŒ Bê°€ ê²¹ì³ìˆìŒ
            - ê¸°ë³¸ NMS: "ê²¹ì¹˜ë‹ˆê¹Œ BëŠ” ì‚­ì œ!" â†’ **ì‹¤ì œë¡œ ìˆëŠ” ì‚¬ëŒë„ ì‚­ì œë¨** âŒ

            ### ğŸ’¡ Soft NMSì˜ ë˜‘ë˜‘í•œ í•´ê²°ì±…

            **"ë°”ë¡œ ì‚­ì œí•˜ì§€ ë§ê³ , ì ìˆ˜ë§Œ ë‚®ì¶°ì£¼ì!"**

            **ë¹„ìœ **:
            - ê¸°ë³¸ NMS: "íƒˆë½!" (0ì  ì²˜ë¦¬)
            - Soft NMS: "ìŒ... ì¢€ ì• ë§¤í•˜ë‹ˆê¹Œ ê°ì !" (90ì  â†’ 60ì ìœ¼ë¡œ ë‚®ì¶¤)

            **ì¥ì **:
            - ì‹¤ì œë¡œ ë‹¤ë¥¸ ê°ì²´ì¸ë° ê²¹ì¹œ ê²½ìš° â†’ ì ìˆ˜ëŠ” ë‚®ì§€ë§Œ ì‚´ì•„ë‚¨ìŒ âœ…
            - ë‚˜ì¤‘ì— ì ìˆ˜ ìˆœìœ¼ë¡œ ë‹¤ì‹œ ì„ íƒ ê°€ëŠ¥
            """)

            st.code("""
# ê¸°ë³¸ NMS
if iou > 0.5:
    ë°•ìŠ¤.ì‚­ì œ()  # ë¬´ì¡°ê±´ 0ì  ì²˜ë¦¬ â†’ ì˜ì˜ ì‚¬ë¼ì§ ğŸ’€

# Soft NMS
if iou > 0.5:
    ë°•ìŠ¤.ì ìˆ˜ = ë°•ìŠ¤.ì ìˆ˜ Ã— (1 - iou)  # ì ìˆ˜ë§Œ ê¹ìŒ ğŸ“‰
    # ì˜ˆ: 0.9ì  â†’ 0.9 Ã— (1 - 0.6) = 0.36ì 
    #     ì™„ì „íˆ ì‚¬ë¼ì§€ì§„ ì•ŠìŒ!
""")

            st.code("""
def soft_nms(detections, sigma=0.5):
    for i in range(len(detections)):
        for j in range(i + 1, len(detections)):
            iou = calculate_iou(detections[i]['bbox'],
                              detections[j]['bbox'])

            if iou > threshold:
                # ê°€ìš°ì‹œì•ˆ ê°€ì¤‘ì¹˜ ì ìš©
                weight = np.exp(-(iou ** 2) / sigma)
                detections[j]['confidence'] *= weight

    return detections
            """, language="python")

        with nms_tabs[2]:
            st.subheader("DIoU NMS (ê³ ê¸‰)")

            st.markdown("""
            ### ğŸ¯ DIoU = Distance-IoU (ê±°ë¦¬ë¥¼ ê³ ë ¤í•œ IoU)

            **ë¬¸ì œ ìƒí™©**:
            - ë‘ ë°•ìŠ¤ê°€ ê²¹ì¹˜ëŠ” ì •ë„ëŠ” ê°™ì€ë°
            - í•˜ë‚˜ëŠ” ë°”ë¡œ ì˜†ì— ë¶™ì–´ìˆê³ , í•˜ë‚˜ëŠ” ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìŒ
            - ê¸°ë³¸ IoU: ë‘˜ ë‹¤ ë˜‘ê°™ì´ íŒë‹¨ ğŸ˜•

            ### ğŸ’¡ DIoUì˜ ê°œì„ 

            **"ê²¹ì¹˜ëŠ” ì •ë„ + ì¤‘ì‹¬ì  ê°„ ê±°ë¦¬ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì!"**

            **ë¹„ìœ **:
            - ê¸°ë³¸ IoU: ë‘ ì‚¬ëŒì´ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ê°€ë§Œ ë´„
            - DIoU: ê²¹ì¹˜ëŠ” ì •ë„ + ë‘ ì‚¬ëŒ ì‚¬ì´ ê±°ë¦¬ë„ ë´„

            **ì‹¤ì œ íš¨ê³¼**:
            - ì¤‘ì‹¬ì ì´ ê°€ê¹Œìš°ë©´ â†’ ê°™ì€ ê°ì²´ì¼ í™•ë¥  ë†’ìŒ
            - ì¤‘ì‹¬ì ì´ ë©€ë©´ â†’ ë‹¤ë¥¸ ê°ì²´ì¼ í™•ë¥  ë†’ìŒ
            """)

            st.code("""
# ê¸°ë³¸ IoU
ê²¹ì¹˜ëŠ”_ë©´ì  / í•©ì¹œ_ë©´ì   # ê±°ë¦¬ëŠ” ë¬´ì‹œ

# DIoU
ê¸°ë³¸_IoU - (ì¤‘ì‹¬ì _ê±°ë¦¬Â² / ëŒ€ê°ì„ _ê±°ë¦¬Â²)
           â†‘ ì´ ê°’ì´ í´ìˆ˜ë¡ íŒ¨ë„í‹°

ì˜ˆì‹œ:
ë°•ìŠ¤A, ë°•ìŠ¤B: IoU = 0.6, ì¤‘ì‹¬ì  ê±°ë¦¬ = 10í”½ì…€  â†’ DIoU ë†’ìŒ âœ…
ë°•ìŠ¤A, ë°•ìŠ¤C: IoU = 0.6, ì¤‘ì‹¬ì  ê±°ë¦¬ = 100í”½ì…€ â†’ DIoU ë‚®ìŒ âœ…
â†’ Aì™€ CëŠ” ë‹¤ë¥¸ ê°ì²´ë¡œ íŒë‹¨!
""", language="python")

            st.info("""
            **ğŸ“ ìš”ì•½**
            - ê¸°ë³¸ NMS: ë‹¨ìˆœí•˜ê³  ë¹ ë¦„
            - Soft NMS: ê²¹ì¹œ ê°ì²´ë„ ì‚´ë¦¼ (ì‚¬ëŒ ì—¬ëŸ¬ëª…)
            - DIoU NMS: ê±°ë¦¬ê¹Œì§€ ê³ ë ¤ (ë” ì •í™•)
            """)

    def render_projects(self):
        """ì‹¤ì „ í”„ë¡œì íŠ¸"""
        st.header("ğŸ’» ì‹¤ì „ í”„ë¡œì íŠ¸")

        project_tabs = st.tabs([
            "êµì‹¤ ë¬¼ê±´ íƒì§€",
            "ì–¼êµ´ ê°ì§€",
            "ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì¸ì‹",
            "ì†ë™ì‘ ì¸ì‹"
        ])

        with project_tabs[0]:
            self.classroom_detector_project()

        with project_tabs[1]:
            self.face_detection_project()

        with project_tabs[2]:
            self.license_plate_project()

        with project_tabs[3]:
            self.hand_gesture_project()

    def classroom_detector_project(self):
        """êµì‹¤ ë¬¼ê±´ íƒì§€ í”„ë¡œì íŠ¸ - ì‹¤ì œ YOLOv8 ëª¨ë¸ ì‚¬ìš©"""
        st.subheader("ğŸ« êµì‹¤ ë¬¼ê±´ íƒì§€ê¸°")

        with st.expander("ğŸ“š ì´ë¡ ì  ë°°ê²½: YOLOv8ê³¼ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€", expanded=False):
            st.markdown("""
            ### YOLOv8 ì•„í‚¤í…ì²˜

            **YOLOv8**ì€ Ultralyticsê°€ 2023ë…„ ì¶œì‹œí•œ ìµœì‹  YOLO ì‹œë¦¬ì¦ˆì…ë‹ˆë‹¤.

            #### í•µì‹¬ ê°œì„  ì‚¬í•­
            1. **Anchor-free ì„¤ê³„**
               - ê¸°ì¡´ YOLOì˜ ì•µì»¤ ë°•ìŠ¤ ì œê±°
               - ê°ì²´ ì¤‘ì‹¬ì ì„ ì§ì ‘ ì˜ˆì¸¡
               - ë” ë¹ ë¥´ê³  ì •í™•í•œ íƒì§€

            2. **CSPNet + C2f ëª¨ë“ˆ**
               - Cross Stage Partial Networksë¡œ íš¨ìœ¨ì  íŠ¹ì§• ì¶”ì¶œ
               - C2f (Coarse-to-Fine) ëª¨ë“ˆë¡œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìœµí•©

            3. **Task-aligned Head**
               - ë¶„ë¥˜ì™€ ìœ„ì¹˜ ì˜ˆì¸¡ì„ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”
               - TaskAlignedAssignerë¡œ ë” ì •í™•í•œ íƒ€ê²Ÿ í• ë‹¹

            #### COCO ë°ì´í„°ì…‹
            - **80ê°œ í´ë˜ìŠ¤**: ì¼ìƒì  ê°ì²´ (ì‚¬ëŒ, ë™ë¬¼, êµí†µìˆ˜ë‹¨, ê°€êµ¬ ë“±)
            - **330K ì´ë¯¸ì§€**: ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„°
            - **êµì‹¤ ê´€ë ¨ í´ë˜ìŠ¤**: book, laptop, chair, backpack, person, cell phone, cup ë“±

            #### ì‹¤ì‹œê°„ íƒì§€ í”„ë¡œì„¸ìŠ¤
            ```
            ì…ë ¥ ì´ë¯¸ì§€ â†’ ì „ì²˜ë¦¬ (640Ã—640) â†’ YOLOv8 ëª¨ë¸
                â†“
            íŠ¹ì§• ì¶”ì¶œ (Backbone) â†’ íŠ¹ì§• ìœµí•© (Neck)
                â†“
            íƒì§€ í—¤ë“œ â†’ [Bounding Box + Class + Confidence]
                â†“
            NMS ì ìš© â†’ ìµœì¢… íƒì§€ ê²°ê³¼
            ```

            #### YOLOv8 ëª¨ë¸ ë³€í˜•
            - **YOLOv8n (Nano)**: 3.2M íŒŒë¼ë¯¸í„° - ì‹¤ì‹œê°„ ì²˜ë¦¬ (ì‚¬ìš© ì¤‘)
            - **YOLOv8s (Small)**: 11.2M íŒŒë¼ë¯¸í„° - ê· í˜•ì¡íŒ ì„±ëŠ¥
            - **YOLOv8m (Medium)**: 25.9M íŒŒë¼ë¯¸í„° - ê³ ì •í™•ë„
            - **YOLOv8l/x**: ëŒ€ê·œëª¨ ëª¨ë¸ - ìµœê³  ì •í™•ë„
            """)

        st.info("""
        ğŸ’¡ **ì‹¤ì œ YOLOv8 ëª¨ë¸ ì‚¬ìš©**: Ultralyticsì˜ ì‚¬ì „í•™ìŠµëœ YOLOv8 ëª¨ë¸ë¡œ ê°ì²´ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
        - ëª¨ë¸: `yolov8n.pt` (COCO ë°ì´í„°ì…‹ í•™ìŠµ)
        - 80ê°œ í´ë˜ìŠ¤ íƒì§€ ê°€ëŠ¥ (ì‚¬ëŒ, ì±…, ë…¸íŠ¸ë¶, ì˜ì, ê°€ë°© ë“±)
        """)

        # Step 1: ëª¨ë¸ ì¤€ë¹„
        st.markdown("### 1ï¸âƒ£ ëª¨ë¸ ì¤€ë¹„")
        model_path = "yolov8n.pt"
        if not os.path.exists(model_path):
            st.warning(f"'{model_path}' ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
            if st.button(f"â¬‡ï¸ '{model_path}' ë‹¤ìš´ë¡œë“œ"):
                if self._ensure_yolo_model(model_path):
                    st.rerun()
        else:
            st.success(f"âœ… '{model_path}' ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.caption(f"ìœ„ì¹˜: {os.path.abspath(model_path)}")

        st.markdown("---")
        st.markdown("### 2ï¸âƒ£ ê°ì²´ íƒì§€")

        # ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ ì´ë¯¸ì§€ ì—…ë¡œë“œ í™œì„±í™”
        if os.path.exists(model_path):
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("""
                **í”„ë¡œì íŠ¸ ëª©í‘œ:**
                - YOLOv8ë¡œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€
                - COCO ë°ì´í„°ì…‹ 80ê°œ í´ë˜ìŠ¤ ì¸ì‹
                - ë°”ìš´ë”© ë°•ìŠ¤ + ì‹ ë¢°ë„ í‘œì‹œ

                **íƒì§€ ê°€ëŠ¥í•œ ë¬¼ê±´ (COCO í´ë˜ìŠ¤):**
                - ğŸ“š ì±… (book)
                - ğŸ’» ë…¸íŠ¸ë¶ (laptop)
                - ğŸª‘ ì˜ì (chair)
                - ğŸ’ ê°€ë°© (backpack)
                - ğŸ‘¤ ì‚¬ëŒ (person)
                - ğŸ“± íœ´ëŒ€í° (cell phone)
                - â˜• ì»µ (cup)
                """)

                uploaded_file = st.file_uploader(
                    "ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                    type=['png', 'jpg', 'jpeg'],
                    key="classroom_upload"
                )

            with col2:
                if uploaded_file:
                    image = Image.open(uploaded_file)
                    st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_container_width=True)

                    if st.button("ğŸ¯ YOLOv8ìœ¼ë¡œ ê°ì²´ íƒì§€", key="classroom_detect", type="primary"):
                        if not os.path.exists(model_path):
                            st.error("ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                            return

                        with st.spinner("YOLOv8 ëª¨ë¸ ë¡œë”© ë° ê°ì²´ íƒì§€ ì¤‘..."):
                            try:
                                from ultralytics import YOLO
                                import matplotlib.pyplot as plt
                                import matplotlib.patches as patches
                                import matplotlib.cm as cm

                                model = YOLO(model_path)

                                # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
                                image_array = np.array(image)

                                # ê°ì²´ íƒì§€ ì‹¤í–‰
                                results = model.predict(
                                    source=image_array,
                                    conf=0.25,  # ì‹ ë¢°ë„ ì„ê³„ê°’
                                    iou=0.45,   # NMS IoU ì„ê³„ê°’
                                    verbose=False
                                )[0]

                                st.success("âœ… íƒì§€ ì™„ë£Œ!")

                                # íƒì§€ ê²°ê³¼ í†µê³„
                                if results.boxes is not None and len(results.boxes) > 0:
                                    st.markdown(f"### ğŸ“Š íƒì§€ëœ ê°ì²´: {len(results.boxes)}ê°œ")

                                    # ê²°ê³¼ ì‹œê°í™”
                                    fig, ax = plt.subplots(figsize=(12, 8))
                                    ax.imshow(image_array)

                                    boxes = results.boxes.xyxy.cpu().numpy()
                                    confidences = results.boxes.conf.cpu().numpy()
                                    class_ids = results.boxes.cls.cpu().numpy().astype(int)
                                    class_names = results.names

                                    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
                                    cmap = cm.get_cmap('tab20')

                                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                                    for box, conf, class_id in zip(boxes, confidences, class_ids):
                                        x1, y1, x2, y2 = box
                                        class_name = class_names[class_id]
                                        color = cmap(class_id % 20 / 20)

                                        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                                        rect = patches.Rectangle(
                                            (x1, y1), x2 - x1, y2 - y1,
                                            linewidth=2, edgecolor=color, facecolor='none'
                                        )
                                        ax.add_patch(rect)

                                        # ë ˆì´ë¸” ê·¸ë¦¬ê¸°
                                        label = f"{class_name}: {conf:.2f}"
                                        ax.text(
                                            x1, y1 - 5, label,
                                            color='white',
                                            fontsize=10,
                                            bbox=dict(facecolor=color, alpha=0.8, edgecolor='none', pad=2)
                                        )

                                    ax.axis('off')
                                    st.pyplot(fig)
                                    plt.close()

                                    # íƒì§€ ê²°ê³¼ ìƒì„¸ ì •ë³´
                                    st.markdown("### ğŸ” íƒì§€ ê²°ê³¼ ìƒì„¸")

                                    # í´ë˜ìŠ¤ë³„ ê·¸ë£¹í™”
                                    class_counts = {}
                                    for class_id in class_ids:
                                        class_name = class_names[class_id]
                                        class_counts[class_name] = class_counts.get(class_name, 0) + 1

                                    col_a, col_b = st.columns(2)

                                    with col_a:
                                        st.markdown("#### í´ë˜ìŠ¤ë³„ ê°œìˆ˜")
                                        for class_name, count in sorted(class_counts.items()):
                                            st.metric(class_name, count)

                                    with col_b:
                                        st.markdown("#### ê°œë³„ ê°ì²´ ì •ë³´")
                                        for i, (box, conf, class_id) in enumerate(zip(boxes, confidences, class_ids)):
                                            x1, y1, x2, y2 = box
                                            class_name = class_names[class_id]
                                            st.text(f"{i+1}. {class_name} - ì‹ ë¢°ë„: {conf:.2%}")
                                            st.caption(f"   ìœ„ì¹˜: [{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]")

                                else:
                                    st.warning("âš ï¸ íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")

                                # ëª¨ë¸ ì •ë³´
                                with st.expander("ğŸ“Š YOLOv8 ëª¨ë¸ ì •ë³´"):
                                    st.markdown("""
                                    **ëª¨ë¸**: YOLOv8n (Nano)
                                    - **íŒŒë¼ë¯¸í„°**: 3.2M
                                    - **í•™ìŠµ ë°ì´í„°**: COCO ë°ì´í„°ì…‹ (80 í´ë˜ìŠ¤)
                                    - **ì…ë ¥ í¬ê¸°**: 640Ã—640
                                    - **ì†ë„**: ~100 FPS (GPU)
                                    - **mAP50-95**: 37.3%

                                    **COCO 80 í´ë˜ìŠ¤**:
                                    - ì‚¬ëŒ, ìì „ê±°, ìë™ì°¨, ì˜¤í† ë°”ì´, ë¹„í–‰ê¸°, ë²„ìŠ¤, ê¸°ì°¨, íŠ¸ëŸ­, ë³´íŠ¸
                                    - ì˜ì, ì†ŒíŒŒ, ì¹¨ëŒ€, ì‹íƒ, í™”ì¥ì‹¤, TV, ë…¸íŠ¸ë¶, ë§ˆìš°ìŠ¤, í‚¤ë³´ë“œ
                                    - í•¸ë“œí°, ì±…, ì‹œê³„, ê½ƒë³‘, ê°€ìœ„, ê³° ì¸í˜•, ì¹«ì†” ë“±
                                    """)

                            except Exception as e:
                                st.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
                                st.info("""
                                **í•´ê²° ë°©ë²•:**
                                1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”)
                                2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: `pip install ultralytics`
                                3. ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
                                """)
        else:
            st.info("â¬†ï¸ ë¨¼ì € ìœ„ì˜ '1ï¸âƒ£ ëª¨ë¸ ì¤€ë¹„' ì„¹ì…˜ì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")

        st.markdown("### í•™ìŠµ ì½”ë“œ")
        st.code("""
from ultralytics import YOLO

# ëª¨ë¸ í•™ìŠµ
model = YOLO('yolov8n.pt')

results = model.train(
    data='classroom.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    project='classroom_detector',
    name='yolov8n_classroom'
)

# ì¶”ë¡ 
model = YOLO('best.pt')
results = model.predict('classroom.jpg')
        """, language="python")

    def face_detection_project(self):
        """ì–¼êµ´ ê°ì§€ í”„ë¡œì íŠ¸"""
        st.subheader("ğŸ˜Š ì–¼êµ´ ê°ì§€ ì‹œìŠ¤í…œ")

        # ì´ë¡ ì  ë°°ê²½ ì¶”ê°€
        with st.expander("ğŸ“š ì´ë¡ ì  ë°°ê²½: ì–¼êµ´ ê°ì§€ ê¸°ìˆ ", expanded=False):
            st.markdown("""
            ### ì–¼êµ´ ê°ì§€ (Face Detection)

            ì–¼êµ´ ê°ì§€ëŠ” ì´ë¯¸ì§€ ë‚´ì—ì„œ ì‚¬ëŒì˜ ì–¼êµ´ ì˜ì—­ì„ ì°¾ì•„ë‚´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

            #### ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ë°œì „ ê³¼ì •

            **1. Viola-Jones (2001)**
            - **Haar Cascade**: ê°„ë‹¨í•œ íŠ¹ì§•ìœ¼ë¡œ ë¹ ë¥¸ íƒì§€
            - **Integral Image**: íš¨ìœ¨ì ì¸ íŠ¹ì§• ê³„ì‚°
            - **AdaBoost**: ì•½í•œ ë¶„ë¥˜ê¸° ì¡°í•©
            - ì¥ì : ë¹ ë¥¸ ì†ë„, ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥
            - ë‹¨ì : ì •ë©´ ì–¼êµ´ë§Œ ì˜ ê°ì§€, ì¡°ëª…ì— ë¯¼ê°

            **2. HOG + SVM (2005)**
            - **HOG (Histogram of Oriented Gradients)**: ì–¼êµ´ ìœ¤ê³½ íŠ¹ì§• ì¶”ì¶œ
            - **SVM (Support Vector Machine)**: ë¶„ë¥˜
            - ì¥ì : ë‹¤ì–‘í•œ ê°ë„ ì–¼êµ´ ê°ì§€
            - ë‹¨ì : Haarë³´ë‹¤ ëŠë¦¼

            **3. MTCNN (2016)**
            - **Multi-task CNN**: 3ë‹¨ê³„ CNN ë„¤íŠ¸ì›Œí¬
            - **P-Net â†’ R-Net â†’ O-Net**: ì ì§„ì  ì •ì œ
            - **ì–¼êµ´ ëœë“œë§ˆí¬ ë™ì‹œ ì˜ˆì¸¡**: ëˆˆ, ì½”, ì… ì¢Œí‘œ
            - ì¥ì : ë†’ì€ ì •í™•ë„, ë‹¤ì–‘í•œ í¬ì¦ˆ/í¬ê¸° ê°ì§€
            - ë‹¨ì : 3ë‹¨ê³„ ì²˜ë¦¬ë¡œ ì†ë„ ì €í•˜

            **4. RetinaFace (2020)**
            - **Single-stage Detector**: YOLO ìŠ¤íƒ€ì¼ì˜ ë¹ ë¥¸ íƒì§€
            - **Multi-task Learning**:
              - ì–¼êµ´ ë°•ìŠ¤ ì˜ˆì¸¡
              - 5ê°œ ëœë“œë§ˆí¬ (ì–‘ ëˆˆ, ì½”, ì–‘ìª½ ì…ê¼¬ë¦¬)
              - 3D ì–¼êµ´ ì •ë³´
            - **Feature Pyramid**: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ
            - ì¥ì : ì†ë„ì™€ ì •í™•ë„ ê· í˜•

            #### ì–¼êµ´ ëœë“œë§ˆí¬ (Facial Landmarks)

            ì–¼êµ´ ë‚´ ì£¼ìš” ì§€ì ì„ ì°¾ì•„ ì¢Œí‘œë¡œ í‘œí˜„:
            - **68 Points (dlib)**: ì–¼êµ´ ìœ¤ê³½, ëˆˆì¹, ëˆˆ, ì½”, ì…
            - **5 Points (RetinaFace)**: ì–‘ ëˆˆ, ì½” ë, ì–‘ìª½ ì…ê¼¬ë¦¬
            - **106/478 Points**: ë” ì •ë°€í•œ 3D ì–¼êµ´ ëª¨ë¸ë§

            **í™œìš© ë¶„ì•¼:**
            - ì–¼êµ´ ì •ë ¬ (Face Alignment)
            - ì–¼êµ´ ì¸ì‹ ì „ì²˜ë¦¬
            - í‘œì • ë¶„ì„
            - AR í•„í„°/ë§ˆìŠ¤í¬ ì ìš©

            #### ë‚˜ì´/ì„±ë³„ ì¶”ì •

            ì–¼êµ´ ê°ì§€ í›„ ì¶”ê°€ CNNìœ¼ë¡œ ì¶”ì •:
            - **ë‚˜ì´ ì¶”ì •**: íšŒê·€ ë¬¸ì œ (0-100ì„¸)
            - **ì„±ë³„ ì¶”ì •**: ì´ì§„ ë¶„ë¥˜ (ë‚¨/ì—¬)
            - **ëª¨ë¸**: AgeNet, GenderNet (Caffe ê¸°ë°˜)

            #### ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì§€ íŒŒì´í”„ë¼ì¸
            ```
            ì…ë ¥ ì´ë¯¸ì§€/ì˜ìƒ â†’ ì–¼êµ´ ê°ì§€ (RetinaFace/MTCNN)
                â†“
            Bounding Box + Confidence
                â†“
            ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ (5 or 68 points)
                â†“
            [ì„ íƒ] ë‚˜ì´/ì„±ë³„ ì¶”ì • CNN
                â†“
            ì‹œê°í™” + ê²°ê³¼ ì¶œë ¥
            ```

            #### MediaPipe ëŒ€ì•ˆ (2025)

            **ë” ì •í™•í•œ ì–¼êµ´ íƒì§€/ë¶„ì„ì„ ì›í•œë‹¤ë©´:**

            - **YOLO Face**: YOLOv8 ê¸°ë°˜, ë‹¤ì¤‘ ì–¼êµ´ ê³ ì† íƒì§€
            - **RetinaFace**: 5ê°œ ëœë“œë§ˆí¬ + 3D ì •ë³´, SOTA ì„±ëŠ¥
            - **SCRFD**: ê²½ëŸ‰ ì‹¤ì‹œê°„ ì–¼êµ´ íƒì§€ (MMDetection)
            - **Face Mesh (MediaPipe)**: 468ê°œ ìƒì„¸ ëœë“œë§ˆí¬

            **API vs ë¡œì»¬ ëª¨ë¸**:
            - Gemini API: ë‚˜ì´/ê°ì •/í‘œì • ìì—°ì–´ ë¶„ì„
            - MediaPipe: ë¹ ë¥¸ ì‹¤ì‹œê°„ íƒì§€, ì •í™•í•œ ì¢Œí‘œ
            """)

        st.markdown("""
        ### í”„ë¡œì íŠ¸ ê°œìš”

        ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ì—ì„œ ì‚¬ëŒì˜ ì–¼êµ´ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.

        **ê¸°ëŠ¥:**
        - ë‹¤ì¤‘ ì–¼êµ´ ê°ì§€
        - ì–¼êµ´ ëœë“œë§ˆí¬ (ëˆˆ, ì½”, ì…)
        - ë‚˜ì´/ì„±ë³„ ì¶”ì • (ì„ íƒ)
        """)

        # ì½”ë“œ ì˜ˆì‹œ - MediaPipe
        with st.expander("ğŸ’» MediaPipe ì–¼êµ´ ê°ì§€ ì½”ë“œ", expanded=False):
            st.code("""
import mediapipe as mp
import cv2
import numpy as np
from PIL import Image

# MediaPipe Face Detection ì´ˆê¸°í™”
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils

# Face Detection ëª¨ë¸ (full-range: 5m)
face_detection = mp_face_detection.FaceDetection(
    model_selection=1,
    min_detection_confidence=0.5
)

# ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open('your_image.jpg').convert('RGB')
image_np = np.array(image)

# ì–¼êµ´ íƒì§€
results = face_detection.process(image_np)

# ê²°ê³¼ ì‹œê°í™”
if results.detections:
    for detection in results.detections:
        mp_drawing.draw_detection(image_np, detection)

        # ë°”ìš´ë”© ë°•ìŠ¤ ë° ì‹ ë¢°ë„
        bbox = detection.location_data.relative_bounding_box
        confidence = detection.score[0]
        print(f"ì–¼êµ´ íƒì§€ ì‹ ë¢°ë„: {confidence:.2%}")

face_detection.close()
""", language="python")

        # ì½”ë“œ ì˜ˆì‹œ - Gemini API
        with st.expander("ğŸ’» Gemini API ì–¼êµ´ ë¶„ì„ ì½”ë“œ", expanded=False):
            st.code("""
import google.generativeai as genai
from PIL import Image
import os

# API í‚¤ ì„¤ì •
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))

# Gemini 2.0 Flash ëª¨ë¸ ì‚¬ìš©
model = genai.GenerativeModel('gemini-2.0-flash-exp')

# ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open('your_image.jpg')

# ì–¼êµ´ ë¶„ì„ í”„ë¡¬í”„íŠ¸
prompt = \"\"\"
ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  ì–¼êµ´ì„ ê°ì§€í•˜ê³  ê° ì–¼êµ´ì— ëŒ€í•´:
1. ìœ„ì¹˜ (ì™¼ìª½/ì¤‘ì•™/ì˜¤ë¥¸ìª½, ìœ„/ì¤‘ê°„/ì•„ë˜)
2. ëŒ€ëµì ì¸ ë‚˜ì´ëŒ€
3. í‘œì •/ê°ì •
4. ì–¼êµ´ íŠ¹ì§• (ì•ˆê²½ ì°©ìš©, ìˆ˜ì—¼ ë“±)

ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.
\"\"\"

# API í˜¸ì¶œ
response = model.generate_content([prompt, image])
print(response.text)
""", language="python")

        # ì…ë ¥ ë°©ì‹ ì„ íƒ
        input_mode = st.radio(
            "ì…ë ¥ ë°©ì‹ ì„ íƒ",
            ["ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ì›¹ìº  ì‹¤ì‹œê°„"],
            key="face_input_mode",
            horizontal=True
        )

        if input_mode == "ì›¹ìº  ì‹¤ì‹œê°„":
            st.info("ğŸ’¡ **ì›¹ìº ìœ¼ë¡œ ì‹¤ì‹œê°„ ì–¼êµ´ íƒì§€** - MediaPipe Face Detection ì‚¬ìš©")

            try:
                from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
                import cv2
                import mediapipe as mp
                import numpy as np
                import av

                class FaceDetectionProcessor(VideoProcessorBase):
                    def __init__(self):
                        self.mp_face_detection = mp.solutions.face_detection
                        self.mp_drawing = mp.solutions.drawing_utils
                        self.face_detection = self.mp_face_detection.FaceDetection(
                            model_selection=1,
                            min_detection_confidence=0.5
                        )

                    def recv(self, frame):
                        img = frame.to_ndarray(format="bgr24")

                        # RGBë¡œ ë³€í™˜
                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                        # ì–¼êµ´ íƒì§€
                        results = self.face_detection.process(img_rgb)

                        # ê²°ê³¼ ê·¸ë¦¬ê¸°
                        if results.detections:
                            for detection in results.detections:
                                self.mp_drawing.draw_detection(img, detection)

                        return av.VideoFrame.from_ndarray(img, format="bgr24")

                webrtc_streamer(
                    key="face_detection_webcam",
                    video_processor_factory=FaceDetectionProcessor,
                    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
                    media_stream_constraints={"video": True, "audio": False}
                )

                st.markdown("""
                **ì‚¬ìš© ë°©ë²•:**
                1. "START" ë²„íŠ¼ í´ë¦­
                2. ì¹´ë©”ë¼ ê¶Œí•œ í—ˆìš©
                3. ì–¼êµ´ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ íƒì§€ë©ë‹ˆë‹¤
                """)

            except ImportError:
                st.error("âŒ streamlit-webrtcê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.code("pip install streamlit-webrtc av", language="bash")

        else:
            # ê¸°ì¡´ ì´ë¯¸ì§€ ì—…ë¡œë“œ ëª¨ë“œ
            col1, col2 = st.columns(2)

            with col1:
                detection_method = st.radio(
                    "íƒì§€ ë°©ë²• ì„ íƒ",
                    ["MediaPipe Face Detection", "Gemini API"],
                    key="face_method"
                )

            with col2:
                uploaded_file = st.file_uploader(
                    "ì–¼êµ´ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                    type=['png', 'jpg', 'jpeg'],
                    key="face_upload"
                )

                if uploaded_file:
                    image = Image.open(uploaded_file).convert('RGB')

                    col_a, col_b = st.columns(2)

                    with col_a:
                        st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€", use_container_width=True)

                    if st.button("ğŸ‘¤ ì–¼êµ´ ê°ì§€ ì‹¤í–‰", key="face_detect", type="primary"):

                        # MediaPipe Face Detection ì‚¬ìš©
                        if detection_method == "MediaPipe Face Detection":
                            with st.spinner("MediaPipeë¡œ ì–¼êµ´ íƒì§€ ì¤‘..."):
                                try:
                                    import mediapipe as mp
                                    import cv2
                                    import numpy as np
                                    import matplotlib.pyplot as plt
                                    import matplotlib.patches as patches

                                    # MediaPipe Face Detection ì´ˆê¸°í™”
                                    mp_face_detection = mp.solutions.face_detection
                                    mp_drawing = mp.solutions.drawing_utils

                                    # Face Detection ëª¨ë¸
                                    face_detection = mp_face_detection.FaceDetection(
                                        model_selection=1,  # 0: short-range (2m), 1: full-range (5m)
                                        min_detection_confidence=0.5
                                    )

                                    # PIL to OpenCV
                                    image_np = np.array(image)
                                    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)

                                    # ì–¼êµ´ íƒì§€
                                    results = face_detection.process(cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB))

                                    if results.detections:
                                        st.success(f"âœ… {len(results.detections)}ê°œì˜ ì–¼êµ´ íƒì§€ ì™„ë£Œ!")

                                        # ì‹œê°í™”
                                        annotated_image = image_np.copy()
                                        h, w, _ = annotated_image.shape

                                        face_info = []

                                        for idx, detection in enumerate(results.detections):
                                            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                                            mp_drawing.draw_detection(annotated_image, detection)

                                            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                                            bbox = detection.location_data.relative_bounding_box
                                            x = int(bbox.xmin * w)
                                            y = int(bbox.ymin * h)
                                            width = int(bbox.width * w)
                                            height = int(bbox.height * h)

                                            # ì‹ ë¢°ë„
                                            confidence = detection.score[0]

                                            # 6ê°œ í‚¤í¬ì¸íŠ¸ (ì˜¤ë¥¸ìª½ ëˆˆ, ì™¼ìª½ ëˆˆ, ì½” ë, ì…, ì˜¤ë¥¸ìª½ ê·€, ì™¼ìª½ ê·€)
                                            keypoints = []
                                            for keypoint in detection.location_data.relative_keypoints:
                                                keypoints.append({
                                                    'x': int(keypoint.x * w),
                                                    'y': int(keypoint.y * h)
                                                })

                                            face_info.append({
                                                "bbox": [x, y, width, height],
                                                "confidence": confidence,
                                                "keypoints": keypoints
                                            })

                                        with col_b:
                                            st.image(annotated_image, caption="ì–¼êµ´ íƒì§€ ê²°ê³¼", use_container_width=True)

                                        # ê²°ê³¼ ì¶œë ¥
                                        st.markdown("#### íƒì§€ ê²°ê³¼")
                                        for i, info in enumerate(face_info):
                                            x, y, w, h = info['bbox']
                                            st.markdown(f"""
                                            **ì–¼êµ´ #{i+1}**
                                            - ì‹ ë¢°ë„: {info['confidence']:.2%}
                                            - ìœ„ì¹˜: [{x}, {y}, {x+w}, {y+h}]
                                            - í¬ê¸°: {w}Ã—{h} px
                                            """)

                                        # í‚¤í¬ì¸íŠ¸ ì •ë³´
                                        with st.expander("ğŸ“Š ì–¼êµ´ í‚¤í¬ì¸íŠ¸ (6ê°œ)"):
                                            keypoint_names = [
                                                "ì˜¤ë¥¸ìª½ ëˆˆ", "ì™¼ìª½ ëˆˆ", "ì½” ë",
                                                "ì… ì¤‘ì•™", "ì˜¤ë¥¸ìª½ ê·€", "ì™¼ìª½ ê·€"
                                            ]

                                            for idx, info in enumerate(face_info):
                                                st.markdown(f"**ì–¼êµ´ #{idx+1} í‚¤í¬ì¸íŠ¸:**")
                                                for i, kp in enumerate(info['keypoints']):
                                                    if i < len(keypoint_names):
                                                        st.caption(f"{keypoint_names[i]}: x={kp['x']}, y={kp['y']}")

                                        # ëª¨ë¸ ì •ë³´
                                        with st.expander("ğŸ“Š MediaPipe Face Detection ì •ë³´"):
                                            st.markdown("""
                                            **ëª¨ë¸**: BlazeFace
                                            - **ì•„í‚¤í…ì²˜**: SSD ë³€í˜•, ê²½ëŸ‰í™” ëª¨ë¸
                                            - **íƒì§€ ë²”ìœ„**: Full-range (ìµœëŒ€ 5m)
                                            - **ì¶œë ¥**:
                                              - ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤
                                              - ì‹ ë¢°ë„ ì ìˆ˜
                                              - 6ê°œ ì–¼êµ´ í‚¤í¬ì¸íŠ¸
                                            - **ì†ë„**: ì‹¤ì‹œê°„ (~200 FPS on GPU)

                                            **6ê°œ í‚¤í¬ì¸íŠ¸:**
                                            1. ì˜¤ë¥¸ìª½ ëˆˆ ì¤‘ì‹¬
                                            2. ì™¼ìª½ ëˆˆ ì¤‘ì‹¬
                                            3. ì½” ë
                                            4. ì… ì¤‘ì•™
                                            5. ì˜¤ë¥¸ìª½ ê·€ (ê·€ì™€ ì–¼êµ´ ê²½ê³„)
                                            6. ì™¼ìª½ ê·€ (ê·€ì™€ ì–¼êµ´ ê²½ê³„)
                                            """)

                                    else:
                                        st.warning("âš ï¸ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ íƒì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")

                                    face_detection.close()

                                except ImportError:
                                    st.error("âŒ MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                    st.code("pip install mediapipe opencv-python", language="bash")
                                except Exception as e:
                                    st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

                        # Gemini API ì‚¬ìš©
                        else:
                            api_key = os.getenv('GOOGLE_API_KEY')
                            if api_key and api_key != 'your_api_key_here':
                                with st.spinner("Gemini APIë¡œ ì–¼êµ´ ë¶„ì„ ì¤‘..."):
                                    try:
                                        genai.configure(api_key=api_key)
                                        model = genai.GenerativeModel('gemini-2.0-flash-exp')

                                        prompt = """
ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  ì–¼êµ´ì„ ê°ì§€í•˜ê³  ê° ì–¼êµ´ì— ëŒ€í•´:
1. ìœ„ì¹˜ (ì™¼ìª½/ì¤‘ì•™/ì˜¤ë¥¸ìª½, ìœ„/ì¤‘ê°„/ì•„ë˜)
2. ëŒ€ëµì ì¸ ë‚˜ì´ëŒ€
3. í‘œì •/ê°ì •
4. ì–¼êµ´ íŠ¹ì§• (ì•ˆê²½ ì°©ìš©, ìˆ˜ì—¼ ë“±)

ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.
                                        """

                                        response = model.generate_content([prompt, image])

                                        with col_b:
                                            st.success("âœ… Gemini API ë¶„ì„ ì™„ë£Œ!")
                                            st.markdown(response.text)

                                    except Exception as e:
                                        st.error(f"âŒ API ì˜¤ë¥˜: {str(e)}")
                            else:
                                st.warning("âš ï¸ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def license_plate_project(self):
        """ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì¸ì‹"""
        st.subheader("ğŸš— ì°¨ëŸ‰ ë²ˆí˜¸íŒ ì¸ì‹")

        # ì´ë¡ ì  ë°°ê²½ ì¶”ê°€
        with st.expander("ğŸ“š ì´ë¡ ì  ë°°ê²½: ë²ˆí˜¸íŒ ì¸ì‹ ì‹œìŠ¤í…œ (ALPR)", expanded=False):
            st.markdown("""
            ### ALPR (Automatic License Plate Recognition)

            **ALPR**ì€ ì°¨ëŸ‰ ë²ˆí˜¸íŒì„ ìë™ìœ¼ë¡œ ì½ëŠ” ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

            #### 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸

            **Stage 1: ì°¨ëŸ‰ íƒì§€ (Vehicle Detection)**
            - **ëª¨ë¸**: YOLOv8, Faster R-CNN
            - **ëª©ì **: ì´ë¯¸ì§€ì—ì„œ ì°¨ëŸ‰ ìœ„ì¹˜ ì°¾ê¸°
            - **COCO í´ë˜ìŠ¤**: car, truck, bus, motorcycle
            - **ì¶œë ¥**: ì°¨ëŸ‰ ë°”ìš´ë”© ë°•ìŠ¤

            **Stage 2: ë²ˆí˜¸íŒ íƒì§€ (License Plate Detection)**
            - **ëª¨ë¸**: íŠ¹í™”ëœ YOLO ë˜ëŠ” CNN
            - **ëª©ì **: ì°¨ëŸ‰ ë‚´ ë²ˆí˜¸íŒ ì˜ì—­ ì •í™•íˆ ì°¾ê¸°
            - **ë°ì´í„°**: ê°êµ­ ë²ˆí˜¸íŒ í˜•íƒœì— ë§ì¶˜ í•™ìŠµ
            - **ì „ì²˜ë¦¬**:
              - ì›ê·¼ ë³€í™˜ (Perspective Transform)
              - ê¸°ìš¸ê¸° ë³´ì • (Deskewing)
              - í¬ê¸° ì •ê·œí™”
            - **ì¶œë ¥**: ë²ˆí˜¸íŒ ë°”ìš´ë”© ë°•ìŠ¤

            **Stage 3: OCR (Optical Character Recognition)**
            - **ì „í†µì  ë°©ë²•**:
              - ì´ì§„í™” (Binarization)
              - ë¬¸ì ë¶„í•  (Character Segmentation)
              - í…œí”Œë¦¿ ë§¤ì¹­
            - **ë”¥ëŸ¬ë‹ ë°©ë²•**:
              - **CRNN (CNN + RNN + CTC)**: ë¬¸ì ì‹œí€€ìŠ¤ ì¸ì‹
              - **EasyOCR/PaddleOCR**: ì‚¬ì „í•™ìŠµ OCR ëª¨ë¸
              - **TrOCR (Transformer OCR)**: Transformer ê¸°ë°˜ ìµœì‹  ê¸°ìˆ 
            - **ì¶œë ¥**: ë²ˆí˜¸íŒ í…ìŠ¤íŠ¸

            #### ë²ˆí˜¸íŒ íŠ¹í™” OCR ë„ì „ê³¼ì œ

            **1. ë‹¤ì–‘í•œ ë²ˆí˜¸íŒ í¬ë§·**
            - í•œêµ­: 12ê°€ 3456, ì„œìš¸12ê°€3456
            - ë¯¸êµ­: ABC-1234
            - ìœ ëŸ½: XX-123-YY
            â†’ êµ­ê°€ë³„ ì •ê·œí‘œí˜„ì‹ í•„ìš”

            **2. ì´ë¯¸ì§€ í’ˆì§ˆ ë¬¸ì œ**
            - ëª¨ì…˜ ë¸”ëŸ¬ (Motion Blur)
            - ì¡°ëª… ë³€í™” (ì•¼ê°„, ì—­ê´‘)
            - ë²ˆí˜¸íŒ ì˜¤ì—¼/ì†ìƒ
            - ì¹´ë©”ë¼ ê°ë„ (ì›ê·¼ ì™œê³¡)
            â†’ ì „ì²˜ë¦¬ì™€ ë°ì´í„° ì¦ê°• í•„ìˆ˜

            **3. ìœ ì‚¬ ë¬¸ì êµ¬ë¶„**
            - O (ì•ŒíŒŒë²³) vs 0 (ìˆ«ì)
            - I (ì•ŒíŒŒë²³) vs 1 (ìˆ«ì)
            - B vs 8, D vs 0
            â†’ ë¬¸ë§¥ ê¸°ë°˜ í›„ì²˜ë¦¬ í•„ìš”

            #### CRNN ì•„í‚¤í…ì²˜ (OCRì˜ í•µì‹¬)

            ```
            ì…ë ¥ ë²ˆí˜¸íŒ ì´ë¯¸ì§€ (HÃ—WÃ—3)
                â†“
            CNN Backbone (íŠ¹ì§• ì¶”ì¶œ)
                â†“
            Feature Map (1Ã—W'Ã—C)
                â†“
            Bidirectional LSTM (ì‹œí€€ìŠ¤ ëª¨ë¸ë§)
                â†“
            CTC Loss (ì •ë ¬ ì—†ëŠ” í•™ìŠµ)
                â†“
            ì¶œë ¥ í…ìŠ¤íŠ¸: "12ê°€3456"
            ```

            **CTC (Connectionist Temporal Classification)**:
            - ë¬¸ì ìœ„ì¹˜ë¥¼ ë¯¸ë¦¬ ì•Œ í•„ìš” ì—†ìŒ
            - ê°€ë³€ ê¸¸ì´ ì¶œë ¥ ê°€ëŠ¥
            - Blank í† í°ìœ¼ë¡œ ì¤‘ë³µ ì œê±°

            #### ì‹¤ì „ ALPR ì‹œìŠ¤í…œ êµ¬í˜„

            **ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬:**
            - **EasyOCR**: 80ê°œ ì–¸ì–´ ì§€ì›, PyTorch ê¸°ë°˜
            - **PaddleOCR**: ì¤‘êµ­ ë°”ì´ë‘, PP-OCR ëª¨ë¸
            - **Tesseract**: ì „í†µì  OCR, ë²ˆí˜¸íŒì—” ë¶€ì í•©

            **ì„±ëŠ¥ ìµœì í™”:**
            - **ì¶”ì  (Tracking)**: ì—¬ëŸ¬ í”„ë ˆì„ ê²°ê³¼ ê²°í•©
            - **ì•™ìƒë¸”**: ë‹¤ì¤‘ OCR ëª¨ë¸ ê²°ê³¼ íˆ¬í‘œ
            - **ì •ê·œí‘œí˜„ì‹ í•„í„°**: í˜•ì‹ì— ë§ëŠ” ê²°ê³¼ë§Œ ì„ íƒ
            """)

        st.markdown("""
        ### í”„ë¡œì íŠ¸ ê°œìš”

        ì°¨ëŸ‰ ë²ˆí˜¸íŒì„ íƒì§€í•˜ê³  OCRë¡œ ë²ˆí˜¸ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.

        **íŒŒì´í”„ë¼ì¸:**
        1. ì°¨ëŸ‰ íƒì§€ (Vehicle Detection)
        2. ë²ˆí˜¸íŒ ì˜ì—­ íƒì§€ (License Plate Detection)
        3. OCRë¡œ ë²ˆí˜¸ ì¸ì‹ (Text Recognition)
        """)

        use_api = st.checkbox("ì‹¤ì œ Gemini API ì‚¬ìš©", key="plate_api")

        uploaded_file = st.file_uploader(
            "ì°¨ëŸ‰ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
            type=['png', 'jpg', 'jpeg'],
            key="plate_upload"
        )

        if uploaded_file:
            image = Image.open(uploaded_file)
            st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", width='stretch')

            if st.button("ë²ˆí˜¸íŒ ì¸ì‹ ì‹¤í–‰", key="plate_detect"):
                if use_api:
                    api_key = os.getenv('GOOGLE_API_KEY')
                    if api_key and api_key != 'your_api_key_here':
                        with st.spinner("ë²ˆí˜¸íŒ ì¸ì‹ ì¤‘..."):
                            try:
                                genai.configure(api_key=api_key)
                                model = genai.GenerativeModel('gemini-2.0-flash-exp')

                                prompt = """
ì´ ì´ë¯¸ì§€ì—ì„œ:
1. ì°¨ëŸ‰ì„ íƒì§€í•˜ê³ 
2. ë²ˆí˜¸íŒ ìœ„ì¹˜ë¥¼ ì°¾ê³ 
3. ë²ˆí˜¸íŒì˜ ìˆ«ì/ë¬¸ìë¥¼ ì½ì–´ì£¼ì„¸ìš”.

ë²ˆí˜¸íŒì´ ëª…í™•í•˜ì§€ ì•Šë‹¤ë©´ ê·¸ ì´ìœ ë„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                                """

                                response = model.generate_content([prompt, image])

                                st.success("âœ… ì¸ì‹ ì™„ë£Œ!")
                                st.write(response.text)

                            except Exception as e:
                                st.error(f"API ì˜¤ë¥˜: {str(e)}")
                    else:
                        st.warning("âš ï¸ API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    with st.spinner("ì‹œë®¬ë ˆì´ì…˜ ì¸ì‹ ì¤‘..."):
                        st.success("âœ… ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ!")
                        st.info("""
**ì¸ì‹ ê²°ê³¼:**

ì°¨ëŸ‰: ìŠ¹ìš©ì°¨ (ì‹ ë¢°ë„ 0.95)
ë²ˆí˜¸íŒ ìœ„ì¹˜: ì „ë©´ ì¤‘ì•™
ë²ˆí˜¸íŒ ë²ˆí˜¸: 12ê°€ 3456

ì¶”ê°€ ì •ë³´:
- ì°¨ëŸ‰ ìƒ‰ìƒ: í°ìƒ‰
- ì°¨ëŸ‰ íƒ€ì…: ì„¸ë‹¨
                        """)

    def hand_gesture_project(self):
        """ì†ë™ì‘ ì¸ì‹"""
        st.subheader("âœ‹ ì†ë™ì‘ ì¸ì‹")

        # ì´ë¡ ì  ë°°ê²½ ì¶”ê°€
        with st.expander("ğŸ“š ì´ë¡ ì  ë°°ê²½: ì† íƒì§€ ë° ì œìŠ¤ì²˜ ì¸ì‹", expanded=False):
            st.markdown("""
            ### Hand Detection & Gesture Recognition

            ì† íƒì§€ì™€ ì œìŠ¤ì²˜ ì¸ì‹ì€ Human-Computer Interaction(HCI)ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.

            #### 1. ì† íƒì§€ (Hand Detection)

            **ê°ì²´ íƒì§€ ê¸°ë°˜ ì ‘ê·¼**
            - **YOLO/SSD**: ì¼ë°˜ ê°ì²´ íƒì§€ ëª¨ë¸ë¡œ ì† íƒì§€
            - **ë°ì´í„°**: EgoHands, COCO (person í‚¤í¬ì¸íŠ¸)
            - **ë¬¸ì œì **: ì†ì€ ì‘ê³  ë°°ê²½ê³¼ ë¹„ìŠ·í•´ íƒì§€ ì–´ë ¤ì›€

            **íŠ¹í™” ëª¨ë¸**
            - **MediaPipe Hands (Google)**:
              - Palm Detection + Hand Landmark 2ë‹¨ê³„
              - ê²½ëŸ‰ ëª¨ë¸ë¡œ ëª¨ë°”ì¼ì—ì„œ ì‹¤ì‹œê°„ ë™ì‘
              - 21ê°œ ì† ëœë“œë§ˆí¬ ì œê³µ
            - **OpenPose Hand**:
              - ì „ì‹  í¬ì¦ˆ ì¶”ì •ì˜ í™•ì¥
              - 21ê°œ ì† í‚¤í¬ì¸íŠ¸

            #### 2. ì† ëœë“œë§ˆí¬ (Hand Landmarks)

            **MediaPipe 21ê°œ ëœë“œë§ˆí¬ êµ¬ì¡°:**
            ```
            0: ì†ëª© (Wrist)
            1-4: ì—„ì§€ (Thumb)
            5-8: ê²€ì§€ (Index)
            9-12: ì¤‘ì§€ (Middle)
            13-16: ì•½ì§€ (Ring)
            17-20: ìƒˆë¼ (Pinky)
            ```

            **ëœë“œë§ˆí¬ë¡œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì •ë³´:**
            - **ì†ê°€ë½ í¼ì¹¨ ì—¬ë¶€**: ê´€ì ˆ ê°ë„ ê³„ì‚°
            - **ì† ë°©í–¥**: ì†ëª©-ì¤‘ì§€ ë²¡í„°
            - **ì† í¬ê¸°**: ì†ëª©-ì¤‘ì§€ ê±°ë¦¬
            - **ì† ëª¨ì–‘**: ì†ê°€ë½ ê°„ ê°ë„ ê´€ê³„

            #### 3. ì œìŠ¤ì²˜ ì¸ì‹ ë°©ë²•

            **A. Rule-based (ê·œì¹™ ê¸°ë°˜)**
            - ì†ê°€ë½ ê°œìˆ˜ ì„¸ê¸°
              - í¼ì³ì§„ ì†ê°€ë½ ëì´ MCP(ì†ë“± ê´€ì ˆ)ë³´ë‹¤ ìœ„ì— ìˆìœ¼ë©´ í¼ì¹¨
              - ì˜ˆ: 5ê°œ â†’ "ë³´", 0ê°œ â†’ "ì£¼ë¨¹", 2ê°œ â†’ "ê°€ìœ„"
            - ì† í˜•íƒœ íŒ¨í„´ ë§¤ì¹­
              - ì—„ì§€+ê²€ì§€ë§Œ í¼ì¹¨ â†’ "ì´"
              - ì—„ì§€+ìƒˆë¼ë§Œ í¼ì¹¨ â†’ "ìƒ¤ì¹´(Shaka)"
            - ì¥ì : ë¹ ë¥´ê³  ì •í™•
            - ë‹¨ì : ë¯¸ë¦¬ ì •ì˜ëœ ì œìŠ¤ì²˜ë§Œ ì¸ì‹

            **B. Machine Learning (ë”¥ëŸ¬ë‹)**
            - **ì…ë ¥**: 21ê°œ ëœë“œë§ˆí¬ ì¢Œí‘œ (x, y, z) Ã— 21 = 63ì°¨ì›
            - **ëª¨ë¸**:
              - MLP (Multi-Layer Perceptron): ê°„ë‹¨í•œ ë¶„ë¥˜
              - LSTM/GRU: ì‹œê°„ ìˆœì„œ ì œìŠ¤ì²˜ (ë™ì  ì œìŠ¤ì²˜)
              - Transformer: ë³µì¡í•œ ì‹œí€€ìŠ¤ ì œìŠ¤ì²˜
            - **ì¶œë ¥**: ì œìŠ¤ì²˜ í´ë˜ìŠ¤ (Rock, Paper, Scissors, OK, Peace ë“±)
            - ì¥ì : ë³µì¡í•˜ê³  ë‹¤ì–‘í•œ ì œìŠ¤ì²˜ í•™ìŠµ ê°€ëŠ¥
            - ë‹¨ì : í•™ìŠµ ë°ì´í„° í•„ìš”

            **C. Sequence-based (ë™ì  ì œìŠ¤ì²˜)**
            - ì •ì  ì œìŠ¤ì²˜: í•œ í”„ë ˆì„ (ì˜ˆ: ì—„ì§€ì²™)
            - ë™ì  ì œìŠ¤ì²˜: ì—¬ëŸ¬ í”„ë ˆì„ (ì˜ˆ: ì† í”ë“¤ê¸°, ìŠ¤ì™€ì´í”„)
            - **DTW (Dynamic Time Warping)**: ì‹œê³„ì—´ íŒ¨í„´ ë§¤ì¹­
            - **3D CNN / LSTM**: ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ í•™ìŠµ

            #### 4. MediaPipe Hands íŒŒì´í”„ë¼ì¸

            ```
            ì…ë ¥ ì´ë¯¸ì§€ (RGB)
                â†“
            Palm Detection Model (ì†ë°”ë‹¥ íƒì§€)
                â†“
            Hand Bounding Box (ì† ì˜ì—­)
                â†“
            Hand Landmark Model (21ê°œ í‚¤í¬ì¸íŠ¸ íšŒê·€)
                â†“
            3D Hand Landmarks (x, y, z) Ã— 21
                â†“
            [ì‘ìš©] ì œìŠ¤ì²˜ ë¶„ë¥˜ / ì†ê°€ë½ ì¹´ìš´íŒ…
            ```

            **Palm Detection Model**:
            - **ì…ë ¥**: ì „ì²´ ì´ë¯¸ì§€
            - **ì¶œë ¥**: ì†ë°”ë‹¥ ì¤‘ì‹¬ + íšŒì „ê° + í¬ê¸°
            - **íŠ¹ì§•**: ì†ë°”ë‹¥ì€ ì†ê°€ë½ë³´ë‹¤ ëœ ì›€ì§ì—¬ ì•ˆì •ì 

            **Hand Landmark Model**:
            - **ì…ë ¥**: Cropëœ ì† ì˜ì—­ (256Ã—256)
            - **ì¶œë ¥**: 21ê°œ 3D ì¢Œí‘œ + ì† ì¡´ì¬ ì—¬ë¶€(handedness)
            - **íŠ¹ì§•**: ì™¼ì†/ì˜¤ë¥¸ì† êµ¬ë¶„ ê°€ëŠ¥

            #### 5. ì‹¤ì „ ì‘ìš© ì˜ˆì‹œ

            **ì†ê°€ë½ ê°œìˆ˜ ì„¸ê¸° ì•Œê³ ë¦¬ì¦˜:**
            ```python
            def count_fingers(landmarks):
                fingers = []

                # ì—„ì§€: x ì¢Œí‘œ ë¹„êµ (ì¢Œìš° ë°˜ì „ ì£¼ì˜)
                if landmarks[4].x < landmarks[3].x:  # ì˜¤ë¥¸ì† ê¸°ì¤€
                    fingers.append(1)

                # ë‚˜ë¨¸ì§€ ì†ê°€ë½: y ì¢Œí‘œ ë¹„êµ (ë < ê´€ì ˆ)
                for id in [8, 12, 16, 20]:  # ê²€ì§€, ì¤‘ì§€, ì•½ì§€, ìƒˆë¼
                    if landmarks[id].y < landmarks[id-2].y:
                        fingers.append(1)

                return sum(fingers)
            ```

            **ì œìŠ¤ì²˜ ë¶„ë¥˜ ë°ì´í„°ì…‹:**
            - **Jester**: 148K ë¹„ë””ì˜¤, 27ê°œ ì œìŠ¤ì²˜
            - **ASL (American Sign Language)**: ìˆ˜í™” ì•ŒíŒŒë²³
            - **Custom**: ì§ì ‘ ìˆ˜ì§‘í•œ íŠ¹ì • ë„ë©”ì¸ ì œìŠ¤ì²˜

            #### 6. MediaPipe ëŒ€ì•ˆ ë¼ì´ë¸ŒëŸ¬ë¦¬ (2025)

            **ë” ë†’ì€ ì„±ëŠ¥ì„ ì›í•œë‹¤ë©´?**

            | ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì†ë„ (FPS) | ì •í™•ë„ | ë‹¤ì¤‘ì¸ë¬¼ | ë‚œì´ë„ | ì¶”ì²œ ìš©ë„ |
            |-----------|-----------|--------|---------|--------|----------|
            | **MediaPipe** | 200+ | ì¤‘ìƒ | âŒ (1ëª…) | ì‰¬ì›€ | ì‹¤ì‹œê°„ ë‹¨ì¼ ì¸ë¬¼, í”„ë¡œí† íƒ€ì… |
            | **MMPose** | 430+ | â­ìµœê³  | âœ… | ì¤‘ê°„ | ì—°êµ¬, ê³ ì •ë°€ë„ í•„ìš” |
            | **YOLOv8 Pose** | 100+ | ìƒ | âœ… | ì‰¬ì›€ | ë‹¤ì¤‘ ì¸ë¬¼, ê°ì²´+í¬ì¦ˆ ë™ì‹œ |
            | **OpenPose** | 15 | ì¤‘ | âœ… | ì–´ë ¤ì›€ | ì—°êµ¬ìš© (ë ˆê±°ì‹œ) |

            **MMPose (OpenMMLab)** - 2025ë…„ SOTA:
            - RTMPose ëª¨ë¸: 430+ FPS (GTX 1660 Ti)
            - COCO 75.8% AP (MediaPipeë³´ë‹¤ ìš°ìˆ˜)
            - ì†, ì–¼êµ´, ì „ì‹ , 3D í¬ì¦ˆ ëª¨ë‘ ì§€ì›
            - PyTorch ê¸°ë°˜, í¬ë¡œìŠ¤ í”Œë«í¼

            **YOLOv8/v7 Pose**:
            - ë‹¤ì¤‘ ì¸ë¬¼ ë™ì‹œ ì¶”ì  (MediaPipeëŠ” 1ëª…ë§Œ)
            - ê°ì²´ íƒì§€ + í¬ì¦ˆ ì¶”ì • í†µí•©
            - Ultralytics íŒ¨í‚¤ì§€ë¡œ ê°„í¸ ì‚¬ìš©

            **ì„ íƒ ê°€ì´ë“œ**:
            - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…, í•™ìŠµìš© â†’ **MediaPipe** âœ…
            - ìµœê³  ì •í™•ë„, ì—°êµ¬ â†’ **MMPose**
            - ë‹¤ì¤‘ ì¸ë¬¼, ì‹¤ë¬´ â†’ **YOLOv8 Pose**

            #### 7. API vs ë¡œì»¬ ëª¨ë¸ ë¹„êµ

            **Gemini API ì¥ì **:
            - ë³µì¡í•œ ì¶”ë¡ : "ì´ ì œìŠ¤ì²˜ì˜ ì˜ë¯¸ëŠ”?"
            - ìì—°ì–´ ì¶œë ¥: ì¸ê°„ ì¹œí™”ì  ì„¤ëª…
            - ë§¥ë½ ì´í•´: ë‚˜ì´, ê°ì • ë¶„ì„

            **MediaPipe/ë¡œì»¬ ëª¨ë¸ ì¥ì **:
            - âœ… **ë¬´ë£Œ**: API ë¹„ìš© $0
            - âœ… **ë¹ ë¦„**: 10-20ë°° ë¹ ë¥¸ ì†ë„
            - âœ… **í”„ë¼ì´ë²„ì‹œ**: ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ë‚˜ê°€ì§€ ì•ŠìŒ
            - âœ… **ì˜¤í”„ë¼ì¸**: ì¸í„°ë„· ì—†ì´ ë™ì‘
            - âœ… **ì •í™•í•œ ì¢Œí‘œ**: 21ê°œ ëœë“œë§ˆí¬ (x, y, z)
            - âœ… **ì‹¤ì‹œê°„**: ë¹„ë””ì˜¤, ì›¹ìº , AR/VR ê°€ëŠ¥

            **ì‹¤ë¬´ í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ**:
            1. MediaPipeë¡œ ë¹ ë¥¸ ëœë“œë§ˆí¬ ì¶”ì¶œ
            2. ë³µì¡í•œ ê²½ìš°ë§Œ Gemini API í˜¸ì¶œ
            â†’ ë¹„ìš© ì ˆê° + ì„±ëŠ¥ ìµœì í™”
            """)

        st.markdown("""
        ### í”„ë¡œì íŠ¸ ê°œìš”

        ì†ì„ íƒì§€í•˜ê³  ì†ê°€ë½ ê°œìˆ˜ë¥¼ ì„¸ê±°ë‚˜ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.

        **ì‘ìš© ë¶„ì•¼:**
        - ê°€ìƒ ë§ˆìš°ìŠ¤
        - ìˆ˜í™” ë²ˆì—­
        - ê²Œì„ ì»¨íŠ¸ë¡¤
        - ìŠ¤ë§ˆíŠ¸í™ˆ ì œì–´
        """)

        # ì½”ë“œ ì˜ˆì‹œ - MediaPipe
        with st.expander("ğŸ’» MediaPipe ì†ë™ì‘ ì¸ì‹ ì½”ë“œ", expanded=False):
            st.code("""
import mediapipe as mp
import cv2
import numpy as np
from PIL import Image

# MediaPipe Hands ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# Hands ëª¨ë¸ (ìµœëŒ€ 2ê°œ ì† íƒì§€)
hands = mp_hands.Hands(
    static_image_mode=True,
    max_num_hands=2,
    min_detection_confidence=0.5
)

# ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open('your_image.jpg').convert('RGB')
image_np = np.array(image)

# ì† íƒì§€
results = hands.process(image_np)

# ê²°ê³¼ ì‹œê°í™”
if results.multi_hand_landmarks:
    for hand_landmarks in results.multi_hand_landmarks:
        # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸° (21ê°œ keypoints)
        mp_drawing.draw_landmarks(
            image_np,
            hand_landmarks,
            mp_hands.HAND_CONNECTIONS,
            mp_drawing_styles.get_default_hand_landmarks_style(),
            mp_drawing_styles.get_default_hand_connections_style()
        )

        # ì†ê°€ë½ ê°œìˆ˜ ì„¸ê¸°
        landmarks = hand_landmarks.landmark
        finger_count = 0

        # ì—„ì§€ (x ì¢Œí‘œ ë¹„êµ)
        if landmarks[4].x < landmarks[3].x:
            finger_count += 1

        # ë‚˜ë¨¸ì§€ ì†ê°€ë½ (y ì¢Œí‘œ ë¹„êµ)
        for tip_id in [8, 12, 16, 20]:
            if landmarks[tip_id].y < landmarks[tip_id - 2].y:
                finger_count += 1

        print(f"í¼ì¹œ ì†ê°€ë½ ê°œìˆ˜: {finger_count}")

hands.close()
""", language="python")

        # ì½”ë“œ ì˜ˆì‹œ - Gemini API
        with st.expander("ğŸ’» Gemini API ì†ë™ì‘ ë¶„ì„ ì½”ë“œ", expanded=False):
            st.code("""
import google.generativeai as genai
from PIL import Image
import os

# API í‚¤ ì„¤ì •
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))

# Gemini 2.0 Flash ëª¨ë¸ ì‚¬ìš©
model = genai.GenerativeModel('gemini-2.0-flash-exp')

# ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open('your_image.jpg')

# ì†ë™ì‘ ë¶„ì„ í”„ë¡¬í”„íŠ¸
prompt = \"\"\"
ì´ ì´ë¯¸ì§€ì—ì„œ ì†ì„ ë¶„ì„í•˜ê³ :
1. ì† ê°œìˆ˜
2. í¼ì³ì§„ ì†ê°€ë½ ê°œìˆ˜
3. ì†ë™ì‘/ì œìŠ¤ì²˜ (ì˜ˆ: ê°€ìœ„, ë°”ìœ„, ë³´, ì—„ì§€ì²™, Vì‚¬ì¸ ë“±)
4. ì†ì˜ ìœ„ì¹˜
5. ì™¼ì†/ì˜¤ë¥¸ì† êµ¬ë¶„

ë¥¼ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.
\"\"\"

# API í˜¸ì¶œ
response = model.generate_content([prompt, image])
print(response.text)
""", language="python")

        # ì…ë ¥ ë°©ì‹ ì„ íƒ
        input_mode = st.radio(
            "ì…ë ¥ ë°©ì‹ ì„ íƒ",
            ["ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ì›¹ìº  ì‹¤ì‹œê°„"],
            key="hand_input_mode",
            horizontal=True
        )

        if input_mode == "ì›¹ìº  ì‹¤ì‹œê°„":
            st.info("ğŸ’¡ **ì›¹ìº ìœ¼ë¡œ ì‹¤ì‹œê°„ ì†ë™ì‘ íƒì§€** - MediaPipe Hands ì‚¬ìš©")

            try:
                from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
                import cv2
                import mediapipe as mp
                import numpy as np
                import av

                class HandDetectionProcessor(VideoProcessorBase):
                    def __init__(self):
                        self.mp_hands = mp.solutions.hands
                        self.mp_drawing = mp.solutions.drawing_utils
                        self.mp_drawing_styles = mp.solutions.drawing_styles
                        self.hands = self.mp_hands.Hands(
                            static_image_mode=False,
                            max_num_hands=2,
                            min_detection_confidence=0.5,
                            min_tracking_confidence=0.5
                        )

                    def recv(self, frame):
                        img = frame.to_ndarray(format="bgr24")

                        # RGBë¡œ ë³€í™˜
                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                        # ì† íƒì§€
                        results = self.hands.process(img_rgb)

                        # ê²°ê³¼ ê·¸ë¦¬ê¸°
                        if results.multi_hand_landmarks:
                            for hand_landmarks in results.multi_hand_landmarks:
                                self.mp_drawing.draw_landmarks(
                                    img,
                                    hand_landmarks,
                                    self.mp_hands.HAND_CONNECTIONS,
                                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                                    self.mp_drawing_styles.get_default_hand_connections_style()
                                )

                                # ì†ê°€ë½ ê°œìˆ˜ í‘œì‹œ
                                # ê°„ë‹¨í•œ ì¹´ìš´íŒ… (ì—„ì§€ëŠ” xì¢Œí‘œ, ë‚˜ë¨¸ì§€ëŠ” yì¢Œí‘œ ë¹„êµ)
                                finger_count = 0
                                landmarks = hand_landmarks.landmark

                                # ì—„ì§€
                                if landmarks[4].x < landmarks[3].x:
                                    finger_count += 1

                                # ë‚˜ë¨¸ì§€ ì†ê°€ë½
                                for tip_id in [8, 12, 16, 20]:
                                    if landmarks[tip_id].y < landmarks[tip_id - 2].y:
                                        finger_count += 1

                                # í™”ë©´ì— í‘œì‹œ
                                cv2.putText(img, f"Fingers: {finger_count}", (10, 30),
                                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

                        return av.VideoFrame.from_ndarray(img, format="bgr24")

                webrtc_streamer(
                    key="hand_detection_webcam",
                    video_processor_factory=HandDetectionProcessor,
                    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
                    media_stream_constraints={"video": True, "audio": False}
                )

                st.markdown("""
                **ì‚¬ìš© ë°©ë²•:**
                1. "START" ë²„íŠ¼ í´ë¦­
                2. ì¹´ë©”ë¼ ê¶Œí•œ í—ˆìš©
                3. ì†ì„ ì¹´ë©”ë¼ì— ë¹„ì¶”ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ íƒì§€ë˜ê³  ì†ê°€ë½ ê°œìˆ˜ê°€ í‘œì‹œë©ë‹ˆë‹¤

                **ì œìŠ¤ì²˜ í…ŒìŠ¤íŠ¸:**
                - âœŠ ì£¼ë¨¹: 0ê°œ
                - â˜ï¸ ê²€ì§€: 1ê°œ
                - âœŒï¸ ê°€ìœ„/Vì‚¬ì¸: 2ê°œ
                - ğŸ–ï¸ ë³´: 5ê°œ
                """)

            except ImportError:
                st.error("âŒ streamlit-webrtcê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.code("pip install streamlit-webrtc av", language="bash")

        else:
            # ê¸°ì¡´ ì´ë¯¸ì§€ ì—…ë¡œë“œ ëª¨ë“œ
            col1, col2 = st.columns(2)

            with col1:
                detection_method = st.radio(
                    "íƒì§€ ë°©ë²• ì„ íƒ",
                    ["MediaPipe Hand Landmarker", "Gemini API"],
                    key="hand_method"
                )

            with col2:
                uploaded_file = st.file_uploader(
                    "ì†ë™ì‘ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                    type=['png', 'jpg', 'jpeg'],
                    key="hand_upload"
                )

                if uploaded_file:
                    image = Image.open(uploaded_file).convert('RGB')

                    col_a, col_b = st.columns(2)

                    with col_a:
                        st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€", use_container_width=True)

                    if st.button("ğŸ¤š ì†ë™ì‘ ì¸ì‹ ì‹¤í–‰", key="hand_detect", type="primary"):

                        # MediaPipe ì‚¬ìš©
                        if detection_method == "MediaPipe Hand Landmarker":
                            st.warning("""
                            âš ï¸ **MediaPipe ì„¤ì¹˜ ì•ˆë‚´**

                            MediaPipeëŠ” Python 3.13ì„ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

                            **í•´ê²° ë°©ë²•:**
                            1. Python 3.11 ë˜ëŠ” 3.12 í™˜ê²½ ì‚¬ìš©
                            2. ë˜ëŠ” Gemini API ë°©ì‹ ì„ íƒ

                            **ì„¤ì¹˜ ëª…ë ¹ (Python 3.11/3.12):**
                            ```bash
                            pip install mediapipe opencv-python
                            ```
                            """)

                            with st.spinner("MediaPipeë¡œ ì† ëœë“œë§ˆí¬ íƒì§€ ì¤‘..."):
                                try:
                                    import mediapipe as mp
                                    import cv2
                                    import numpy as np
                                    import matplotlib.pyplot as plt
                                    import matplotlib.patches as patches

                                    # MediaPipe Hands ì´ˆê¸°í™”
                                    mp_hands = mp.solutions.hands
                                    mp_drawing = mp.solutions.drawing_utils
                                    mp_drawing_styles = mp.solutions.drawing_styles

                                    # Hands ëª¨ë¸ (static_image_mode=True for images)
                                    hands = mp_hands.Hands(
                                        static_image_mode=True,
                                        max_num_hands=2,
                                        min_detection_confidence=0.5,
                                        min_tracking_confidence=0.5
                                    )

                                    # PIL to OpenCV
                                    image_np = np.array(image)
                                    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)

                                    # ì† íƒì§€
                                    results = hands.process(cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB))

                                    if results.multi_hand_landmarks:
                                        st.success(f"âœ… {len(results.multi_hand_landmarks)}ê°œì˜ ì† íƒì§€ ì™„ë£Œ!")

                                        # ì‹œê°í™”
                                        annotated_image = image_np.copy()

                                        hand_info = []

                                        for idx, (hand_landmarks, handedness) in enumerate(zip(
                                            results.multi_hand_landmarks,
                                            results.multi_handedness
                                        )):
                                            # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                                            mp_drawing.draw_landmarks(
                                                annotated_image,
                                                hand_landmarks,
                                                mp_hands.HAND_CONNECTIONS,
                                                mp_drawing_styles.get_default_hand_landmarks_style(),
                                                mp_drawing_styles.get_default_hand_connections_style()
                                            )

                                            # ì†ê°€ë½ ê°œìˆ˜ ì„¸ê¸°
                                            def count_fingers(landmarks):
                                                fingers = []

                                                # ì—„ì§€: x ì¢Œí‘œ ë¹„êµ
                                                if handedness.classification[0].label == "Right":
                                                    if landmarks[4].x < landmarks[3].x:
                                                        fingers.append(1)
                                                else:  # Left
                                                    if landmarks[4].x > landmarks[3].x:
                                                        fingers.append(1)

                                                # ë‚˜ë¨¸ì§€ ì†ê°€ë½: y ì¢Œí‘œ ë¹„êµ
                                                tip_ids = [8, 12, 16, 20]  # ê²€ì§€, ì¤‘ì§€, ì•½ì§€, ìƒˆë¼
                                                for tip_id in tip_ids:
                                                    if landmarks[tip_id].y < landmarks[tip_id - 2].y:
                                                        fingers.append(1)

                                                return sum(fingers)

                                            finger_count = count_fingers(hand_landmarks.landmark)
                                            hand_label = handedness.classification[0].label
                                            hand_score = handedness.classification[0].score

                                            # ì œìŠ¤ì²˜ ì¸ì‹ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
                                            gesture = "ì•Œ ìˆ˜ ì—†ìŒ"
                                            if finger_count == 0:
                                                gesture = "ì£¼ë¨¹ (Rock)"
                                            elif finger_count == 2:
                                                gesture = "ê°€ìœ„ (Scissors) ë˜ëŠ” Vì‚¬ì¸"
                                            elif finger_count == 5:
                                                gesture = "ë³´ (Paper)"
                                            elif finger_count == 1:
                                                gesture = "í¬ì¸íŒ… ë˜ëŠ” ì—„ì§€ì²™"

                                            hand_info.append({
                                                "hand": hand_label,
                                                "confidence": hand_score,
                                                "fingers": finger_count,
                                                "gesture": gesture
                                            })

                                            # ì´ë¯¸ì§€ì— ì†ê°€ë½ ê°œìˆ˜ì™€ ì œìŠ¤ì²˜ í‘œì‹œ
                                            h, w, _ = annotated_image.shape
                                            wrist = hand_landmarks.landmark[0]
                                            text_x = int(wrist.x * w)
                                            text_y = int(wrist.y * h) - 20

                                            # í…ìŠ¤íŠ¸ ë°°ê²½
                                            cv2.rectangle(
                                                annotated_image,
                                                (text_x - 10, text_y - 30),
                                                (text_x + 200, text_y + 10),
                                                (0, 0, 0),
                                                -1
                                            )

                                            # ì†ê°€ë½ ê°œìˆ˜ í‘œì‹œ
                                            cv2.putText(
                                                annotated_image,
                                                f"Fingers: {finger_count}",
                                                (text_x, text_y - 10),
                                                cv2.FONT_HERSHEY_SIMPLEX,
                                                0.6,
                                                (0, 255, 0),
                                                2
                                            )

                                            # ì œìŠ¤ì²˜ í‘œì‹œ
                                            cv2.putText(
                                                annotated_image,
                                                gesture,
                                                (text_x, text_y + 10),
                                                cv2.FONT_HERSHEY_SIMPLEX,
                                                0.5,
                                                (255, 255, 0),
                                                1
                                            )

                                        with col_b:
                                            st.image(annotated_image, caption="ì† ëœë“œë§ˆí¬ íƒì§€ ê²°ê³¼", use_container_width=True)

                                        # ê²°ê³¼ ì¶œë ¥
                                        st.markdown("#### íƒì§€ ê²°ê³¼")
                                        for i, info in enumerate(hand_info):
                                            st.markdown(f"""
                                            **ì† #{i+1}**
                                            - ì†: {info['hand']} (ì‹ ë¢°ë„: {info['confidence']:.2%})
                                            - í¼ì³ì§„ ì†ê°€ë½: {info['fingers']}ê°œ
                                            - ì¶”ì • ì œìŠ¤ì²˜: {info['gesture']}
                                            """)

                                        # ëœë“œë§ˆí¬ ì¢Œí‘œ ì •ë³´
                                        with st.expander("ğŸ“Š 21ê°œ ì† ëœë“œë§ˆí¬ ì¢Œí‘œ"):
                                            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
                                                st.markdown(f"**ì† #{idx+1} ëœë“œë§ˆí¬:**")
                                                landmark_names = [
                                                    "WRIST", "THUMB_CMC", "THUMB_MCP", "THUMB_IP", "THUMB_TIP",
                                                    "INDEX_FINGER_MCP", "INDEX_FINGER_PIP", "INDEX_FINGER_DIP", "INDEX_FINGER_TIP",
                                                    "MIDDLE_FINGER_MCP", "MIDDLE_FINGER_PIP", "MIDDLE_FINGER_DIP", "MIDDLE_FINGER_TIP",
                                                    "RING_FINGER_MCP", "RING_FINGER_PIP", "RING_FINGER_DIP", "RING_FINGER_TIP",
                                                    "PINKY_MCP", "PINKY_PIP", "PINKY_DIP", "PINKY_TIP"
                                                ]

                                                for i, landmark in enumerate(hand_landmarks.landmark):
                                                    st.caption(f"{i}: {landmark_names[i]} - x:{landmark.x:.3f}, y:{landmark.y:.3f}, z:{landmark.z:.3f}")

                                    else:
                                        st.warning("âš ï¸ ì´ë¯¸ì§€ì—ì„œ ì†ì„ íƒì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")

                                    hands.close()

                                except ImportError:
                                    st.error("âŒ MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                                    st.code("pip install mediapipe opencv-python", language="bash")
                                except Exception as e:
                                    st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

                        # Gemini API ì‚¬ìš©
                        else:
                            api_key = os.getenv('GOOGLE_API_KEY')
                            if api_key and api_key != 'your_api_key_here':
                                with st.spinner("Gemini APIë¡œ ì†ë™ì‘ ë¶„ì„ ì¤‘..."):
                                    try:
                                        genai.configure(api_key=api_key)
                                        model = genai.GenerativeModel('gemini-2.0-flash-exp')

                                        prompt = """
ì´ ì´ë¯¸ì§€ì—ì„œ ì†ì„ ë¶„ì„í•˜ê³ :
1. ì† ê°œìˆ˜
2. í¼ì³ì§„ ì†ê°€ë½ ê°œìˆ˜
3. ì†ë™ì‘/ì œìŠ¤ì²˜ (ì˜ˆ: ê°€ìœ„, ë°”ìœ„, ë³´, ì—„ì§€ì²™, Vì‚¬ì¸ ë“±)
4. ì†ì˜ ìœ„ì¹˜
5. ì™¼ì†/ì˜¤ë¥¸ì† êµ¬ë¶„

ë¥¼ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.
                                        """

                                        response = model.generate_content([prompt, image])

                                        with col_b:
                                            st.success("âœ… Gemini API ë¶„ì„ ì™„ë£Œ!")
                                            st.markdown(response.text)

                                    except Exception as e:
                                        st.error(f"âŒ API ì˜¤ë¥˜: {str(e)}")
                            else:
                                st.warning("âš ï¸ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        st.markdown("### MediaPipe Hand Tracking")
        st.code("""
import mediapipe as mp
import cv2

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5
)

# ì´ë¯¸ì§€ ì²˜ë¦¬
results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

if results.multi_hand_landmarks:
    for hand_landmarks in results.multi_hand_landmarks:
        mp_drawing.draw_landmarks(
            image,
            hand_landmarks,
            mp_hands.HAND_CONNECTIONS
        )
        """, language="python")

    # Helper methods
    def calculate_iou(self, box1, box2):
        """IoU ê³„ì‚°"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])

        intersection = max(0, x2 - x1) * max(0, y2 - y1)

        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0

    def apply_nms(self, detections, iou_threshold):
        """NMS ì ìš©"""
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)

        keep = []
        while detections:
            best = detections.pop(0)
            keep.append(best)

            remaining = []
            for det in detections:
                iou = self.calculate_iou(best['bbox'], det['bbox'])
                if iou <= iou_threshold:
                    remaining.append(det)

            detections = remaining

        return keep