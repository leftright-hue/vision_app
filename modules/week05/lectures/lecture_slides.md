# ğŸ¯ Week 5: ê°ì²´ íƒì§€ ì´ë¡ ê³¼ YOLO ì‹¤ìŠµ

## ğŸ“Œ í•™ìŠµ ëª©í‘œ

ì´ë²ˆ ì£¼ì°¨ì—ì„œëŠ” ì»´í“¨í„° ë¹„ì „ì˜ í•µì‹¬ íƒœìŠ¤í¬ì¸ ê°ì²´ íƒì§€(Object Detection)ì˜ ì´ë¡ ê³¼ ì‹¤ì œ êµ¬í˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**í•µì‹¬ í•™ìŠµ ë‚´ìš©:**
- ğŸ” ê°ì²´ íƒì§€ì˜ ê¸°ë³¸ ê°œë…ê³¼ í‰ê°€ ì§€í‘œ
- ğŸ“ˆ R-CNN ê³„ì—´ì˜ ë°œì „ ê³¼ì •ê³¼ Two-stage ë°©ì‹
- âš¡ YOLOì˜ One-stage ë°©ì‹ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬
- ğŸ› ï¸ YOLOv8ì„ í™œìš©í•œ ì»¤ìŠ¤í…€ ê°ì²´ íƒì§€ê¸° êµ¬ì¶•

---

## 1. ê°ì²´ íƒì§€ ê°œìš”

### 1.1 ê°ì²´ íƒì§€ë€?

#### ì •ì˜
> **ê°ì²´ íƒì§€(Object Detection)**: ì´ë¯¸ì§€ì—ì„œ ê´€ì‹¬ ìˆëŠ” ê°ì²´ë“¤ì„ ì°¾ì•„ë‚´ê³ , ê° ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ í‘œì‹œí•˜ë©°, ê°ì²´ì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…

#### ì´ë¯¸ì§€ ë¶„ë¥˜ vs ê°ì²´ íƒì§€
```python
# ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification)
input: ì´ë¯¸ì§€
output: í´ë˜ìŠ¤ ë¼ë²¨ (ì˜ˆ: "ê³ ì–‘ì´")

# ê°ì²´ íƒì§€ (Object Detection)  
input: ì´ë¯¸ì§€
output: [
    {"class": "ê³ ì–‘ì´", "bbox": [x1, y1, x2, y2], "confidence": 0.95},
    {"class": "ê°œ", "bbox": [x3, y3, x4, y4], "confidence": 0.87},
    ...
]
```

#### ê°ì²´ íƒì§€ì˜ ë„ì „ê³¼ì œ
1. **ë‹¤ì¤‘ ê°ì²´**: í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ê°ì²´ê°€ ì¡´ì¬
2. **ë‹¤ì–‘í•œ í¬ê¸°**: ê°™ì€ í´ë˜ìŠ¤ë¼ë„ í¬ê¸°ê°€ ë‹¤ì–‘í•¨
3. **ê°€ë ¤ì§(Occlusion)**: ê°ì²´ë“¤ì´ ì„œë¡œ ê²¹ì³ìˆìŒ
4. **ë°°ê²½ ë³µì¡ì„±**: ë³µì¡í•œ ë°°ê²½ì—ì„œ ê°ì²´ êµ¬ë¶„
5. **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ë¹ ë¥¸ ì¶”ë¡  ì†ë„ ìš”êµ¬

### 1.2 ê°ì²´ íƒì§€ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

#### 1. ë°”ìš´ë”© ë°•ìŠ¤ (Bounding Box)
```python
# ë°”ìš´ë”© ë°•ìŠ¤ í‘œí˜„ ë°©ì‹ë“¤
bbox_formats = {
    "xyxy": [x_min, y_min, x_max, y_max],           # ì¢Œìƒë‹¨, ìš°í•˜ë‹¨ ì¢Œí‘œ
    "xywh": [x_center, y_center, width, height],    # ì¤‘ì‹¬ì ê³¼ í¬ê¸°
    "cxcywh": [cx, cy, w, h],                       # ì •ê·œí™”ëœ ì¤‘ì‹¬ì ê³¼ í¬ê¸°
}

# ë°”ìš´ë”© ë°•ìŠ¤ ë³€í™˜ í•¨ìˆ˜
def xyxy_to_xywh(bbox):
    """xyxy í˜•ì‹ì„ xywh í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    x1, y1, x2, y2 = bbox
    x_center = (x1 + x2) / 2
    y_center = (y1 + y2) / 2
    width = x2 - x1
    height = y2 - y1
    return [x_center, y_center, width, height]

def xywh_to_xyxy(bbox):
    """xywh í˜•ì‹ì„ xyxy í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    x_center, y_center, width, height = bbox
    x1 = x_center - width / 2
    y1 = y_center - height / 2
    x2 = x_center + width / 2
    y2 = y_center + height / 2
    return [x1, y1, x2, y2]
```

#### 2. ì‹ ë¢°ë„ ì ìˆ˜ (Confidence Score)
```python
# ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
confidence = P(object) * IoU(pred_box, true_box)

# P(object): í•´ë‹¹ ìœ„ì¹˜ì— ê°ì²´ê°€ ìˆì„ í™•ë¥ 
# IoU: ì˜ˆì¸¡ ë°•ìŠ¤ì™€ ì‹¤ì œ ë°•ìŠ¤ì˜ ê²¹ì¹¨ ì •ë„
```

#### 3. í´ë˜ìŠ¤ í™•ë¥  (Class Probability)
```python
# ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬
class_probs = softmax([logit_cat, logit_dog, logit_car, ...])
# ì˜ˆ: [0.7, 0.2, 0.05, 0.03, 0.02]
```

### 1.3 í‰ê°€ ì§€í‘œ

#### IoU (Intersection over Union)
```python
def calculate_iou(box1, box2):
    """
    ë‘ ë°”ìš´ë”© ë°•ìŠ¤ì˜ IoU ê³„ì‚°
    
    Args:
        box1, box2: [x1, y1, x2, y2] í˜•ì‹ì˜ ë°”ìš´ë”© ë°•ìŠ¤
    
    Returns:
        iou: 0ê³¼ 1 ì‚¬ì´ì˜ IoU ê°’
    """
    # êµì§‘í•© ì˜ì—­ ê³„ì‚°
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    # êµì§‘í•© ë©´ì 
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    
    # ê° ë°•ìŠ¤ì˜ ë©´ì 
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    # í•©ì§‘í•© ë©´ì 
    union = area1 + area2 - intersection
    
    # IoU ê³„ì‚°
    iou = intersection / union if union > 0 else 0
    return iou

# IoU í•´ì„
# IoU > 0.5: ì¼ë°˜ì ìœ¼ë¡œ "ì¢‹ì€" íƒì§€ë¡œ ê°„ì£¼
# IoU > 0.7: "ë§¤ìš° ì¢‹ì€" íƒì§€
# IoU > 0.9: "ê±°ì˜ ì™„ë²½í•œ" íƒì§€
```

#### mAP (mean Average Precision)
```python
def calculate_ap(precisions, recalls):
    """
    Average Precision ê³„ì‚° (11-point interpolation)
    """
    ap = 0
    for t in np.arange(0, 1.1, 0.1):  # 0.0, 0.1, 0.2, ..., 1.0
        if np.sum(recalls >= t) == 0:
            p = 0
        else:
            p = np.max(precisions[recalls >= t])
        ap += p / 11
    return ap

def calculate_map(all_aps):
    """
    mean Average Precision ê³„ì‚°
    """
    return np.mean(all_aps)

# mAP ë³€í˜•ë“¤
# mAP@0.5: IoU ì„ê³„ê°’ 0.5ì—ì„œì˜ mAP
# mAP@0.5:0.95: IoU 0.5ë¶€í„° 0.95ê¹Œì§€ 0.05 ê°„ê²©ìœ¼ë¡œ í‰ê· í•œ mAP
# mAP@small/medium/large: ê°ì²´ í¬ê¸°ë³„ mAP
```

---

## 2. R-CNN ê³„ì—´ì˜ ë°œì „ì‚¬

### 2.1 R-CNN (2014): ê°ì²´ íƒì§€ì˜ ì‹œì‘

#### í•µì‹¬ ì•„ì´ë””ì–´
1. **Region Proposal**: Selective Searchë¡œ ê°ì²´ê°€ ìˆì„ ë§Œí•œ ì˜ì—­ ì œì•ˆ
2. **CNN Feature Extraction**: ê° ì˜ì—­ì—ì„œ CNNìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
3. **Classification**: SVMìœ¼ë¡œ ê°ì²´ ë¶„ë¥˜

#### R-CNN êµ¬ì¡°
```python
class RCNN:
    def __init__(self):
        self.region_proposal = SelectiveSearch()
        self.cnn = AlexNet(pretrained=True)
        self.svm_classifiers = {}  # í´ë˜ìŠ¤ë³„ SVM
        self.bbox_regressors = {}  # í´ë˜ìŠ¤ë³„ ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€
    
    def forward(self, image):
        # 1. Region Proposal (ì•½ 2000ê°œ ì˜ì—­)
        regions = self.region_proposal(image)
        
        # 2. ê° ì˜ì—­ì„ 227x227ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        resized_regions = [resize(region, (227, 227)) for region in regions]
        
        # 3. CNNìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
        features = []
        for region in resized_regions:
            feature = self.cnn.extract_features(region)  # 4096-dim
            features.append(feature)
        
        # 4. SVMìœ¼ë¡œ ë¶„ë¥˜
        predictions = []
        for feature in features:
            class_scores = {}
            for class_name, svm in self.svm_classifiers.items():
                score = svm.predict(feature)
                class_scores[class_name] = score
            predictions.append(class_scores)
        
        # 5. ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€
        refined_boxes = []
        for i, (feature, region) in enumerate(zip(features, regions)):
            predicted_class = max(predictions[i], key=predictions[i].get)
            regressor = self.bbox_regressors[predicted_class]
            refined_box = regressor.predict(feature, region)
            refined_boxes.append(refined_box)
        
        return predictions, refined_boxes
```

#### R-CNNì˜ í•œê³„
- **ì†ë„**: ì´ë¯¸ì§€ë‹¹ 47ì´ˆ (GPU ê¸°ì¤€)
- **ë©”ëª¨ë¦¬**: ê° ì˜ì—­ë§ˆë‹¤ CNN ì—°ì‚° í•„ìš”
- **ë³µì¡ì„±**: 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ (Region Proposal â†’ CNN â†’ SVM)

### 2.2 Fast R-CNN (2015): ì†ë„ ê°œì„ 

#### ì£¼ìš” ê°œì„ ì‚¬í•­
1. **ì „ì²´ ì´ë¯¸ì§€ CNN**: ì´ë¯¸ì§€ ì „ì²´ì— í•œ ë²ˆë§Œ CNN ì ìš©
2. **RoI Pooling**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì˜ì—­ì„ ê³ ì • í¬ê¸°ë¡œ ë³€í™˜
3. **Multi-task Loss**: ë¶„ë¥˜ì™€ ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€ë¥¼ ë™ì‹œì— í•™ìŠµ

#### Fast R-CNN êµ¬ì¡°
```python
class FastRCNN:
    def __init__(self, num_classes):
        self.backbone = VGG16(pretrained=True)
        self.roi_pool = RoIPooling(output_size=(7, 7))
        
        # ë¶„ë¥˜ì™€ íšŒê·€ë¥¼ ìœ„í•œ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Linear(7 * 7 * 512, 4096),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(4096, num_classes + 1)  # +1 for background
        )
        
        self.bbox_regressor = nn.Sequential(
            nn.Linear(7 * 7 * 512, 4096),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(4096, 4 * num_classes)  # 4 coordinates per class
        )
    
    def forward(self, image, rois):
        # 1. ì „ì²´ ì´ë¯¸ì§€ì— CNN ì ìš©
        feature_map = self.backbone(image)  # [1, 512, H/16, W/16]
        
        # 2. RoI Pooling
        roi_features = []
        for roi in rois:
            pooled = self.roi_pool(feature_map, roi)  # [512, 7, 7]
            roi_features.append(pooled.flatten())
        
        roi_features = torch.stack(roi_features)  # [N, 512*7*7]
        
        # 3. ë¶„ë¥˜ ë° ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€
        class_scores = self.classifier(roi_features)  # [N, num_classes+1]
        bbox_deltas = self.bbox_regressor(roi_features)  # [N, 4*num_classes]
        
        return class_scores, bbox_deltas

# RoI Pooling êµ¬í˜„
class RoIPooling(nn.Module):
    def __init__(self, output_size):
        super().__init__()
        self.output_size = output_size
        self.adaptive_pool = nn.AdaptiveMaxPool2d(output_size)
    
    def forward(self, feature_map, roi):
        # roi: [x1, y1, x2, y2] (feature map ì¢Œí‘œê³„)
        x1, y1, x2, y2 = roi
        
        # ê´€ì‹¬ ì˜ì—­ ì¶”ì¶œ
        roi_feature = feature_map[:, :, y1:y2, x1:x2]
        
        # ê³ ì • í¬ê¸°ë¡œ í’€ë§
        pooled = self.adaptive_pool(roi_feature)
        
        return pooled
```

#### ì„±ëŠ¥ ê°œì„ 
- **ì†ë„**: ì´ë¯¸ì§€ë‹¹ 2.3ì´ˆ (9ë°° ë¹ ë¦„)
- **ì •í™•ë„**: mAP 66% (R-CNN ëŒ€ë¹„ í–¥ìƒ)

### 2.3 Faster R-CNN (2015): End-to-End í•™ìŠµ

#### í˜ì‹ ì  ì•„ì´ë””ì–´: RPN (Region Proposal Network)
```python
class RegionProposalNetwork(nn.Module):
    def __init__(self, in_channels=512, num_anchors=9):
        super().__init__()
        
        # 3x3 ì»¨ë³¼ë£¨ì…˜
        self.conv = nn.Conv2d(in_channels, 512, 3, padding=1)
        
        # ë¶„ë¥˜: ê°ì²´/ë°°ê²½
        self.cls_logits = nn.Conv2d(512, num_anchors * 2, 1)
        
        # íšŒê·€: ë°”ìš´ë”© ë°•ìŠ¤ ì¡°ì •
        self.bbox_pred = nn.Conv2d(512, num_anchors * 4, 1)
        
        # ì•µì»¤ ìƒì„±ê¸°
        self.anchor_generator = AnchorGenerator(
            scales=[8, 16, 32],  # 3ê°œ ìŠ¤ì¼€ì¼
            ratios=[0.5, 1.0, 2.0]  # 3ê°œ ë¹„ìœ¨
        )  # ì´ 9ê°œ ì•µì»¤ per position
    
    def forward(self, feature_map):
        batch_size, _, H, W = feature_map.shape
        
        # íŠ¹ì§• ì¶”ì¶œ
        x = F.relu(self.conv(feature_map))
        
        # ë¶„ë¥˜ ì ìˆ˜
        cls_logits = self.cls_logits(x)  # [B, 18, H, W]
        cls_logits = cls_logits.view(batch_size, 2, -1)  # [B, 2, H*W*9]
        
        # ë°”ìš´ë”© ë°•ìŠ¤ íšŒê·€
        bbox_pred = self.bbox_pred(x)  # [B, 36, H, W]
        bbox_pred = bbox_pred.view(batch_size, 4, -1)  # [B, 4, H*W*9]
        
        # ì•µì»¤ ìƒì„±
        anchors = self.anchor_generator(feature_map.shape[-2:])
        
        return cls_logits, bbox_pred, anchors

class AnchorGenerator:
    def __init__(self, scales, ratios, stride=16):
        self.scales = scales
        self.ratios = ratios
        self.stride = stride
    
    def __call__(self, feature_size):
        H, W = feature_size
        anchors = []
        
        for y in range(H):
            for x in range(W):
                # íŠ¹ì§• ë§µ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                center_x = x * self.stride
                center_y = y * self.stride
                
                for scale in self.scales:
                    for ratio in self.ratios:
                        # ì•µì»¤ í¬ê¸° ê³„ì‚°
                        w = scale * np.sqrt(ratio)
                        h = scale / np.sqrt(ratio)
                        
                        # ì•µì»¤ ë°•ìŠ¤ ì¢Œí‘œ
                        x1 = center_x - w / 2
                        y1 = center_y - h / 2
                        x2 = center_x + w / 2
                        y2 = center_y + h / 2
                        
                        anchors.append([x1, y1, x2, y2])
        
        return torch.tensor(anchors)
```

#### Faster R-CNN ì „ì²´ êµ¬ì¡°
```python
class FasterRCNN(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        
        # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬
        self.backbone = ResNet50(pretrained=True)
        
        # RPN
        self.rpn = RegionProposalNetwork()
        
        # Fast R-CNN í—¤ë“œ
        self.roi_head = FastRCNNHead(num_classes)
    
    def forward(self, images, targets=None):
        # 1. íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(images)
        
        # 2. RPNìœ¼ë¡œ ê°ì²´ ì œì•ˆ
        rpn_cls, rpn_bbox, anchors = self.rpn(features)
        
        # 3. ì œì•ˆëœ ì˜ì—­ ì„ ë³„ (NMS ì ìš©)
        proposals = self.generate_proposals(rpn_cls, rpn_bbox, anchors)
        
        # 4. Fast R-CNNìœ¼ë¡œ ìµœì¢… ë¶„ë¥˜ ë° íšŒê·€
        if self.training:
            # í•™ìŠµ ì‹œ: Ground Truthì™€ ë§¤ì¹­
            proposals = self.assign_targets(proposals, targets)
        
        cls_scores, bbox_preds = self.roi_head(features, proposals)
        
        return cls_scores, bbox_preds, proposals
```

#### ì„±ëŠ¥ ê°œì„ 
- **ì†ë„**: ì´ë¯¸ì§€ë‹¹ 0.2ì´ˆ (ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥)
- **ì •í™•ë„**: mAP 73.2%
- **End-to-End**: ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ í•œ ë²ˆì— í•™ìŠµ

---

## 3. One-stage vs Two-stage Detectors

### 3.1 Two-stage Detectorsì˜ íŠ¹ì§•

#### ì¥ì 
- **ë†’ì€ ì •í™•ë„**: ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì •ë°€í•œ íƒì§€
- **ì•ˆì •ì  ì„±ëŠ¥**: ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥
- **ì‘ì€ ê°ì²´ íƒì§€**: ì‘ì€ ê°ì²´ë„ ì˜ íƒì§€

#### ë‹¨ì 
- **ëŠë¦° ì†ë„**: ë‘ ë²ˆì˜ ë„¤íŠ¸ì›Œí¬ í†µê³¼ í•„ìš”
- **ë³µì¡í•œ êµ¬ì¡°**: RPN + Detection Head
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ë§ì€ ì¤‘ê°„ ê²°ê³¼ ì €ì¥

### 3.2 One-stage Detectorsì˜ ë“±ì¥

#### í•µì‹¬ ì•„ì´ë””ì–´
> "Region Proposal ë‹¨ê³„ë¥¼ ì—†ì• ê³  í•œ ë²ˆì— ê°ì²´ë¥¼ íƒì§€í•˜ì!"

#### ëŒ€í‘œì ì¸ One-stage Detectors
1. **YOLO (You Only Look Once)**
2. **SSD (Single Shot MultiBox Detector)**
3. **RetinaNet**

### 3.3 YOLOì˜ í˜ì‹ 

#### YOLO v1 (2016)ì˜ í•µì‹¬ ê°œë…
```python
class YOLOv1(nn.Module):
    def __init__(self, num_classes=20, grid_size=7):
        super().__init__()
        self.num_classes = num_classes
        self.grid_size = grid_size
        self.num_boxes = 2  # ê·¸ë¦¬ë“œ ì…€ë‹¹ ì˜ˆì¸¡í•  ë°•ìŠ¤ ìˆ˜
        
        # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ (Darknet-19 ê¸°ë°˜)
        self.backbone = self.build_backbone()
        
        # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´
        # ì¶œë ¥ í¬ê¸°: SÃ—SÃ—(BÃ—5 + C)
        # S=7, B=2, C=20 â†’ 7Ã—7Ã—30
        output_size = grid_size * grid_size * (self.num_boxes * 5 + num_classes)
        self.fc = nn.Sequential(
            nn.Linear(1024 * 7 * 7, 4096),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.5),
            nn.Linear(4096, output_size)
        )
    
    def forward(self, x):
        # ë°±ë³¸ì„ í†µí•œ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)  # [B, 1024, 7, 7]
        
        # í‰íƒ„í™” ë° ì™„ì „ì—°ê²°ì¸µ
        features = features.view(features.size(0), -1)
        output = self.fc(features)
        
        # ì¶œë ¥ ì¬êµ¬ì„±: [B, S, S, (BÃ—5 + C)]
        batch_size = x.size(0)
        output = output.view(batch_size, self.grid_size, self.grid_size, -1)
        
        return output
    
    def decode_predictions(self, predictions):
        """
        YOLO ì¶œë ¥ì„ ë°”ìš´ë”© ë°•ìŠ¤ì™€ í´ë˜ìŠ¤ í™•ë¥ ë¡œ ë³€í™˜
        """
        batch_size, S, S, _ = predictions.shape
        
        # ì¶œë ¥ ë¶„í•´
        # Box 1: [x, y, w, h, confidence]
        # Box 2: [x, y, w, h, confidence]  
        # Class probabilities: [C1, C2, ..., C20]
        
        boxes = predictions[:, :, :, :self.num_boxes * 5]  # [B, S, S, 10]
        class_probs = predictions[:, :, :, self.num_boxes * 5:]  # [B, S, S, 20]
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë³€í™˜ (ê·¸ë¦¬ë“œ ìƒëŒ€ ì¢Œí‘œ â†’ ì ˆëŒ€ ì¢Œí‘œ)
        decoded_boxes = []
        
        for i in range(S):
            for j in range(S):
                for b in range(self.num_boxes):
                    # ë°•ìŠ¤ ì •ë³´ ì¶”ì¶œ
                    box_idx = b * 5
                    x = boxes[:, i, j, box_idx]      # ê·¸ë¦¬ë“œ ì…€ ë‚´ ìƒëŒ€ x
                    y = boxes[:, i, j, box_idx + 1]  # ê·¸ë¦¬ë“œ ì…€ ë‚´ ìƒëŒ€ y
                    w = boxes[:, i, j, box_idx + 2]  # ì „ì²´ ì´ë¯¸ì§€ ëŒ€ë¹„ ë„ˆë¹„
                    h = boxes[:, i, j, box_idx + 3]  # ì „ì²´ ì´ë¯¸ì§€ ëŒ€ë¹„ ë†’ì´
                    conf = boxes[:, i, j, box_idx + 4]  # ì‹ ë¢°ë„
                    
                    # ì ˆëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
                    center_x = (j + x) / S  # 0~1 ë²”ìœ„
                    center_y = (i + y) / S  # 0~1 ë²”ìœ„
                    
                    decoded_boxes.append({
                        'center_x': center_x,
                        'center_y': center_y,
                        'width': w,
                        'height': h,
                        'confidence': conf,
                        'grid_i': i,
                        'grid_j': j,
                        'box_id': b
                    })
        
        return decoded_boxes, class_probs
```

#### YOLO Loss Function
```python
def yolo_loss(predictions, targets, lambda_coord=5, lambda_noobj=0.5):
    """
    YOLO v1 ì†ì‹¤ í•¨ìˆ˜
    
    Args:
        predictions: [B, S, S, (BÃ—5 + C)] ëª¨ë¸ ì¶œë ¥
        targets: Ground truth ì •ë³´
        lambda_coord: ì¢Œí‘œ ì†ì‹¤ ê°€ì¤‘ì¹˜
        lambda_noobj: ê°ì²´ ì—†ëŠ” ì…€ì˜ ì‹ ë¢°ë„ ì†ì‹¤ ê°€ì¤‘ì¹˜
    """
    
    # 1. ì¢Œí‘œ ì†ì‹¤ (Coordinate Loss)
    coord_loss = 0
    for target in targets:
        if target['has_object']:
            # ì¤‘ì‹¬ì  ì†ì‹¤
            coord_loss += (pred_x - target_x)**2 + (pred_y - target_y)**2
            
            # í¬ê¸° ì†ì‹¤ (ì œê³±ê·¼ ì‚¬ìš©ìœ¼ë¡œ í° ë°•ìŠ¤ì™€ ì‘ì€ ë°•ìŠ¤ ê· í˜•)
            coord_loss += (sqrt(pred_w) - sqrt(target_w))**2 + (sqrt(pred_h) - sqrt(target_h))**2
    
    # 2. ì‹ ë¢°ë„ ì†ì‹¤ (Confidence Loss)
    conf_loss_obj = 0    # ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
    conf_loss_noobj = 0  # ê°ì²´ê°€ ì—†ëŠ” ê²½ìš°
    
    for i in range(S):
        for j in range(S):
            for b in range(B):
                if grid_has_object[i][j]:
                    # ê°ì²´ê°€ ìˆëŠ” ê·¸ë¦¬ë“œ: ì‹ ë¢°ë„ë¥¼ IoUì— ë§ì¶”ê¸°
                    target_conf = calculate_iou(pred_box, true_box)
                    conf_loss_obj += (pred_conf - target_conf)**2
                else:
                    # ê°ì²´ê°€ ì—†ëŠ” ê·¸ë¦¬ë“œ: ì‹ ë¢°ë„ë¥¼ 0ì— ë§ì¶”ê¸°
                    conf_loss_noobj += pred_conf**2
    
    # 3. ë¶„ë¥˜ ì†ì‹¤ (Classification Loss)
    class_loss = 0
    for target in targets:
        if target['has_object']:
            class_loss += sum((pred_class_prob - target_class)**2)
    
    # ì´ ì†ì‹¤
    total_loss = (lambda_coord * coord_loss + 
                  conf_loss_obj + 
                  lambda_noobj * conf_loss_noobj + 
                  class_loss)
    
    return total_loss
```

---

## 4. YOLO ì•„í‚¤í…ì²˜ì˜ ë°œì „

### 4.1 YOLOv2/YOLO9000 (2017)

#### ì£¼ìš” ê°œì„ ì‚¬í•­
1. **Batch Normalization**: ëª¨ë“  ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì— ì¶”ê°€
2. **High Resolution Classifier**: 448Ã—448 í•´ìƒë„ë¡œ ì‚¬ì „ í›ˆë ¨
3. **Anchor Boxes**: Faster R-CNNì˜ ì•µì»¤ ê°œë… ë„ì…
4. **Dimension Clusters**: K-meansë¡œ ìµœì  ì•µì»¤ í¬ê¸° ê²°ì •

```python
class YOLOv2(nn.Module):
    def __init__(self, num_classes=80, num_anchors=5):
        super().__init__()
        self.num_classes = num_classes
        self.num_anchors = num_anchors
        
        # Darknet-19 ë°±ë³¸
        self.backbone = Darknet19()
        
        # ê²€ì¶œ í—¤ë“œ
        self.detection_head = nn.Sequential(
            nn.Conv2d(1024, 1024, 3, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),
            nn.Conv2d(1024, 1024, 3, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1),
            nn.Conv2d(1024, num_anchors * (5 + num_classes), 1)
        )
        
        # ì•µì»¤ ë°•ìŠ¤ (K-meansë¡œ ê²°ì •ëœ í¬ê¸°)
        self.anchors = [
            (0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434),
            (7.88282, 3.52778), (9.77052, 9.16828)
        ]
    
    def forward(self, x):
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)  # [B, 1024, 13, 13]
        
        # ê²€ì¶œ í—¤ë“œ ì ìš©
        output = self.detection_head(features)  # [B, 425, 13, 13]
        
        # ì¶œë ¥ ì¬êµ¬ì„±: [B, 13, 13, 5, 85]
        batch_size = x.size(0)
        grid_size = output.size(-1)
        output = output.view(batch_size, self.num_anchors, 
                           5 + self.num_classes, grid_size, grid_size)
        output = output.permute(0, 3, 4, 1, 2)  # [B, 13, 13, 5, 85]
        
        return output

# K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì•µì»¤ í¬ê¸° ê²°ì •
def generate_anchors(annotations, num_anchors=5):
    """
    K-meansë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ì•µì»¤ ë°•ìŠ¤ í¬ê¸° ê²°ì •
    """
    # ëª¨ë“  ë°”ìš´ë”© ë°•ìŠ¤ì˜ ë„ˆë¹„, ë†’ì´ ìˆ˜ì§‘
    boxes = []
    for ann in annotations:
        for bbox in ann['bboxes']:
            w, h = bbox[2], bbox[3]  # ì •ê·œí™”ëœ ë„ˆë¹„, ë†’ì´
            boxes.append([w, h])
    
    boxes = np.array(boxes)
    
    # K-means í´ëŸ¬ìŠ¤í„°ë§
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=num_anchors, random_state=42)
    kmeans.fit(boxes)
    
    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì ì´ ì•µì»¤ í¬ê¸°
    anchors = kmeans.cluster_centers_
    
    return anchors.tolist()
```

### 4.2 YOLOv3 (2018): ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ê²€ì¶œ

#### í•µì‹¬ í˜ì‹ : Feature Pyramid Network (FPN)
```python
class YOLOv3(nn.Module):
    def __init__(self, num_classes=80):
        super().__init__()
        self.num_classes = num_classes
        
        # Darknet-53 ë°±ë³¸
        self.backbone = Darknet53()
        
        # 3ê°œ ìŠ¤ì¼€ì¼ì—ì„œ ê²€ì¶œ
        self.detection_layers = nn.ModuleList([
            self.make_detection_layer(1024, 512),  # 13Ã—13
            self.make_detection_layer(768, 256),   # 26Ã—26  
            self.make_detection_layer(384, 128),   # 52Ã—52
        ])
        
        # ê° ìŠ¤ì¼€ì¼ë³„ ì•µì»¤ (ì´ 9ê°œ)
        self.anchors = [
            [(116, 90), (156, 198), (373, 326)],    # í° ê°ì²´ìš©
            [(30, 61), (62, 45), (59, 119)],        # ì¤‘ê°„ ê°ì²´ìš©
            [(10, 13), (16, 30), (33, 23)]          # ì‘ì€ ê°ì²´ìš©
        ]
    
    def make_detection_layer(self, in_channels, mid_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, 1),
            nn.BatchNorm2d(mid_channels),
            nn.LeakyReLU(0.1),
            nn.Conv2d(mid_channels, mid_channels * 2, 3, padding=1),
            nn.BatchNorm2d(mid_channels * 2),
            nn.LeakyReLU(0.1),
            nn.Conv2d(mid_channels * 2, 3 * (5 + self.num_classes), 1)
        )
    
    def forward(self, x):
        # ë°±ë³¸ì„ í†µí•œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ
        features = self.backbone(x)
        
        # featuresëŠ” 3ê°œ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì§• ë§µ ë¦¬ìŠ¤íŠ¸
        # features[0]: [B, 1024, 13, 13] - í° ê°ì²´ìš©
        # features[1]: [B, 512, 26, 26]  - ì¤‘ê°„ ê°ì²´ìš©  
        # features[2]: [B, 256, 52, 52]  - ì‘ì€ ê°ì²´ìš©
        
        detections = []
        
        for i, (feature, detection_layer) in enumerate(zip(features, self.detection_layers)):
            # ê° ìŠ¤ì¼€ì¼ì—ì„œ ê²€ì¶œ ìˆ˜í–‰
            detection = detection_layer(feature)
            
            # ì¶œë ¥ ì¬êµ¬ì„±
            batch_size, _, grid_h, grid_w = detection.shape
            detection = detection.view(batch_size, 3, 5 + self.num_classes, grid_h, grid_w)
            detection = detection.permute(0, 3, 4, 1, 2)  # [B, H, W, 3, 85]
            
            detections.append(detection)
        
        return detections

class Darknet53(nn.Module):
    """
    YOLOv3ì˜ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬
    ResNetì˜ ì”ì°¨ ì—°ê²°ì„ ë„ì…í•œ Darknet
    """
    def __init__(self):
        super().__init__()
        
        # ì´ˆê¸° ë ˆì´ì–´ë“¤
        self.conv1 = self.conv_bn_leaky(3, 32, 3)
        self.conv2 = self.conv_bn_leaky(32, 64, 3, stride=2)
        
        # ì”ì°¨ ë¸”ë¡ë“¤
        self.res_block1 = self.make_layer(64, 32, 1)
        self.conv3 = self.conv_bn_leaky(64, 128, 3, stride=2)
        self.res_block2 = self.make_layer(128, 64, 2)
        self.conv4 = self.conv_bn_leaky(128, 256, 3, stride=2)
        self.res_block3 = self.make_layer(256, 128, 8)
        self.conv5 = self.conv_bn_leaky(256, 512, 3, stride=2)
        self.res_block4 = self.make_layer(512, 256, 8)
        self.conv6 = self.conv_bn_leaky(512, 1024, 3, stride=2)
        self.res_block5 = self.make_layer(1024, 512, 4)
    
    def conv_bn_leaky(self, in_channels, out_channels, kernel_size, stride=1):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, 
                     stride=stride, padding=kernel_size//2, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.1, inplace=True)
        )
    
    def make_layer(self, in_channels, mid_channels, num_blocks):
        layers = []
        for _ in range(num_blocks):
            layers.append(ResidualBlock(in_channels, mid_channels))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.res_block1(x)
        x = self.conv3(x)
        x = self.res_block2(x)
        x = self.conv4(x)
        route1 = self.res_block3(x)  # 52Ã—52 íŠ¹ì§• (ì‘ì€ ê°ì²´ìš©)
        x = self.conv5(route1)
        route2 = self.res_block4(x)  # 26Ã—26 íŠ¹ì§• (ì¤‘ê°„ ê°ì²´ìš©)
        x = self.conv6(route2)
        route3 = self.res_block5(x)  # 13Ã—13 íŠ¹ì§• (í° ê°ì²´ìš©)
        
        return [route3, route2, route1]

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, mid_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, mid_channels, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_channels)
        self.conv2 = nn.Conv2d(mid_channels, in_channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(in_channels)
        self.leaky_relu = nn.LeakyReLU(0.1, inplace=True)
    
    def forward(self, x):
        residual = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.leaky_relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        out += residual
        out = self.leaky_relu(out)
        
        return out
```

### 4.3 YOLOv4 (2020): ìµœì í™”ì˜ ì§‘ëŒ€ì„±

#### ì£¼ìš” ê°œì„ ì‚¬í•­
1. **CSPDarknet53**: Cross Stage Partial ì—°ê²°
2. **PANet**: Path Aggregation Network
3. **Mosaic Data Augmentation**: 4ê°œ ì´ë¯¸ì§€ ì¡°í•©
4. **CIoU Loss**: Complete IoU ì†ì‹¤ í•¨ìˆ˜

```python
# Mosaic Data Augmentation
def mosaic_augmentation(images, labels, input_size=416):
    """
    4ê°œ ì´ë¯¸ì§€ë¥¼ ì¡°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ëª¨ìì´í¬ ì´ë¯¸ì§€ ìƒì„±
    """
    # 4ê°œ ì´ë¯¸ì§€ ì„ íƒ
    indices = np.random.choice(len(images), 4, replace=False)
    
    # ëª¨ìì´í¬ ì´ë¯¸ì§€ ì´ˆê¸°í™”
    mosaic_img = np.zeros((input_size, input_size, 3), dtype=np.uint8)
    mosaic_labels = []
    
    # ì¤‘ì‹¬ì  ëœë¤ ì„ íƒ (ì´ë¯¸ì§€ í¬ê¸°ì˜ 0.5~1.5 ë²”ìœ„)
    center_x = int(np.random.uniform(0.5, 1.5) * input_size // 2)
    center_y = int(np.random.uniform(0.5, 1.5) * input_size // 2)
    
    for i, idx in enumerate(indices):
        img = images[idx]
        label = labels[idx]
        
        # ê° ì´ë¯¸ì§€ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        h, w = img.shape[:2]
        scale = min(input_size / h, input_size / w)
        new_h, new_w = int(h * scale), int(w * scale)
        img_resized = cv2.resize(img, (new_w, new_h))
        
        # 4ê°œ ì˜ì—­ ì¤‘ í•˜ë‚˜ì— ë°°ì¹˜
        if i == 0:  # ì¢Œìƒë‹¨
            x1, y1 = max(center_x - new_w, 0), max(center_y - new_h, 0)
            x2, y2 = center_x, center_y
        elif i == 1:  # ìš°ìƒë‹¨
            x1, y1 = center_x, max(center_y - new_h, 0)
            x2, y2 = min(center_x + new_w, input_size), center_y
        elif i == 2:  # ì¢Œí•˜ë‹¨
            x1, y1 = max(center_x - new_w, 0), center_y
            x2, y2 = center_x, min(center_y + new_h, input_size)
        else:  # ìš°í•˜ë‹¨
            x1, y1 = center_x, center_y
            x2, y2 = min(center_x + new_w, input_size), min(center_y + new_h, input_size)
        
        # ì´ë¯¸ì§€ ë°°ì¹˜
        mosaic_img[y1:y2, x1:x2] = img_resized[:y2-y1, :x2-x1]
        
        # ë¼ë²¨ ì¢Œí‘œ ì¡°ì •
        for bbox in label:
            # ì›ë³¸ ì¢Œí‘œë¥¼ ëª¨ìì´í¬ ì¢Œí‘œë¡œ ë³€í™˜
            bbox_x1 = bbox[0] * new_w + x1
            bbox_y1 = bbox[1] * new_h + y1
            bbox_x2 = bbox[2] * new_w + x1
            bbox_y2 = bbox[3] * new_h + y1
            
            # í´ë¦¬í•‘
            bbox_x1 = max(0, min(bbox_x1, input_size))
            bbox_y1 = max(0, min(bbox_y1, input_size))
            bbox_x2 = max(0, min(bbox_x2, input_size))
            bbox_y2 = max(0, min(bbox_y2, input_size))
            
            # ìœ íš¨í•œ ë°•ìŠ¤ë§Œ ì¶”ê°€
            if bbox_x2 > bbox_x1 and bbox_y2 > bbox_y1:
                mosaic_labels.append([bbox_x1, bbox_y1, bbox_x2, bbox_y2, bbox[4]])
    
    return mosaic_img, mosaic_labels

# CIoU Loss
def ciou_loss(pred_boxes, target_boxes):
    """
    Complete IoU Loss
    IoU + ì¤‘ì‹¬ì  ê±°ë¦¬ + ì¢…íš¡ë¹„ ì¼ê´€ì„±ì„ ëª¨ë‘ ê³ ë ¤
    """
    # ê¸°ë³¸ IoU ê³„ì‚°
    iou = calculate_iou(pred_boxes, target_boxes)
    
    # ì¤‘ì‹¬ì  ê±°ë¦¬
    pred_center_x = (pred_boxes[:, 0] + pred_boxes[:, 2]) / 2
    pred_center_y = (pred_boxes[:, 1] + pred_boxes[:, 3]) / 2
    target_center_x = (target_boxes[:, 0] + target_boxes[:, 2]) / 2
    target_center_y = (target_boxes[:, 1] + target_boxes[:, 3]) / 2
    
    center_distance = ((pred_center_x - target_center_x) ** 2 + 
                      (pred_center_y - target_center_y) ** 2)
    
    # ëŒ€ê°ì„  ê±°ë¦¬
    pred_w = pred_boxes[:, 2] - pred_boxes[:, 0]
    pred_h = pred_boxes[:, 3] - pred_boxes[:, 1]
    target_w = target_boxes[:, 2] - target_boxes[:, 0]
    target_h = target_boxes[:, 3] - target_boxes[:, 1]
    
    diagonal_distance = ((torch.max(pred_boxes[:, 2], target_boxes[:, 2]) - 
                         torch.min(pred_boxes[:, 0], target_boxes[:, 0])) ** 2 +
                        (torch.max(pred_boxes[:, 3], target_boxes[:, 3]) - 
                         torch.min(pred_boxes[:, 1], target_boxes[:, 1])) ** 2)
    
    # ì¢…íš¡ë¹„ ì¼ê´€ì„±
    v = (4 / (np.pi ** 2)) * torch.pow(
        torch.atan(target_w / target_h) - torch.atan(pred_w / pred_h), 2)
    
    alpha = v / (1 - iou + v + 1e-8)
    
    # CIoU ê³„ì‚°
    ciou = iou - center_distance / diagonal_distance - alpha * v
    
    # Loss (1 - CIoU)
    loss = 1 - ciou
    
    return loss.mean()
```

### 4.4 YOLOv5 (2020): ì‹¤ìš©ì„± ê°•í™”

#### ì£¼ìš” íŠ¹ì§•
1. **PyTorch êµ¬í˜„**: ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ PyTorch ê¸°ë°˜
2. **AutoAnchor**: ìë™ ì•µì»¤ ìµœì í™”
3. **Model Scaling**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ (n, s, m, l, x)
4. **TTA (Test Time Augmentation)**: ì¶”ë¡  ì‹œ ì¦ê°•

```python
# YOLOv5 ëª¨ë¸ ìŠ¤ì¼€ì¼ë§
class YOLOv5:
    def __init__(self, model_size='s'):
        self.model_configs = {
            'n': {'depth': 0.33, 'width': 0.25},  # nano
            's': {'depth': 0.33, 'width': 0.50},  # small
            'm': {'depth': 0.67, 'width': 0.75},  # medium
            'l': {'depth': 1.00, 'width': 1.00},  # large
            'x': {'depth': 1.33, 'width': 1.25},  # xlarge
        }
        
        config = self.model_configs[model_size]
        self.depth_multiple = config['depth']
        self.width_multiple = config['width']
    
    def scale_model(self, base_channels, base_depth):
        """ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì±„ë„ ìˆ˜ì™€ ê¹Šì´ ì¡°ì •"""
        scaled_channels = int(base_channels * self.width_multiple)
        scaled_depth = max(1, int(base_depth * self.depth_multiple))
        
        return scaled_channels, scaled_depth

# AutoAnchor
class AutoAnchor:
    def __init__(self, dataset, num_anchors=9, img_size=640):
        self.dataset = dataset
        self.num_anchors = num_anchors
        self.img_size = img_size
    
    def generate_anchors(self):
        """ë°ì´í„°ì…‹ ë¶„ì„ì„ í†µí•œ ìë™ ì•µì»¤ ìƒì„±"""
        # ëª¨ë“  ë°”ìš´ë”© ë°•ìŠ¤ ìˆ˜ì§‘
        boxes = []
        for data in self.dataset:
            for bbox in data['bboxes']:
                w = bbox[2] * self.img_size  # ì ˆëŒ€ í¬ê¸°ë¡œ ë³€í™˜
                h = bbox[3] * self.img_size
                boxes.append([w, h])
        
        boxes = np.array(boxes)
        
        # K-means++ í´ëŸ¬ìŠ¤í„°ë§
        anchors = self.kmeans_anchors(boxes, self.num_anchors)
        
        # 3ê°œ ìŠ¤ì¼€ì¼ë¡œ ë¶„í• 
        anchors = anchors[np.argsort(anchors.prod(1))]  # ë©´ì  ê¸°ì¤€ ì •ë ¬
        
        return {
            'small': anchors[:3],    # ì‘ì€ ê°ì²´ìš©
            'medium': anchors[3:6],  # ì¤‘ê°„ ê°ì²´ìš©
            'large': anchors[6:9],   # í° ê°ì²´ìš©
        }
    
    def kmeans_anchors(self, boxes, k):
        """K-meansë¥¼ ì‚¬ìš©í•œ ì•µì»¤ í´ëŸ¬ìŠ¤í„°ë§"""
        from sklearn.cluster import KMeans
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        boxes_norm = boxes / self.img_size
        
        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
        kmeans.fit(boxes_norm)
        
        # ì›ë˜ í¬ê¸°ë¡œ ë³µì›
        anchors = kmeans.cluster_centers_ * self.img_size
        
        return anchors
```

### 4.5 YOLOv8 (2023): ìµœì‹  ê¸°ìˆ  ì§‘ì•½

#### í˜ì‹ ì  ê°œì„ ì‚¬í•­
1. **Anchor-Free**: ì•µì»¤ ë°•ìŠ¤ ì—†ì´ ì§ì ‘ ì˜ˆì¸¡
2. **Decoupled Head**: ë¶„ë¥˜ì™€ íšŒê·€ í—¤ë“œ ë¶„ë¦¬
3. **New Backbone**: CSPDarknet â†’ C2f ëª¨ë“ˆ
4. **Advanced Augmentation**: MixUp, CutMix ë“±

```python
class YOLOv8Head(nn.Module):
    """
    YOLOv8ì˜ Decoupled Head
    ë¶„ë¥˜ì™€ íšŒê·€ë¥¼ ë³„ë„ ë¸Œëœì¹˜ì—ì„œ ì²˜ë¦¬
    """
    def __init__(self, num_classes, in_channels, num_layers=2):
        super().__init__()
        self.num_classes = num_classes
        
        # ë¶„ë¥˜ ë¸Œëœì¹˜
        cls_layers = []
        for i in range(num_layers):
            cls_layers.extend([
                nn.Conv2d(in_channels, in_channels, 3, padding=1),
                nn.BatchNorm2d(in_channels),
                nn.SiLU(inplace=True)
            ])
        cls_layers.append(nn.Conv2d(in_channels, num_classes, 1))
        self.cls_head = nn.Sequential(*cls_layers)
        
        # íšŒê·€ ë¸Œëœì¹˜ (ë°”ìš´ë”© ë°•ìŠ¤ + ê°ì²´ì„±)
        reg_layers = []
        for i in range(num_layers):
            reg_layers.extend([
                nn.Conv2d(in_channels, in_channels, 3, padding=1),
                nn.BatchNorm2d(in_channels),
                nn.SiLU(inplace=True)
            ])
        reg_layers.append(nn.Conv2d(in_channels, 4 + 1, 1))  # 4(bbox) + 1(objectness)
        self.reg_head = nn.Sequential(*reg_layers)
    
    def forward(self, x):
        # ë¶„ë¥˜ ì˜ˆì¸¡
        cls_output = self.cls_head(x)  # [B, num_classes, H, W]
        
        # íšŒê·€ ì˜ˆì¸¡
        reg_output = self.reg_head(x)  # [B, 5, H, W]
        
        return cls_output, reg_output

# Anchor-Free ì˜ˆì¸¡ ë””ì½”ë”©
def decode_yolov8_predictions(cls_output, reg_output, stride):
    """
    YOLOv8 ì•µì»¤ í”„ë¦¬ ì˜ˆì¸¡ ë””ì½”ë”©
    """
    batch_size, num_classes, H, W = cls_output.shape
    
    # ê·¸ë¦¬ë“œ ìƒì„±
    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')
    grid = torch.stack([grid_x, grid_y], dim=-1).float()  # [H, W, 2]
    
    # íšŒê·€ ì¶œë ¥ ë¶„í•´
    bbox_pred = reg_output[:, :4]      # [B, 4, H, W] - ë°”ìš´ë”© ë°•ìŠ¤
    obj_pred = reg_output[:, 4:5]      # [B, 1, H, W] - ê°ì²´ì„±
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ë””ì½”ë”© (ltrb â†’ xyxy)
    bbox_pred = bbox_pred.permute(0, 2, 3, 1)  # [B, H, W, 4]
    
    # ê±°ë¦¬ ê¸°ë°˜ ì˜ˆì¸¡ì„ ì ˆëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
    lt = grid.unsqueeze(0) - bbox_pred[..., :2]  # left, top
    rb = grid.unsqueeze(0) + bbox_pred[..., 2:]  # right, bottom
    
    # ìµœì¢… ë°”ìš´ë”© ë°•ìŠ¤ (í”½ì…€ ì¢Œí‘œ)
    bbox_final = torch.cat([lt, rb], dim=-1) * stride
    
    # í´ë˜ìŠ¤ í™•ë¥ ê³¼ ê°ì²´ì„± ê²°í•©
    cls_prob = torch.sigmoid(cls_output).permute(0, 2, 3, 1)  # [B, H, W, num_classes]
    obj_prob = torch.sigmoid(obj_pred).permute(0, 2, 3, 1)   # [B, H, W, 1]
    
    # ìµœì¢… ì‹ ë¢°ë„ = í´ë˜ìŠ¤ í™•ë¥  Ã— ê°ì²´ì„±
    confidence = cls_prob * obj_prob
    
    return bbox_final, confidence
```

---

## 5. NMS (Non-Maximum Suppression)

### 5.1 NMSì˜ í•„ìš”ì„±

#### ë¬¸ì œì : ì¤‘ë³µ ê²€ì¶œ
```python
# ê°ì²´ íƒì§€ ê²°ê³¼ ì˜ˆì‹œ (ì¤‘ë³µ ê²€ì¶œ)
detections = [
    {"bbox": [100, 100, 200, 200], "class": "person", "confidence": 0.9},
    {"bbox": [105, 95, 205, 195], "class": "person", "confidence": 0.85},
    {"bbox": [98, 102, 198, 202], "class": "person", "confidence": 0.8},
    {"bbox": [300, 150, 400, 250], "class": "car", "confidence": 0.95},
]
# â†’ ê°™ì€ ì‚¬ëŒì„ 3ë²ˆ ê²€ì¶œ!
```

### 5.2 NMS ì•Œê³ ë¦¬ì¦˜

```python
def non_max_suppression(detections, iou_threshold=0.5, conf_threshold=0.5):
    """
    Non-Maximum Suppression êµ¬í˜„
    
    Args:
        detections: ê²€ì¶œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        iou_threshold: IoU ì„ê³„ê°’ (ê²¹ì¹¨ ì •ë„)
        conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
    
    Returns:
        filtered_detections: NMS ì ìš© í›„ ê²°ê³¼
    """
    
    # 1. ì‹ ë¢°ë„ ì„ê³„ê°’ ì´í•˜ ì œê±°
    detections = [det for det in detections if det['confidence'] >= conf_threshold]
    
    if not detections:
        return []
    
    # 2. ì‹ ë¢°ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    detections.sort(key=lambda x: x['confidence'], reverse=True)
    
    # 3. í´ë˜ìŠ¤ë³„ë¡œ NMS ì ìš©
    final_detections = []
    classes = set(det['class'] for det in detections)
    
    for class_name in classes:
        # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ê²€ì¶œ ê²°ê³¼ë§Œ ì¶”ì¶œ
        class_detections = [det for det in detections if det['class'] == class_name]
        
        # NMS ì ìš©
        keep = []
        while class_detections:
            # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ê²€ì¶œ ê²°ê³¼ ì„ íƒ
            best = class_detections.pop(0)
            keep.append(best)
            
            # ë‚˜ë¨¸ì§€ì™€ IoU ê³„ì‚°í•˜ì—¬ ê²¹ì¹˜ëŠ” ê²ƒë“¤ ì œê±°
            remaining = []
            for det in class_detections:
                iou = calculate_iou(best['bbox'], det['bbox'])
                if iou <= iou_threshold:
                    remaining.append(det)
            
            class_detections = remaining
        
        final_detections.extend(keep)
    
    return final_detections

# ì‚¬ìš© ì˜ˆì‹œ
filtered_results = non_max_suppression(
    detections, 
    iou_threshold=0.5,
    conf_threshold=0.5
)
print(f"ì›ë³¸: {len(detections)}ê°œ â†’ NMS í›„: {len(filtered_results)}ê°œ")
```

### 5.3 ê³ ê¸‰ NMS ê¸°ë²•

#### Soft NMS
```python
def soft_nms(detections, sigma=0.5, iou_threshold=0.3, score_threshold=0.001):
    """
    Soft NMS: ê²¹ì¹˜ëŠ” ë°•ìŠ¤ì˜ ì ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì§€ ì•Šê³  ê°ì†Œì‹œí‚´
    """
    detections = detections.copy()
    
    for i in range(len(detections)):
        if detections[i]['confidence'] < score_threshold:
            continue
            
        for j in range(i + 1, len(detections)):
            if detections[i]['class'] != detections[j]['class']:
                continue
                
            iou = calculate_iou(detections[i]['bbox'], detections[j]['bbox'])
            
            if iou > iou_threshold:
                # ê°€ìš°ì‹œì•ˆ ê°€ì¤‘ì¹˜ ì ìš©
                weight = np.exp(-(iou ** 2) / sigma)
                detections[j]['confidence'] *= weight
    
    # ìµœì¢… ì„ê³„ê°’ ì´í•˜ ì œê±°
    return [det for det in detections if det['confidence'] >= score_threshold]

# DIoU-NMS (Distance-IoU NMS)
def diou_nms(detections, iou_threshold=0.5):
    """
    DIoUë¥¼ ì‚¬ìš©í•œ NMS (ì¤‘ì‹¬ì  ê±°ë¦¬ë„ ê³ ë ¤)
    """
    def calculate_diou(box1, box2):
        # ê¸°ë³¸ IoU
        iou = calculate_iou(box1, box2)
        
        # ì¤‘ì‹¬ì  ê±°ë¦¬
        center1_x = (box1[0] + box1[2]) / 2
        center1_y = (box1[1] + box1[3]) / 2
        center2_x = (box2[0] + box2[2]) / 2
        center2_y = (box2[1] + box2[3]) / 2
        
        center_distance = ((center1_x - center2_x) ** 2 + 
                          (center1_y - center2_y) ** 2)
        
        # ëŒ€ê°ì„  ê±°ë¦¬
        diagonal_distance = ((max(box1[2], box2[2]) - min(box1[0], box2[0])) ** 2 +
                           (max(box1[3], box2[3]) - min(box1[1], box2[1])) ** 2)
        
        # DIoU
        diou = iou - center_distance / diagonal_distance
        
        return diou
    
    # DIoUë¥¼ ì‚¬ìš©í•˜ì—¬ NMS ì ìš©
    # (êµ¬í˜„ì€ ê¸°ë³¸ NMSì™€ ìœ ì‚¬í•˜ì§€ë§Œ IoU ëŒ€ì‹  DIoU ì‚¬ìš©)
    pass
```

---

## 6. ì‹¤ìŠµ í”„ë¡œì íŠ¸: êµì‹¤ ë¬¼ê±´ íƒì§€ê¸°

### 6.1 í”„ë¡œì íŠ¸ ê°œìš”

#### ëª©í‘œ
êµì‹¤ì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” 5ê°€ì§€ ë¬¼ê±´ì„ íƒì§€í•˜ëŠ” ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ êµ¬ì¶•

#### íƒì§€ ëŒ€ìƒ í´ë˜ìŠ¤
1. **ì±… (Book)**
2. **ë…¸íŠ¸ë¶ (Laptop)**
3. **ì˜ì (Chair)**
4. **ì¹ íŒ (Whiteboard)**
5. **ê°€ë°© (Bag)**

### 6.2 ë°ì´í„°ì…‹ ì¤€ë¹„ ì „ëµ

#### Roboflowë¥¼ í™œìš©í•œ ë°ì´í„° ìˆ˜ì§‘
```python
# ë°ì´í„° ìˆ˜ì§‘ ê³„íš
data_collection_plan = {
    "sources": [
        "ì§ì ‘ ì´¬ì˜ (êµì‹¤, ë„ì„œê´€, ì¹´í˜)",
        "ì˜¤í”ˆ ë°ì´í„°ì…‹ (Open Images, COCO)",
        "ì›¹ í¬ë¡¤ë§ (ì €ì‘ê¶Œ ì£¼ì˜)",
        "ë°ì´í„° ì¦ê°•"
    ],
    
    "target_counts": {
        "book": 500,
        "laptop": 400,
        "chair": 600,
        "whiteboard": 300,
        "bag": 450
    },
    
    "annotation_guidelines": {
        "bbox_quality": "ê°ì²´ ì „ì²´ë¥¼ í¬í•¨í•˜ë˜ ì—¬ë°± ìµœì†Œí™”",
        "occlusion": "50% ì´ìƒ ê°€ë ¤ì§„ ê°ì²´ëŠ” ì œì™¸",
        "size_limit": "ì´ë¯¸ì§€ í¬ê¸°ì˜ 1% ì´ìƒì¸ ê°ì²´ë§Œ í¬í•¨",
        "edge_cases": "ë¶€ë¶„ì ìœ¼ë¡œ ì˜ë¦° ê°ì²´ë„ í¬í•¨"
    }
}
```

#### ë°ì´í„° ì¦ê°• ì „ëµ
```python
import albumentations as A

def create_augmentation_pipeline():
    """
    êµì‹¤ ë¬¼ê±´ íƒì§€ë¥¼ ìœ„í•œ ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸
    """
    return A.Compose([
        # ê¸°í•˜í•™ì  ë³€í™˜
        A.HorizontalFlip(p=0.5),
        A.RandomRotate90(p=0.3),
        A.ShiftScaleRotate(
            shift_limit=0.1,
            scale_limit=0.2,
            rotate_limit=15,
            p=0.5
        ),
        
        # ìƒ‰ìƒ ë³€í™˜
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=0.5
        ),
        A.HueSaturationValue(
            hue_shift_limit=20,
            sat_shift_limit=30,
            val_shift_limit=20,
            p=0.5
        ),
        
        # ë…¸ì´ì¦ˆ ë° ë¸”ëŸ¬
        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
        A.GaussianBlur(blur_limit=(3, 7), p=0.3),
        
        # ì¡°ëª… ë³€í™”
        A.RandomShadow(p=0.3),
        A.RandomSunFlare(p=0.2),
        
        # ì»·ì•„ì›ƒ
        A.CoarseDropout(
            max_holes=8,
            max_height=32,
            max_width=32,
            p=0.3
        ),
        
    ], bbox_params=A.BboxParams(
        format='yolo',
        label_fields=['class_labels']
    ))

# ì‚¬ìš© ì˜ˆì‹œ
augmentation = create_augmentation_pipeline()

def augment_image(image, bboxes, class_labels):
    """ì´ë¯¸ì§€ì™€ ë°”ìš´ë”© ë°•ìŠ¤ì— ì¦ê°• ì ìš©"""
    augmented = augmentation(
        image=image,
        bboxes=bboxes,
        class_labels=class_labels
    )
    
    return augmented['image'], augmented['bboxes'], augmented['class_labels']
```

### 6.3 YOLOv8 ì»¤ìŠ¤í…€ í•™ìŠµ

#### ë°ì´í„°ì…‹ êµ¬ì„±
```yaml
# dataset.yaml
path: ./classroom_objects  # ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ
train: images/train
val: images/val
test: images/test

# í´ë˜ìŠ¤ ì •ì˜
nc: 5  # í´ë˜ìŠ¤ ìˆ˜
names: ['book', 'laptop', 'chair', 'whiteboard', 'bag']
```

#### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
```python
from ultralytics import YOLO
import torch

def train_classroom_detector():
    """
    êµì‹¤ ë¬¼ê±´ íƒì§€ê¸° í•™ìŠµ
    """
    
    # 1. ì‚¬ì „í›ˆë ¨ëœ YOLOv8 ëª¨ë¸ ë¡œë“œ
    model = YOLO('yolov8n.pt')  # nano ë²„ì „ (ë¹ ë¥¸ í•™ìŠµ)
    
    # 2. í•™ìŠµ ì„¤ì •
    training_config = {
        'data': 'dataset.yaml',
        'epochs': 100,
        'imgsz': 640,
        'batch': 16,
        'lr0': 0.01,
        'weight_decay': 0.0005,
        'mosaic': 1.0,
        'mixup': 0.1,
        'copy_paste': 0.1,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'workers': 4,
        'project': 'classroom_detector',
        'name': 'yolov8n_classroom',
        'save_period': 10,
        'patience': 20,
        'save': True,
        'plots': True,
        'val': True,
    }
    
    # 3. í•™ìŠµ ì‹¤í–‰
    results = model.train(**training_config)
    
    # 4. ìµœê³  ëª¨ë¸ ì €ì¥
    best_model = YOLO('runs/detect/yolov8n_classroom/weights/best.pt')
    
    return best_model, results

# í•™ìŠµ ì‹¤í–‰
if __name__ == "__main__":
    model, results = train_classroom_detector()
    print("í•™ìŠµ ì™„ë£Œ!")
    print(f"ìµœê³  mAP50: {results.results_dict['metrics/mAP50(B)']:.3f}")
```

#### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```python
def hyperparameter_tuning():
    """
    Ray Tuneì„ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    """
    from ray import tune
    from ray.tune.schedulers import ASHAScheduler
    
    def objective(config):
        model = YOLO('yolov8n.pt')
        
        results = model.train(
            data='dataset.yaml',
            epochs=50,
            imgsz=640,
            batch=config['batch'],
            lr0=config['lr0'],
            weight_decay=config['weight_decay'],
            mosaic=config['mosaic'],
            verbose=False
        )
        
        # mAP50ì„ ìµœëŒ€í™”
        return {"mAP50": results.results_dict['metrics/mAP50(B)']}
    
    # íƒìƒ‰ ê³µê°„ ì •ì˜
    search_space = {
        'batch': tune.choice([8, 16, 32]),
        'lr0': tune.loguniform(1e-4, 1e-1),
        'weight_decay': tune.loguniform(1e-5, 1e-2),
        'mosaic': tune.uniform(0.5, 1.0),
    }
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler = ASHAScheduler(
        metric="mAP50",
        mode="max",
        max_t=50,
        grace_period=10,
        reduction_factor=2
    )
    
    # íŠœë‹ ì‹¤í–‰
    analysis = tune.run(
        objective,
        config=search_space,
        num_samples=20,
        scheduler=scheduler,
        resources_per_trial={"cpu": 2, "gpu": 0.5}
    )
    
    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
    best_config = analysis.best_config
    print("ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:", best_config)
    
    return best_config
```

### 6.4 ëª¨ë¸ í‰ê°€ ë° ë¶„ì„

#### ìƒì„¸ í‰ê°€ ë©”íŠ¸ë¦­
```python
def evaluate_model(model, test_dataset):
    """
    ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ í‰ê°€
    """
    from sklearn.metrics import classification_report, confusion_matrix
    import matplotlib.pyplot as plt
    
    # 1. ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
    results = model.val(data='dataset.yaml', split='test')
    
    # 2. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
    class_names = ['book', 'laptop', 'chair', 'whiteboard', 'bag']
    
    metrics_summary = {
        'overall': {
            'mAP50': results.box.map50,
            'mAP50-95': results.box.map,
            'precision': results.box.mp,
            'recall': results.box.mr,
        },
        'per_class': {}
    }
    
    for i, class_name in enumerate(class_names):
        metrics_summary['per_class'][class_name] = {
            'AP50': results.box.ap50[i],
            'AP50-95': results.box.ap[i],
            'precision': results.box.p[i],
            'recall': results.box.r[i],
        }
    
    # 3. í˜¼ë™ í–‰ë ¬ ìƒì„±
    predictions = []
    ground_truths = []
    
    for image_path in test_dataset:
        pred = model.predict(image_path, verbose=False)[0]
        # ... ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë¼ë²¨ ìˆ˜ì§‘
    
    # 4. ì‹œê°í™”
    plot_evaluation_results(metrics_summary, predictions, ground_truths)
    
    return metrics_summary

def plot_evaluation_results(metrics, predictions, ground_truths):
    """í‰ê°€ ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # í´ë˜ìŠ¤ë³„ AP50 ë¹„êµ
    classes = list(metrics['per_class'].keys())
    ap50_values = [metrics['per_class'][cls]['AP50'] for cls in classes]
    
    axes[0, 0].bar(classes, ap50_values, alpha=0.7)
    axes[0, 0].set_title('í´ë˜ìŠ¤ë³„ AP50')
    axes[0, 0].set_ylabel('AP50')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # Precision-Recall ê³¡ì„ 
    # ... PR ê³¡ì„  ê·¸ë¦¬ê¸°
    
    # í˜¼ë™ í–‰ë ¬
    cm = confusion_matrix(ground_truths, predictions)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes, ax=axes[1, 0])
    axes[1, 0].set_title('í˜¼ë™ í–‰ë ¬')
    
    # ê²€ì¶œ ì˜ˆì‹œ ì´ë¯¸ì§€
    # ... ê²€ì¶œ ê²°ê³¼ ì˜ˆì‹œ í‘œì‹œ
    
    plt.tight_layout()
    plt.show()
```

---

## 7. ì‹¤ì‹œê°„ ì¶”ë¡  ë° ìµœì í™”

### 7.1 ëª¨ë¸ ìµœì í™” ê¸°ë²•

#### TensorRT ìµœì í™”
```python
def optimize_with_tensorrt(model_path, input_shape=(1, 3, 640, 640)):
    """
    TensorRTë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ìµœì í™”
    """
    import tensorrt as trt
    import pycuda.driver as cuda
    
    # ONNXë¡œ ë³€í™˜
    model = YOLO(model_path)
    model.export(format='onnx', imgsz=640)
    
    # TensorRT ì—”ì§„ ë¹Œë“œ
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, logger)
    
    # ONNX íŒŒì¼ íŒŒì‹±
    with open(model_path.replace('.pt', '.onnx'), 'rb') as model_file:
        parser.parse(model_file.read())
    
    # ë¹Œë” ì„¤ì •
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB
    config.set_flag(trt.BuilderFlag.FP16)  # FP16 ì •ë°€ë„
    
    # ì—”ì§„ ë¹Œë“œ
    engine = builder.build_engine(network, config)
    
    # ì—”ì§„ ì €ì¥
    with open(model_path.replace('.pt', '.trt'), 'wb') as f:
        f.write(engine.serialize())
    
    print("TensorRT ìµœì í™” ì™„ë£Œ!")
    return model_path.replace('.pt', '.trt')

# ì–‘ìí™” (Quantization)
def quantize_model(model_path):
    """
    ëª¨ë¸ ì–‘ìí™” (INT8)
    """
    model = YOLO(model_path)
    
    # INT8 ì–‘ìí™” ì„¤ì •
    model.export(
        format='onnx',
        int8=True,
        data='dataset.yaml'  # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°
    )
    
    print("ëª¨ë¸ ì–‘ìí™” ì™„ë£Œ!")
```

#### ëª¨ë°”ì¼ ìµœì í™”
```python
def optimize_for_mobile(model_path):
    """
    ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ìš© ìµœì í™”
    """
    model = YOLO(model_path)
    
    # CoreML ë³€í™˜ (iOS)
    model.export(format='coreml', nms=True)
    
    # TensorFlow Lite ë³€í™˜ (Android)
    model.export(format='tflite', int8=True)
    
    # NCNN ë³€í™˜ (ê²½ëŸ‰í™”)
    model.export(format='ncnn')
    
    print("ëª¨ë°”ì¼ ìµœì í™” ì™„ë£Œ!")
```

### 7.2 ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬

```python
import cv2
import time
from collections import deque

class RealTimeDetector:
    def __init__(self, model_path, conf_threshold=0.5, iou_threshold=0.5):
        self.model = YOLO(model_path)
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.fps_queue = deque(maxlen=30)
        self.detection_history = deque(maxlen=100)
    
    def process_video(self, source=0, output_path=None):
        """
        ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì²˜ë¦¬
        
        Args:
            source: ë¹„ë””ì˜¤ ì†ŒìŠ¤ (0=ì›¹ìº , íŒŒì¼ê²½ë¡œ, RTSP URL)
            output_path: ì¶œë ¥ ë¹„ë””ì˜¤ ì €ì¥ ê²½ë¡œ
        """
        cap = cv2.VideoCapture(source)
        
        # ë¹„ë””ì˜¤ ì„¤ì •
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        print("ì‹¤ì‹œê°„ íƒì§€ ì‹œì‘... (ESCë¡œ ì¢…ë£Œ)")
        
        while True:
            start_time = time.time()
            
            ret, frame = cap.read()
            if not ret:
                break
            
            # ê°ì²´ íƒì§€
            results = self.model.predict(
                frame,
                conf=self.conf_threshold,
                iou=self.iou_threshold,
                verbose=False
            )[0]
            
            # ê²°ê³¼ ì‹œê°í™”
            annotated_frame = self.draw_detections(frame, results)
            
            # FPS ê³„ì‚°
            end_time = time.time()
            fps = 1 / (end_time - start_time)
            self.fps_queue.append(fps)
            avg_fps = sum(self.fps_queue) / len(self.fps_queue)
            
            # ì •ë³´ í‘œì‹œ
            self.draw_info(annotated_frame, avg_fps, results)
            
            # í™”ë©´ ì¶œë ¥
            cv2.imshow('êµì‹¤ ë¬¼ê±´ íƒì§€ê¸°', annotated_frame)
            
            # ë¹„ë””ì˜¤ ì €ì¥
            if output_path:
                out.write(annotated_frame)
            
            # ì¢…ë£Œ ì¡°ê±´
            if cv2.waitKey(1) & 0xFF == 27:  # ESC í‚¤
                break
        
        # ì •ë¦¬
        cap.release()
        if output_path:
            out.release()
        cv2.destroyAllWindows()
        
        print(f"í‰ê·  FPS: {avg_fps:.2f}")
    
    def draw_detections(self, frame, results):
        """ê²€ì¶œ ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ê·¸ë¦¬ê¸°"""
        annotated_frame = frame.copy()
        
        if results.boxes is not None:
            boxes = results.boxes.xyxy.cpu().numpy()
            confidences = results.boxes.conf.cpu().numpy()
            class_ids = results.boxes.cls.cpu().numpy().astype(int)
            
            class_names = ['book', 'laptop', 'chair', 'whiteboard', 'bag']
            colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]
            
            for box, conf, class_id in zip(boxes, confidences, class_ids):
                x1, y1, x2, y2 = box.astype(int)
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                color = colors[class_id % len(colors)]
                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
                
                # ë¼ë²¨ ê·¸ë¦¬ê¸°
                label = f"{class_names[class_id]}: {conf:.2f}"
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                
                cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), 
                            (x1 + label_size[0], y1), color, -1)
                cv2.putText(annotated_frame, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return annotated_frame
    
    def draw_info(self, frame, fps, results):
        """ì •ë³´ íŒ¨ë„ ê·¸ë¦¬ê¸°"""
        # FPS í‘œì‹œ
        cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # ê²€ì¶œ ê°œìˆ˜ í‘œì‹œ
        num_detections = len(results.boxes) if results.boxes is not None else 0
        cv2.putText(frame, f"Objects: {num_detections}", (10, 70), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # ëª¨ë¸ ì •ë³´
        cv2.putText(frame, "Classroom Object Detector", (10, frame.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    detector = RealTimeDetector('best.pt')
    
    # ì›¹ìº ìœ¼ë¡œ ì‹¤ì‹œê°„ íƒì§€
    detector.process_video(source=0)
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
    # detector.process_video(source='input_video.mp4', output_path='output_video.mp4')
```

---

## ğŸ“š ì°¸ê³  ìë£Œ ë° ì¶”ê°€ í•™ìŠµ

### ë…¼ë¬¸ ë° ë¬¸ì„œ
- **R-CNN**: "Rich feature hierarchies for accurate object detection" (Girshick et al., 2014)
- **Fast R-CNN**: "Fast R-CNN" (Girshick, 2015)
- **Faster R-CNN**: "Faster R-CNN: Towards Real-Time Object Detection" (Ren et al., 2015)
- **YOLO v1**: "You Only Look Once: Unified, Real-Time Object Detection" (Redmon et al., 2016)
- **YOLOv8**: "YOLOv8: A New State-of-the-Art for Object Detection" (Ultralytics, 2023)

### ì‹¤ìŠµ ë„êµ¬ ë° í”Œë«í¼
- [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)
- [Roboflow](https://roboflow.com/) - ë°ì´í„°ì…‹ ê´€ë¦¬ ë° ì¦ê°•
- [Google Colab](https://colab.research.google.com/) - ë¬´ë£Œ GPU í•™ìŠµ í™˜ê²½
- [Weights & Biases](https://wandb.ai/) - ì‹¤í—˜ ì¶”ì  ë° ì‹œê°í™”

### ë°ì´í„°ì…‹
- [COCO Dataset](https://cocodataset.org/) - ëŒ€ê·œëª¨ ê°ì²´ íƒì§€ ë°ì´í„°ì…‹
- [Open Images](https://storage.googleapis.com/openimages/web/index.html) - êµ¬ê¸€ì˜ ì˜¤í”ˆ ì´ë¯¸ì§€ ë°ì´í„°ì…‹
- [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) - ì „í†µì ì¸ ê°ì²´ íƒì§€ ë²¤ì¹˜ë§ˆí¬

---

## ğŸ¯ ì´ë²ˆ ì£¼ì°¨ í•µì‹¬ ì •ë¦¬

### í•™ìŠµ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

âœ… **ê°ì²´ íƒì§€ ê¸°ì´ˆ ê°œë…**
- ë°”ìš´ë”© ë°•ìŠ¤, ì‹ ë¢°ë„, í´ë˜ìŠ¤ í™•ë¥ 
- IoU, mAP ë“± í‰ê°€ ì§€í‘œ
- NMS ì•Œê³ ë¦¬ì¦˜ ì´í•´

âœ… **R-CNN ê³„ì—´ ë°œì „ì‚¬**
- R-CNN â†’ Fast R-CNN â†’ Faster R-CNN
- Two-stage ë°©ì‹ì˜ ì¥ë‹¨ì 
- RPNê³¼ ì•µì»¤ ë°•ìŠ¤ ê°œë…

âœ… **YOLO ì•„í‚¤í…ì²˜**
- One-stage ë°©ì‹ì˜ í˜ì‹ 
- YOLOv1ë¶€í„° YOLOv8ê¹Œì§€ì˜ ë°œì „
- Anchor-free ë°©ì‹ì˜ ì´í•´

âœ… **ì‹¤ì „ êµ¬í˜„ ëŠ¥ë ¥**
- YOLOv8ì„ í™œìš©í•œ ì»¤ìŠ¤í…€ í•™ìŠµ
- ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ì¦ê°•
- ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ì‹œìŠ¤í…œ êµ¬ì¶•

**ğŸš€ ì´ì œ ì—¬ëŸ¬ë¶„ì€ ê°ì²´ íƒì§€ì˜ ì´ë¡ ë¶€í„° ì‹¤ì œ êµ¬í˜„ê¹Œì§€ ì™„ì „íˆ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤!**

ë‹¤ìŒ ì£¼ì—ëŠ” ì´ëŸ¬í•œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ê°ì²´ íƒì§€ë¥¼ ë”ìš± ì‹¬í™”í•˜ê³ , ì›¹ ì„œë¹„ìŠ¤ë¡œ ë°°í¬í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê² ìŠµë‹ˆë‹¤.
